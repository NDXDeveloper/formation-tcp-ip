ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 8.14 Implications rÃ©seau pour les microservices

## Introduction

Les **microservices** transforment un monolithe en dizaines ou centaines de services indÃ©pendants qui communiquent par le rÃ©seau. Cette architecture apporte de nombreux avantages (scalabilitÃ©, indÃ©pendance technologique, dÃ©ploiements indÃ©pendants), mais introduit une **complexitÃ© rÃ©seau considÃ©rable**.

```
Monolithe :                    Microservices :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”
â”‚              â”‚              â”‚Auth â”‚â—„â”€â–ºâ”‚User â”‚â—„â”€â–ºâ”‚Orderâ”‚
â”‚  Application â”‚              â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”˜
â”‚              â”‚                 â–²         â–²         â–²
â”‚  (In-memory  â”‚                 â”‚         â”‚         â”‚
â”‚   function   â”‚                 â””â”€â”€â”€â”€â–ºâ”Œâ”€â”€â”€â”´â”€â”€â”€â”â—„â”€â”€â”€â”€â”˜
â”‚   calls)     â”‚                       â”‚Paymentâ”‚
â”‚              â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              Tout passe par le rÃ©seau !
```

Dans un monolithe, `userService.getUser(123)` est un appel de fonction en mÃ©moire (~1 nanoseconde). Dans une architecture microservices, cet appel devient une requÃªte HTTP sur le rÃ©seau (~10 millisecondes) - **10 millions de fois plus lent** !

Cette section explore les implications rÃ©seau et les patterns pour construire des systÃ¨mes microservices robustes.

## Le dÃ©fi rÃ©seau fondamental

### Fallacies of Distributed Computing

En 1994, Peter Deutsch a listÃ© les **8 hypothÃ¨ses fausses** que les dÃ©veloppeurs font sur les systÃ¨mes distribuÃ©s :

1. **Le rÃ©seau est fiable** âŒ
   - Les paquets se perdent, les connexions tombent

2. **La latence est zÃ©ro** âŒ
   - Chaque appel rÃ©seau prend du temps (1-100ms)

3. **La bande passante est infinie** âŒ
   - Limite physique, congestion possible

4. **Le rÃ©seau est sÃ©curisÃ©** âŒ
   - Man-in-the-middle, eavesdropping

5. **La topologie ne change pas** âŒ
   - Services dÃ©marrent/arrÃªtent, machines tombent

6. **Il y a un seul administrateur** âŒ
   - Multiples Ã©quipes, clouds, providers

7. **Le coÃ»t de transport est zÃ©ro** âŒ
   - Bande passante = argent

8. **Le rÃ©seau est homogÃ¨ne** âŒ
   - DiffÃ©rents protocoles, versions, latences

**Implications** : Chaque appel rÃ©seau peut Ã©chouer, Ãªtre lent, ou retourner des donnÃ©es incohÃ©rentes.

### Comparaison Monolithe vs Microservices

```python
# MONOLITHE : Appel en mÃ©moire
class UserService:
    def get_user(self, user_id):
        # AccÃ¨s direct DB
        return db.query(User).get(user_id)

class OrderService:
    def create_order(self, user_id, items):
        # Appel fonction en mÃ©moire
        user = user_service.get_user(user_id)  # < 1Âµs

        if not user:
            raise ValueError("User not found")

        order = Order(user_id=user_id, items=items)
        db.add(order)
        return order

# MICROSERVICES : Appel rÃ©seau
class OrderService:
    def create_order(self, user_id, items):
        # Appel HTTP Ã  un autre service
        try:
            response = requests.get(
                f'http://user-service/users/{user_id}',
                timeout=2.0  # ~10ms en moyenne
            )
            response.raise_for_status()
            user = response.json()
        except requests.Timeout:
            # RÃ©seau peut Ãªtre lent
            raise ServiceTimeout("User service timeout")
        except requests.RequestException:
            # Service peut Ãªtre down
            raise ServiceUnavailable("User service unavailable")

        if not user:
            raise ValueError("User not found")

        order = Order(user_id=user_id, items=items)
        db.add(order)
        return order

# RÃ©sultat :
# Monolithe : 1Âµs, toujours rÃ©ussit
# Microservices : 10ms, peut Ã©chouer
```

### Calcul de latence cumulÃ©e

```python
# Exemple : Page dashboard utilisateur
# Monolithe : 5 appels fonction en mÃ©moire = 5Âµs total

# Microservices : 5 appels rÃ©seau
latencies = {
    'user_service': 10,      # ms
    'order_service': 15,     # ms
    'payment_service': 20,   # ms
    'notification_service': 8, # ms
    'analytics_service': 12  # ms
}

# SÃ©quentiel
total_sequential = sum(latencies.values())  # 65ms

# ParallÃ¨le (best case)
total_parallel = max(latencies.values())  # 20ms

# Dans la rÃ©alitÃ© :
# - Network overhead
# - Serialization/deserialization
# - Service processing time
# â†’ Ajouter 30-50% : ~90ms sÃ©quentiel, ~30ms parallÃ¨le

print(f"Monolithe : 5Âµs")
print(f"Microservices (sÃ©quentiel) : {total_sequential}ms = {total_sequential*1000}Âµs")
print(f"Microservices (parallÃ¨le) : {total_parallel}ms = {total_parallel*1000}Âµs")

# Microservices sont 4000-18000Ã— plus lents !
```

**Conclusion** : Le rÃ©seau est le goulot d'Ã©tranglement majeur des microservices.

## Service Discovery

Dans une architecture microservices, les services doivent se **dÃ©couvrir** dynamiquement car :
- Les adresses IP changent (containers, auto-scaling)
- Plusieurs instances par service (load balancing)
- Services dÃ©marrent/arrÃªtent constamment

### 1. Service Discovery cÃ´tÃ© client

```python
# Consul, Etcd, ZooKeeper
import consul

class ServiceDiscovery:
    def __init__(self):
        self.consul = consul.Consul(host='localhost', port=8500)

    def register_service(self, name, host, port):
        """Enregistre un service dans Consul"""
        self.consul.agent.service.register(
            name=name,
            service_id=f"{name}-{host}-{port}",
            address=host,
            port=port,
            check=consul.Check.http(
                f"http://{host}:{port}/health",
                interval="10s"
            )
        )

    def discover_service(self, name):
        """Trouve toutes les instances d'un service"""
        _, services = self.consul.health.service(name, passing=True)

        instances = []
        for service in services:
            instances.append({
                'host': service['Service']['Address'],
                'port': service['Service']['Port']
            })

        return instances

    def deregister_service(self, service_id):
        """DÃ©senregistre un service"""
        self.consul.agent.service.deregister(service_id)

# Utilisation dans un service
discovery = ServiceDiscovery()

# Au dÃ©marrage : s'enregistrer
discovery.register_service('user-service', '10.0.1.5', 8080)

# Appeler un autre service
class OrderService:
    def __init__(self):
        self.discovery = ServiceDiscovery()

    def get_user(self, user_id):
        # DÃ©couvrir les instances du service utilisateur
        user_instances = self.discovery.discover_service('user-service')

        if not user_instances:
            raise ServiceUnavailable("No user-service instances available")

        # Load balancing simple (round-robin)
        instance = random.choice(user_instances)

        # Appeler l'instance
        url = f"http://{instance['host']}:{instance['port']}/users/{user_id}"
        response = requests.get(url, timeout=2.0)

        return response.json()
```

**Avantages** :
- âœ… ContrÃ´le total du load balancing
- âœ… Pas de proxy intermÃ©diaire

**InconvÃ©nients** :
- âŒ Logique discovery dans chaque service
- âŒ Mise Ã  jour du registry = redÃ©ployer clients

### 2. Service Discovery cÃ´tÃ© serveur

```python
# Utilisation d'un load balancer (Nginx, HAProxy, AWS ELB)
"""
Service A veut appeler Service B :

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service A â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Load Balancerâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚Service B-1â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  (user-svc)  â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚Service B-2â”‚
                                               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                               â”‚Service B-3â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Service A appelle simplement : http://user-service
Le load balancer route vers une instance disponible
"""

# Configuration Nginx
"""
upstream user-service {
    server user-service-1:8080;
    server user-service-2:8080;
    server user-service-3:8080;
}

server {
    listen 80;
    server_name user-service;

    location / {
        proxy_pass http://user-service;
    }
}
"""

# Code simplifiÃ© cÃ´tÃ© client
class OrderService:
    def get_user(self, user_id):
        # Appel simple au nom du service
        # Le DNS/Load Balancer gÃ¨re le routing
        response = requests.get(
            f'http://user-service/users/{user_id}',
            timeout=2.0
        )
        return response.json()
```

**Avantages** :
- âœ… Code client simple
- âœ… Changements centralisÃ©s

**InconvÃ©nients** :
- âŒ Load balancer = SPOF
- âŒ Moins de contrÃ´le

### 3. Service Mesh (moderne)

Un **service mesh** (Istio, Linkerd, Consul Connect) gÃ¨re automatiquement :
- Service discovery
- Load balancing
- Encryption (mTLS)
- Observability
- Circuit breaking
- Retry logic

```yaml
# Exemple Istio : Configuration automatique
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
  - user-service
  http:
  - route:
    - destination:
        host: user-service
        subset: v1
      weight: 90
    - destination:
        host: user-service
        subset: v2
      weight: 10  # Canary deployment
    timeout: 2s
    retries:
      attempts: 3
      perTryTimeout: 500ms
```

```python
# Code application : AUCUN changement !
# Le sidecar proxy gÃ¨re tout automatiquement
class OrderService:
    def get_user(self, user_id):
        # Appel simple
        response = requests.get(f'http://user-service/users/{user_id}')
        return response.json()

        # Istio sidecar gÃ¨re automatiquement :
        # - Service discovery
        # - Load balancing
        # - Retries (3 tentatives)
        # - Timeout (2s)
        # - Circuit breaking
        # - mTLS encryption
        # - Metrics/tracing
```

**Architecture Service Mesh** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Service A Pod                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Service A  â”‚â—„â”€â”€â–ºâ”‚Envoy Sidecar â”‚ â—„â”¼â”€â”
â”‚  â”‚ Container  â”‚    â”‚  (Proxy)     â”‚  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                         â”‚
                                    Network
                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚         Service B Pod                â”‚ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚ Service B  â”‚â—„â”€â”€â–ºâ”‚Envoy Sidecar â”‚ â—„â”¼â”€â”˜
â”‚  â”‚ Container  â”‚    â”‚  (Proxy)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Services communiquent via leurs sidecars
```

**Avantages** :
- âœ… Configuration centralisÃ©e
- âœ… FonctionnalitÃ©s avancÃ©es sans code
- âœ… Polyglotte (fonctionne avec tous langages)
- âœ… ObservabilitÃ© complÃ¨te

**InconvÃ©nients** :
- âŒ ComplexitÃ© opÃ©rationnelle
- âŒ Overhead latence (~1-2ms par hop)
- âŒ Courbe d'apprentissage

## Patterns de communication

### 1. Synchrone : REST/HTTP

```python
# Service Order appelle Service User
import requests

class OrderService:
    def __init__(self):
        self.user_service_url = "http://user-service"

    def create_order(self, user_id, items):
        # Appel synchrone : bloque jusqu'Ã  rÃ©ponse
        try:
            response = requests.get(
                f"{self.user_service_url}/users/{user_id}",
                timeout=2.0
            )
            response.raise_for_status()
            user = response.json()
        except requests.RequestException as e:
            raise ServiceError(f"Failed to get user: {e}")

        # CrÃ©er commande
        order = Order(user_id=user_id, items=items)
        db.add(order)

        return order
```

**Avantages** :
- âœ… Simple Ã  implÃ©menter
- âœ… RÃ©ponse immÃ©diate
- âœ… Facile Ã  dÃ©bugger

**InconvÃ©nients** :
- âŒ Couplage fort (service doit Ãªtre disponible)
- âŒ Latence cumulÃ©e
- âŒ Cascade de failures

**Cas d'usage** : Lecture de donnÃ©es, opÃ©rations rapides

### 2. Asynchrone : Message Queue

```python
# Service Order publie un Ã©vÃ©nement
import pika
import json

class OrderService:
    def __init__(self):
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters('rabbitmq')
        )
        self.channel = self.connection.channel()
        self.channel.exchange_declare(
            exchange='events',
            exchange_type='topic'
        )

    def create_order(self, user_id, items):
        # CrÃ©er commande immÃ©diatement
        order = Order(user_id=user_id, items=items)
        db.add(order)
        db.commit()

        # Publier Ã©vÃ©nement (fire-and-forget)
        event = {
            'event_type': 'order.created',
            'order_id': order.id,
            'user_id': user_id,
            'items': items,
            'timestamp': datetime.utcnow().isoformat()
        }

        self.channel.basic_publish(
            exchange='events',
            routing_key='order.created',
            body=json.dumps(event)
        )

        return order

# Service Email consomme l'Ã©vÃ©nement
class EmailService:
    def __init__(self):
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters('rabbitmq')
        )
        self.channel = self.connection.channel()
        self.channel.queue_declare(queue='email_notifications')
        self.channel.queue_bind(
            exchange='events',
            queue='email_notifications',
            routing_key='order.created'
        )

    def start_consuming(self):
        def callback(ch, method, properties, body):
            event = json.loads(body)

            # Traiter de maniÃ¨re asynchrone
            self.send_order_confirmation(
                event['user_id'],
                event['order_id']
            )

            ch.basic_ack(delivery_tag=method.delivery_tag)

        self.channel.basic_consume(
            queue='email_notifications',
            on_message_callback=callback
        )

        self.channel.start_consuming()

    def send_order_confirmation(self, user_id, order_id):
        # Envoyer email
        print(f"Sending confirmation for order {order_id}")
```

**Avantages** :
- âœ… DÃ©couplage fort (services indÃ©pendants)
- âœ… RÃ©silience (messages persistÃ©s)
- âœ… ScalabilitÃ© (consommateurs multiples)
- âœ… Pas de blocage

**InconvÃ©nients** :
- âŒ ComplexitÃ© (infrastructure messaging)
- âŒ Pas de rÃ©ponse immÃ©diate
- âŒ Ordre des messages non garanti (sauf config)
- âŒ Debugging plus difficile

**Cas d'usage** : Notifications, analytics, opÃ©rations longues

### 3. API Gateway Pattern

```python
# Gateway agrÃ¨ge plusieurs microservices
from fastapi import FastAPI, HTTPException
import httpx
import asyncio

app = FastAPI()

class APIGateway:
    def __init__(self):
        self.client = httpx.AsyncClient(timeout=5.0)

    async def get_user_dashboard(self, user_id: int):
        """AgrÃ¨ge donnÃ©es de multiples services"""
        try:
            # Appels parallÃ¨les
            user, orders, recommendations = await asyncio.gather(
                self.get_user(user_id),
                self.get_user_orders(user_id),
                self.get_recommendations(user_id),
                return_exceptions=True
            )

            # GÃ©rer les erreurs individuellement
            result = {}

            if isinstance(user, Exception):
                result['user'] = None
                result['user_error'] = str(user)
            else:
                result['user'] = user

            if isinstance(orders, Exception):
                result['orders'] = []
                result['orders_error'] = str(orders)
            else:
                result['orders'] = orders

            if isinstance(recommendations, Exception):
                result['recommendations'] = []
            else:
                result['recommendations'] = recommendations

            return result

        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

    async def get_user(self, user_id: int):
        response = await self.client.get(f'http://user-service/users/{user_id}')
        response.raise_for_status()
        return response.json()

    async def get_user_orders(self, user_id: int):
        response = await self.client.get(f'http://order-service/users/{user_id}/orders')
        response.raise_for_status()
        return response.json()

    async def get_recommendations(self, user_id: int):
        response = await self.client.get(f'http://recommendation-service/users/{user_id}')
        response.raise_for_status()
        return response.json()

gateway = APIGateway()

@app.get('/api/dashboard/{user_id}')
async def get_dashboard(user_id: int):
    return await gateway.get_user_dashboard(user_id)
```

**Avantages** :
- âœ… Point d'entrÃ©e unique
- âœ… AgrÃ©gation de donnÃ©es
- âœ… Authentification centralisÃ©e
- âœ… Rate limiting centralisÃ©
- âœ… Cache possible

**InconvÃ©nients** :
- âŒ SPOF potentiel
- âŒ Peut devenir complexe

**Architecture** :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ API Gateway â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚              â”‚              â”‚
          â–¼              â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ User    â”‚   â”‚  Order   â”‚   â”‚ Payment â”‚
    â”‚ Service â”‚   â”‚ Service  â”‚   â”‚ Service â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Backend for Frontend (BFF)

```python
# Un gateway par type de client
"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Web Client   â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Web BFF    â”‚â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Mobile Client â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Mobile BFF  â”‚â”€â”€â”¼â”€â”€â”€â–ºâ”‚ Services â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ IoT Device   â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  IoT BFF    â”‚â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

# Web BFF : Retourne HTML enrichi
@app.get('/dashboard/{user_id}')
async def web_dashboard(user_id: int):
    data = await gateway.get_user_dashboard(user_id)

    # Format riche pour web
    return {
        'user': {
            **data['user'],
            'avatar_url_large': f"{data['user']['avatar']}_large.jpg",
            'full_profile_link': f"/users/{user_id}"
        },
        'orders': data['orders'][:10],  # 10 derniÃ¨res
        'recommendations': data['recommendations'][:20],  # 20 suggestions
        'ui_config': {
            'show_ads': True,
            'theme': 'dark'
        }
    }

# Mobile BFF : Format compact
@app.get('/mobile/dashboard/{user_id}')
async def mobile_dashboard(user_id: int):
    data = await gateway.get_user_dashboard(user_id)

    # Format compact pour mobile
    return {
        'user': {
            'id': data['user']['id'],
            'name': data['user']['name'],
            'avatar': f"{data['user']['avatar']}_thumb.jpg"  # Thumbnail
        },
        'orders': data['orders'][:5],  # 5 derniÃ¨res
        'recommendations': data['recommendations'][:5]  # 5 suggestions
    }
```

**Avantages** :
- âœ… OptimisÃ© par type de client
- âœ… Ã‰quipes indÃ©pendantes par plateforme
- âœ… Pas de compromis dans les APIs

**InconvÃ©nients** :
- âŒ Duplication de logique
- âŒ Plus de services Ã  maintenir

## RÃ©silience et Fault Tolerance

### 1. Timeouts Ã  tous les niveaux

```python
import httpx
import asyncio

class ResilientClient:
    """Client HTTP avec timeouts complets"""

    def __init__(self):
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(
                connect=2.0,    # Connexion TCP
                read=5.0,       # Lecture rÃ©ponse
                write=5.0,      # Ã‰criture requÃªte
                pool=1.0        # Attente connexion du pool
            )
        )

    async def call_service(self, url: str):
        try:
            response = await self.client.get(url)
            return response.json()
        except httpx.TimeoutException as e:
            # Timeout spÃ©cifique
            raise ServiceTimeout(f"Service timeout: {e}")
        except httpx.RequestError as e:
            # Erreur rÃ©seau
            raise ServiceError(f"Network error: {e}")
```

**Timeouts recommandÃ©s** :

```python
# RÃ¨gle : Parent timeout > somme enfants timeout

# Service A (parent)
PARENT_TIMEOUT = 10.0  # secondes

# Service A appelle B, C, D en sÃ©quence
SERVICE_B_TIMEOUT = 3.0
SERVICE_C_TIMEOUT = 3.0
SERVICE_D_TIMEOUT = 3.0

# Total : 9.0s < 10.0s âœ“ OK

# Si en parallÃ¨le
PARALLEL_TIMEOUT = max(SERVICE_B_TIMEOUT, SERVICE_C_TIMEOUT, SERVICE_D_TIMEOUT)
# = 3.0s < 10.0s âœ“ OK avec marge
```

### 2. Circuit Breaker distribuÃ©

```python
import time
from enum import Enum
from collections import deque

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class DistributedCircuitBreaker:
    """Circuit breaker avec Ã©tat partagÃ© (Redis)"""

    def __init__(self, service_name, redis_client):
        self.service_name = service_name
        self.redis = redis_client
        self.key_prefix = f"circuit:{service_name}"

        # Configuration
        self.failure_threshold = 5
        self.recovery_timeout = 60
        self.success_threshold = 2

    def get_state(self):
        """RÃ©cupÃ¨re l'Ã©tat du circuit (partagÃ© entre instances)"""
        state = self.redis.get(f"{self.key_prefix}:state")
        return CircuitState(state) if state else CircuitState.CLOSED

    def set_state(self, state: CircuitState):
        """DÃ©finit l'Ã©tat du circuit"""
        self.redis.set(f"{self.key_prefix}:state", state.value)

    async def call(self, func, *args, **kwargs):
        """ExÃ©cute fonction avec circuit breaker"""
        state = self.get_state()

        if state == CircuitState.OPEN:
            # VÃ©rifier si timeout rÃ©cupÃ©ration Ã©coulÃ©
            last_failure = float(self.redis.get(f"{self.key_prefix}:last_failure") or 0)
            if time.time() - last_failure > self.recovery_timeout:
                self.set_state(CircuitState.HALF_OPEN)
                self.redis.set(f"{self.key_prefix}:success_count", 0)
            else:
                raise CircuitBreakerOpen(f"Circuit breaker open for {self.service_name}")

        try:
            result = await func(*args, **kwargs)
            self.on_success()
            return result

        except Exception as e:
            self.on_failure()
            raise

    def on_success(self):
        """SuccÃ¨s : rÃ©initialiser compteur Ã©checs"""
        state = self.get_state()

        if state == CircuitState.HALF_OPEN:
            # IncrÃ©menter succÃ¨s
            success_count = self.redis.incr(f"{self.key_prefix}:success_count")

            if success_count >= self.success_threshold:
                self.set_state(CircuitState.CLOSED)
                self.redis.set(f"{self.key_prefix}:failure_count", 0)
        else:
            self.redis.set(f"{self.key_prefix}:failure_count", 0)

    def on_failure(self):
        """Ã‰chec : incrÃ©menter compteur"""
        failure_count = self.redis.incr(f"{self.key_prefix}:failure_count")
        self.redis.set(f"{self.key_prefix}:last_failure", time.time())

        if failure_count >= self.failure_threshold:
            self.set_state(CircuitState.OPEN)

# Utilisation
import redis
import httpx

redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
breaker = DistributedCircuitBreaker('user-service', redis_client)

async def get_user(user_id):
    async def call_user_service():
        async with httpx.AsyncClient() as client:
            response = await client.get(f'http://user-service/users/{user_id}')
            response.raise_for_status()
            return response.json()

    return await breaker.call(call_user_service)
```

### 3. Retry avec Exponential Backoff

```python
import asyncio
import random

async def retry_with_backoff(
    func,
    max_retries=3,
    base_delay=1.0,
    max_delay=60.0,
    exponential_base=2,
    jitter=True,
    retry_on=(Exception,)
):
    """Retry avec exponential backoff et jitter"""
    for attempt in range(max_retries):
        try:
            return await func()

        except retry_on as e:
            if attempt == max_retries - 1:
                raise

            # Calculer dÃ©lai
            delay = min(base_delay * (exponential_base ** attempt), max_delay)

            # Ajouter jitter
            if jitter:
                delay = delay * (0.5 + random.random())

            print(f"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.2f}s")
            await asyncio.sleep(delay)

# Utilisation
async def call_unreliable_service():
    async with httpx.AsyncClient() as client:
        response = await client.get('http://unreliable-service/data')
        response.raise_for_status()
        return response.json()

data = await retry_with_backoff(
    call_unreliable_service,
    max_retries=5,
    retry_on=(httpx.TimeoutException, httpx.HTTPError)
)
```

### 4. Bulkhead Pattern (isolation)

```python
from concurrent.futures import ThreadPoolExecutor

class BulkheadExecutor:
    """Isole les appels Ã  diffÃ©rents services"""

    def __init__(self):
        # Pool sÃ©parÃ© par service
        self.executors = {
            'user-service': ThreadPoolExecutor(max_workers=10),
            'order-service': ThreadPoolExecutor(max_workers=20),
            'payment-service': ThreadPoolExecutor(max_workers=5),
        }

    async def call_service(self, service_name, func, *args, **kwargs):
        """ExÃ©cute fonction dans pool dÃ©diÃ©"""
        executor = self.executors.get(service_name)
        if not executor:
            raise ValueError(f"Unknown service: {service_name}")

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(executor, func, *args, **kwargs)

# Si user-service est lent/down :
# - Son pool se sature (10 workers bloquÃ©s)
# - Mais order-service continue de fonctionner (pool sÃ©parÃ©)
# â†’ Isolation des failures
```

## ObservabilitÃ©

### 1. Distributed Tracing

```python
# OpenTelemetry : Standard pour tracing
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Configuration
trace.set_tracer_provider(TracerProvider())
jaeger_exporter = JaegerExporter(
    agent_host_name='localhost',
    agent_port=6831,
)
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(jaeger_exporter)
)

tracer = trace.get_tracer(__name__)

# Utilisation
@app.post('/api/orders')
async def create_order(order_data: dict):
    # Span parent
    with tracer.start_as_current_span("create_order") as span:
        span.set_attribute("order.items_count", len(order_data['items']))

        # Appel user service
        with tracer.start_as_current_span("get_user"):
            user = await get_user(order_data['user_id'])

        # Appel payment service
        with tracer.start_as_current_span("process_payment"):
            payment = await process_payment(order_data['payment'])

        # CrÃ©er commande
        with tracer.start_as_current_span("save_order"):
            order = Order(**order_data)
            db.add(order)
            db.commit()

        span.set_attribute("order.id", order.id)
        return {"order_id": order.id}

# Trace complÃ¨te visible dans Jaeger :
"""
create_order (total: 150ms)
â”œâ”€â”€ get_user (20ms)
â”‚   â””â”€â”€ db_query (15ms)
â”œâ”€â”€ process_payment (100ms)
â”‚   â”œâ”€â”€ validate_card (10ms)
â”‚   â””â”€â”€ charge_card (90ms)
â””â”€â”€ save_order (30ms)
    â””â”€â”€ db_insert (25ms)
"""
```

**Visualisation Jaeger** :

```
Timeline :
0ms     50ms    100ms   150ms
|-------|-------|-------|
create_order â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  get_user â”€â”€â”€â”€â”€â”€
            process_payment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                          save_order â”€â”€â”€â”€â”€â”€

Latence totale : 150ms
Latence rÃ©seau : 20ms + 100ms = 120ms (80% du temps !)
```

### 2. Structured Logging

```python
import logging
import json
from datetime import datetime

class StructuredLogger:
    """Logger JSON structurÃ©"""

    def __init__(self, service_name):
        self.service_name = service_name
        self.logger = logging.getLogger(service_name)

    def log(self, level, message, **context):
        """Log structurÃ© en JSON"""
        log_entry = {
            'timestamp': datetime.utcnow().isoformat(),
            'service': self.service_name,
            'level': level,
            'message': message,
            **context
        }

        self.logger.log(
            getattr(logging, level.upper()),
            json.dumps(log_entry)
        )

    def info(self, message, **context):
        self.log('info', message, **context)

    def error(self, message, **context):
        self.log('error', message, **context)

# Utilisation
logger = StructuredLogger('order-service')

@app.post('/api/orders')
async def create_order(order_data: dict):
    logger.info(
        'Order creation started',
        user_id=order_data['user_id'],
        items_count=len(order_data['items']),
        total_amount=order_data['total']
    )

    try:
        order = await order_service.create(order_data)

        logger.info(
            'Order created successfully',
            order_id=order.id,
            user_id=order.user_id,
            processing_time_ms=order.processing_time
        )

        return {"order_id": order.id}

    except Exception as e:
        logger.error(
            'Order creation failed',
            user_id=order_data['user_id'],
            error=str(e),
            error_type=type(e).__name__
        )
        raise

# Logs JSON pour Elasticsearch/Splunk
"""
{
  "timestamp": "2025-01-15T10:30:00.123456",
  "service": "order-service",
  "level": "info",
  "message": "Order creation started",
  "user_id": 12345,
  "items_count": 3,
  "total_amount": 99.99
}
"""
```

### 3. MÃ©triques (Prometheus)

```python
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# DÃ©finir mÃ©triques
request_count = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

active_requests = Gauge(
    'http_active_requests',
    'Number of active requests',
    ['endpoint']
)

# Middleware FastAPI
from fastapi import FastAPI, Request
import time

app = FastAPI()

@app.middleware("http")
async def prometheus_middleware(request: Request, call_next):
    # IncrÃ©menter requests actives
    endpoint = request.url.path
    active_requests.labels(endpoint=endpoint).inc()

    # Mesurer durÃ©e
    start_time = time.time()

    try:
        response = await call_next(request)

        # MÃ©triques succÃ¨s
        duration = time.time() - start_time

        request_count.labels(
            method=request.method,
            endpoint=endpoint,
            status=response.status_code
        ).inc()

        request_duration.labels(
            method=request.method,
            endpoint=endpoint
        ).observe(duration)

        return response

    finally:
        # DÃ©crÃ©menter requests actives
        active_requests.labels(endpoint=endpoint).dec()

# Exposer mÃ©triques sur :9090/metrics
start_http_server(9090)

# MÃ©triques exportÃ©es :
"""
# HELP http_requests_total Total HTTP requests
# TYPE http_requests_total counter
http_requests_total{method="POST",endpoint="/api/orders",status="200"} 1523
http_requests_total{method="POST",endpoint="/api/orders",status="500"} 12

# HELP http_request_duration_seconds HTTP request duration
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{method="POST",endpoint="/api/orders",le="0.1"} 1200
http_request_duration_seconds_bucket{method="POST",endpoint="/api/orders",le="0.5"} 1450
http_request_duration_seconds_bucket{method="POST",endpoint="/api/orders",le="1.0"} 1520
http_request_duration_seconds_sum{method="POST",endpoint="/api/orders"} 234.56
http_request_duration_seconds_count{method="POST",endpoint="/api/orders"} 1535
"""
```

## Performance et optimisation

### 1. Connection Pooling

```python
import httpx

# Pool de connexions rÃ©utilisables
class ServiceClient:
    def __init__(self, base_url, pool_size=100):
        # Limites de connexions
        limits = httpx.Limits(
            max_keepalive_connections=pool_size,
            max_connections=pool_size * 2,
            keepalive_expiry=30.0
        )

        # Client avec pool
        self.client = httpx.AsyncClient(
            base_url=base_url,
            limits=limits,
            timeout=5.0,
            http2=True  # HTTP/2 pour multiplexage
        )

    async def get(self, path):
        # RÃ©utilise connexion existante du pool
        response = await self.client.get(path)
        return response.json()

# Sans pool : Nouvelle connexion TCP Ã  chaque requÃªte
# Avec pool : Connexion rÃ©utilisÃ©e (100Ã— plus rapide)
```

### 2. Request Batching

```python
class BatchingClient:
    """Groupe plusieurs requÃªtes en une seule"""

    def __init__(self, service_url, batch_size=100, batch_timeout=0.1):
        self.service_url = service_url
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout

        self.pending = []
        self.batch_task = None

    async def get_user(self, user_id):
        """Get user avec batching automatique"""
        future = asyncio.Future()

        self.pending.append({
            'user_id': user_id,
            'future': future
        })

        # DÃ©marrer timer si pas dÃ©jÃ  fait
        if not self.batch_task:
            self.batch_task = asyncio.create_task(self._flush_after_timeout())

        # Flush si batch plein
        if len(self.pending) >= self.batch_size:
            await self._flush()

        return await future

    async def _flush_after_timeout(self):
        """Flush aprÃ¨s timeout"""
        await asyncio.sleep(self.batch_timeout)
        await self._flush()

    async def _flush(self):
        """Envoyer le batch"""
        if not self.pending:
            return

        batch = self.pending
        self.pending = []
        self.batch_task = None

        # Une seule requÃªte HTTP pour tous
        user_ids = [item['user_id'] for item in batch]

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f'{self.service_url}/users/batch',
                    json={'user_ids': user_ids}
                )
                users = response.json()

            # Distribuer rÃ©sultats
            users_map = {u['id']: u for u in users}
            for item in batch:
                user = users_map.get(item['user_id'])
                item['future'].set_result(user)

        except Exception as e:
            # Propager erreur Ã  tous
            for item in batch:
                item['future'].set_exception(e)

# Utilisation
client = BatchingClient('http://user-service')

# Ces 150 appels sont groupÃ©s en 2 requÃªtes HTTP :
# - 100 premiers (batch plein)
# - 50 restants (timeout)
users = await asyncio.gather(*[
    client.get_user(i) for i in range(150)
])

# Au lieu de 150 requÃªtes HTTP â†’ 2 requÃªtes
# RÃ©duction latence : 150 Ã— 10ms = 1500ms â†’ 20ms
```

### 3. Caching distribuÃ©

```python
import redis
import hashlib
import json

class DistributedCache:
    """Cache distribuÃ© avec Redis"""

    def __init__(self, redis_url='redis://localhost'):
        self.redis = redis.from_url(redis_url, decode_responses=True)

    def cache_key(self, prefix, *args, **kwargs):
        """GÃ©nÃ¨re clÃ© de cache"""
        key_data = f"{prefix}:{args}:{sorted(kwargs.items())}"
        return hashlib.md5(key_data.encode()).hexdigest()

    async def get_or_fetch(self, key, fetch_func, ttl=300):
        """Cache-aside pattern"""
        # VÃ©rifier cache
        cached = self.redis.get(key)
        if cached:
            return json.loads(cached)

        # Fetch
        data = await fetch_func()

        # Mettre en cache
        self.redis.setex(key, ttl, json.dumps(data))

        return data

    def invalidate(self, pattern):
        """Invalider cache par pattern"""
        for key in self.redis.scan_iter(match=pattern):
            self.redis.delete(key)

# Utilisation
cache = DistributedCache()

async def get_user(user_id):
    key = cache.cache_key('user', user_id)

    async def fetch():
        async with httpx.AsyncClient() as client:
            response = await client.get(f'http://user-service/users/{user_id}')
            return response.json()

    return await cache.get_or_fetch(key, fetch, ttl=600)

# Premier appel : fetch service (100ms)
user = await get_user(123)

# Appels suivants (10 min) : cache (1ms)
user = await get_user(123)  # 100Ã— plus rapide !
```

## SÃ©curitÃ© inter-services

### 1. mTLS (Mutual TLS)

```python
# Configuration certificats
"""
Service A                Service B
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          â”‚  â”€â”€â”€â”€â”€â–º    â”‚          â”‚
â”‚  Cert A  â”‚  TLS       â”‚  Cert B  â”‚
â”‚          â”‚  â—„â”€â”€â”€â”€â”€    â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Les deux services vÃ©rifient mutuellement leurs certificats
"""

# Client avec mTLS
import httpx

client = httpx.Client(
    cert=(
        '/path/to/client-cert.pem',
        '/path/to/client-key.pem'
    ),
    verify='/path/to/ca-cert.pem'  # VÃ©rifie serveur
)

response = client.get('https://service-b/data')

# Service Mesh (Istio) gÃ¨re mTLS automatiquement
# Aucun code nÃ©cessaire !
```

### 2. Service-to-Service Authentication (JWT)

```python
import jwt
from datetime import datetime, timedelta

class ServiceAuth:
    """Authentication entre services"""

    def __init__(self, service_name, secret_key):
        self.service_name = service_name
        self.secret_key = secret_key

    def generate_token(self, target_service, ttl=300):
        """GÃ©nÃ¨re token pour appeler un service"""
        payload = {
            'iss': self.service_name,  # Issuer (qui Ã©met)
            'aud': target_service,     # Audience (pour qui)
            'iat': datetime.utcnow(),
            'exp': datetime.utcnow() + timedelta(seconds=ttl)
        }

        return jwt.encode(payload, self.secret_key, algorithm='HS256')

    def verify_token(self, token):
        """VÃ©rifie token reÃ§u"""
        try:
            payload = jwt.decode(
                token,
                self.secret_key,
                algorithms=['HS256'],
                audience=self.service_name  # VÃ©rifier audience
            )
            return payload['iss']  # Retourner service source

        except jwt.InvalidTokenError:
            raise Unauthorized("Invalid service token")

# Service A (appelant)
auth = ServiceAuth('order-service', SECRET_KEY)

async def call_user_service(user_id):
    # GÃ©nÃ©rer token
    token = auth.generate_token('user-service')

    # Appel avec token
    async with httpx.AsyncClient() as client:
        response = await client.get(
            f'http://user-service/users/{user_id}',
            headers={'Authorization': f'Bearer {token}'}
        )
        return response.json()

# Service B (receveur)
from fastapi import Header, HTTPException

@app.get('/users/{user_id}')
async def get_user(user_id: int, authorization: str = Header(None)):
    if not authorization or not authorization.startswith('Bearer '):
        raise HTTPException(401, "Missing token")

    token = authorization.split(' ')[1]

    # VÃ©rifier token
    try:
        source_service = auth.verify_token(token)
        print(f"Request from {source_service}")
    except Unauthorized:
        raise HTTPException(401, "Invalid token")

    user = db.get_user(user_id)
    return user
```

### 3. API Rate Limiting

```python
import time
from collections import defaultdict

class RateLimiter:
    """Rate limiting par service"""

    def __init__(self, redis_client):
        self.redis = redis_client

    def is_allowed(self, service_name, rate=100, per=60):
        """
        Rate limiting avec sliding window

        service_name: Nom du service appelant
        rate: Nombre de requÃªtes
        per: PÃ©riode en secondes
        """
        key = f"ratelimit:{service_name}"
        now = time.time()
        window_start = now - per

        # Supprimer requÃªtes hors fenÃªtre
        self.redis.zremrangebyscore(key, 0, window_start)

        # Compter requÃªtes dans fenÃªtre
        count = self.redis.zcard(key)

        if count >= rate:
            return False

        # Ajouter cette requÃªte
        self.redis.zadd(key, {str(now): now})
        self.redis.expire(key, per)

        return True

# Middleware
from fastapi import Request, HTTPException

rate_limiter = RateLimiter(redis_client)

@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    # Extraire service source du token
    token = request.headers.get('Authorization', '').replace('Bearer ', '')

    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=['HS256'])
        source_service = payload['iss']
    except:
        raise HTTPException(401, "Invalid token")

    # VÃ©rifier rate limit
    if not rate_limiter.is_allowed(source_service, rate=1000, per=60):
        raise HTTPException(429, "Rate limit exceeded")

    response = await call_next(request)
    return response
```

## Cas d'usage rÃ©els

### 1. Netflix : Chaos Engineering

Netflix teste la rÃ©silience en **causant intentionnellement des pannes** :

```python
# Chaos Monkey : ArrÃªte alÃ©atoirement des instances
import random

class ChaosMonkey:
    """Simule pannes alÃ©atoires en production"""

    def __init__(self, failure_probability=0.01):
        self.failure_probability = failure_probability

    async def call_service(self, func):
        """Appel service avec chaos"""
        # 1% de chance de panne simulÃ©e
        if random.random() < self.failure_probability:
            raise ServiceUnavailable("Chaos Monkey killed this request")

        return await func()

# Utilisation
chaos = ChaosMonkey(failure_probability=0.01)

@app.get('/api/data')
async def get_data():
    try:
        data = await chaos.call_service(fetch_from_service)
        return data
    except ServiceUnavailable:
        # Fallback
        return get_cached_data()

# Netflix teste ainsi en production :
# - Circuit breakers fonctionnent
# - Fallbacks activÃ©s correctement
# - SystÃ¨me reste disponible malgrÃ© pannes
```

### 2. Uber : Service Mesh adoption

Uber a migrÃ© vers un service mesh pour gÃ©rer :
- 2000+ microservices
- 40 000+ instances
- 150 000+ requÃªtes/seconde

```yaml
# Configuration Envoy (service mesh Uber)
# Retry automatique + circuit breaking
clusters:
- name: ride-service
  connect_timeout: 0.25s
  type: STRICT_DNS
  lb_policy: ROUND_ROBIN
  circuit_breakers:
    thresholds:
    - max_connections: 100
      max_pending_requests: 100
      max_requests: 100
      max_retries: 3
  outlier_detection:
    consecutive_5xx: 5
    interval: 30s
    base_ejection_time: 30s
  load_assignment:
    cluster_name: ride-service
    endpoints:
    - lb_endpoints:
      - endpoint:
          address:
            socket_address:
              address: ride-service
              port_value: 8080
```

**RÃ©sultats** :
- âœ… 50% rÃ©duction des erreurs 5xx
- âœ… P99 latency rÃ©duite de 30%
- âœ… DÃ©ploiements plus sÃ»rs (canary automatique)

### 3. Amazon : Two-Pizza Teams

Amazon organise ses Ã©quipes en **"two-pizza teams"** - assez petites pour Ãªtre nourries avec deux pizzas (~8 personnes).

```
Chaque Ã©quipe possÃ¨de :
- Ses propres microservices
- Sa propre base de donnÃ©es
- Son propre pipeline CI/CD
- Autonomie complÃ¨te

Communication :
- APIs bien dÃ©finies
- Contracts tests
- SLA documentÃ©s

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    API    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Payments Team  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Orders Team    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚           â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚Payment Svc â”‚ â”‚           â”‚ â”‚ Order Svc  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚           â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚           â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ PaymentDB  â”‚ â”‚           â”‚ â”‚  OrderDB   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚           â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Spotify : Backend for Frontend

Spotify utilise des BFF pour chaque plateforme :

```python
# iOS BFF : Format optimisÃ© mobile
@app.get('/mobile/playlist/{playlist_id}')
async def get_playlist_mobile(playlist_id: str):
    playlist = await playlist_service.get(playlist_id)

    return {
        'id': playlist.id,
        'name': playlist.name,
        'cover': f"{playlist.cover}_300x300.jpg",  # RÃ©solution mobile
        'tracks': [
            {
                'id': t.id,
                'title': t.title,
                'artist': t.artist_name,  # DÃ©normalisÃ©
                'duration': t.duration_ms,
                'preview_url': t.preview_url  # PrÃ©-calculÃ©
            }
            for t in playlist.tracks[:50]  # Limit mobile
        ]
    }

# Web BFF : Format riche
@app.get('/web/playlist/{playlist_id}')
async def get_playlist_web(playlist_id: str):
    playlist = await playlist_service.get(playlist_id)

    # AgrÃ©gation de plusieurs services
    creator, recommendations, analytics = await asyncio.gather(
        user_service.get(playlist.creator_id),
        recommendation_service.get_similar(playlist_id),
        analytics_service.get_stats(playlist_id)
    )

    return {
        'id': playlist.id,
        'name': playlist.name,
        'description': playlist.description,
        'cover': f"{playlist.cover}_1200x1200.jpg",  # Haute rÃ©solution
        'creator': {
            'id': creator.id,
            'name': creator.name,
            'followers': creator.follower_count
        },
        'tracks': [
            # Format dÃ©taillÃ© avec relations
            {
                'id': t.id,
                'title': t.title,
                'album': {
                    'id': t.album.id,
                    'name': t.album.name,
                    'cover': t.album.cover
                },
                'artists': [
                    {'id': a.id, 'name': a.name}
                    for a in t.artists
                ],
                'duration': t.duration_ms,
                'explicit': t.explicit,
                'popularity': t.popularity
            }
            for t in playlist.tracks
        ],
        'recommendations': recommendations,
        'analytics': analytics
    }
```

## Anti-patterns Ã  Ã©viter

### 1. Distributed Monolith

```python
# âŒ MAUVAIS : Services trop couplÃ©s
"""
Order Service                Payment Service
     â”‚                             â”‚
     â”œâ”€â”€â–º get_payment_method() â”€â”€â”€â”€â”¤
     â”œâ”€â”€â–º validate_card() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”œâ”€â”€â–º calculate_fees() â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”œâ”€â”€â–º process_payment() â”€â”€â”€â”€â”€â”€â”€â”¤
     â””â”€â”€â–º send_receipt() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5 appels synchrones pour une commande !
â†’ Monolithe distribuÃ© (pire des deux mondes)
"""

# âœ… BON : Service autonome
"""
Order Service
     â”‚
     â””â”€â”€â–º process_order() â”€â”€â”€â”€â”€â”€â”
                                â”‚
                         Payment Service
                         (tout en un appel)
"""

@payment_service.post('/process-order-payment')
async def process_order_payment(order_data):
    # Tout dans un appel
    payment_method = get_payment_method(order_data['user_id'])
    validate_card(payment_method)
    fees = calculate_fees(order_data['amount'])
    payment = process_payment(payment_method, order_data['amount'] + fees)
    send_receipt(order_data['user_id'], payment)

    return payment
```

### 2. Shared Database

```python
# âŒ MAUVAIS : Database partagÃ©e entre services
"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Order Serviceâ”‚â”€â”€â”  â”Œâ”€â”‚User Serviceâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚  â”‚
                 â–¼  â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚Shared Databaseâ”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†’ Couplage au schÃ©ma
â†’ Impossible de scaler indÃ©pendamment
"""

# âœ… BON : Database par service
"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Order Serviceâ”‚       â”‚User Serviceâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                    â”‚
      â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Order DB â”‚         â”‚ User DB  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†’ IndÃ©pendance totale
â†’ ScalabilitÃ© indÃ©pendante
"""
```

### 3. Synchronous Everything

```python
# âŒ MAUVAIS : Tout en synchrone
@app.post('/orders')
async def create_order(order_data):
    user = await user_service.get(order_data['user_id'])  # 10ms
    inventory = await inventory_service.check(order_data['items'])  # 50ms
    payment = await payment_service.charge(order_data['payment'])  # 200ms

    # OpÃ©rations non-critiques mais bloquantes
    await email_service.send_confirmation(user.email)  # 500ms
    await analytics_service.track_order(order_data)  # 100ms
    await recommendation_service.update(user.id)  # 150ms

    return {"order_id": order.id}

# Total : 1010ms dont 750ms non-critiques !

# âœ… BON : Async pour opÃ©rations non-critiques
@app.post('/orders')
async def create_order(order_data):
    # Critiques : synchrone
    user = await user_service.get(order_data['user_id'])
    inventory = await inventory_service.check(order_data['items'])
    payment = await payment_service.charge(order_data['payment'])

    order = create_order_record(order_data)

    # Non-critiques : async (message queue)
    publish_event('order.created', {
        'order_id': order.id,
        'user_id': user.id,
        'email': user.email
    })

    return {"order_id": order.id}

# Total : 260ms (4Ã— plus rapide)
```

### 4. No Timeouts

```python
# âŒ MAUVAIS : Pas de timeout
response = requests.get('http://slow-service/data')
# Si service down/lent â†’ bloque indÃ©finiment

# âœ… BON : Timeout partout
response = requests.get(
    'http://slow-service/data',
    timeout=(2.0, 5.0)  # connect, read
)
```

## RÃ©capitulatif

### DÃ©fis clÃ©s

```
Challenge                   Solution
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Latence rÃ©seau              â€¢ Batching
                           â€¢ Caching
                           â€¢ Async communication

Failures rÃ©seau             â€¢ Timeouts
                           â€¢ Retry + backoff
                           â€¢ Circuit breakers
                           â€¢ Bulkheads

Service discovery           â€¢ Consul/Etcd
                           â€¢ Service mesh
                           â€¢ DNS

ObservabilitÃ©              â€¢ Distributed tracing
                           â€¢ Structured logging
                           â€¢ Metrics (Prometheus)

SÃ©curitÃ©                   â€¢ mTLS
                           â€¢ Service auth (JWT)
                           â€¢ Rate limiting

Ã‰volution                  â€¢ API versioning
                           â€¢ Backward compatibility
                           â€¢ Contract testing
```

### Checklist de conception

```python
# Avant de passer en microservices :

â–¡ Ã‰quipe prÃªte ? (10+ dÃ©veloppeurs)
â–¡ Besoin de scalabilitÃ© indÃ©pendante ?
â–¡ DÃ©ploiements indÃ©pendants nÃ©cessaires ?
â–¡ Infrast prÃªte ? (Kubernetes, monitoring)
â–¡ Accepter complexitÃ© opÃ©rationnelle ?

# Si non : Rester monolithe modulaire !

# DÃ©cisions par service :

â–¡ Communication : Sync (REST) ou Async (queue) ?
â–¡ Database : Propre ou partagÃ©e ?
â–¡ Resilience : Timeouts ? Retries ? Circuit breaker ?
â–¡ Observability : Tracing ? Logs ? Metrics ?
â–¡ Security : mTLS ? JWT ? Rate limiting ?
```

### Best Practices

âœ… **Design** :
- Services autonomes (database propre)
- APIs bien dÃ©finies
- Backward compatible

âœ… **Communication** :
- Async pour opÃ©rations non-critiques
- Batching quand possible
- Timeouts partout

âœ… **RÃ©silience** :
- Retry avec backoff
- Circuit breakers
- Graceful degradation

âœ… **ObservabilitÃ©** :
- Distributed tracing
- Structured logging
- MÃ©triques dÃ©taillÃ©es

âœ… **SÃ©curitÃ©** :
- mTLS entre services
- Service authentication
- Rate limiting

## Conclusion

Les microservices transforment des appels de fonctions en appels rÃ©seau. Cette transformation introduit :

- ğŸ“Š **Latence** : 10ms au lieu de 1Âµs
- ğŸ”¥ **Failures** : RÃ©seau peut tomber
- ğŸ” **ComplexitÃ©** : Debugging distribuÃ©
- ğŸ”’ **SÃ©curitÃ©** : Surface d'attaque plus large

Mais avec les bons patterns et outils, les microservices permettent :

- âœ¨ **ScalabilitÃ©** : Scaler services indÃ©pendamment
- ğŸš€ **Vitesse** : Ã‰quipes autonomes
- ğŸ”§ **FlexibilitÃ©** : Technologies par service
- ğŸ“ˆ **RÃ©silience** : Isolation des pannes

---


**Fin du Module 8 : Programmation rÃ©seau (perspective dÃ©veloppeur)**

â­ï¸ [9. Architectures et concepts avancÃ©s](/09-architectures-avancees/README.md)
