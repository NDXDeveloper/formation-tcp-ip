üîù Retour au [Sommaire](/SOMMAIRE.md)

# 8.7 Programmation asynchrone et event-driven

## Introduction

La **programmation asynchrone** repr√©sente un changement de paradigme fondamental dans la fa√ßon d'√©crire du code r√©seau. Au lieu d'attendre passivement qu'une op√©ration I/O se termine (bloquant), ou de g√©rer des milliers de threads (complexe), la programmation asynchrone permet d'√©crire du code **concurrent** qui reste **s√©quentiel** en apparence.

```python
# Code SYNCHRONE (bloquant)
def fetch_user(user_id):
    response = requests.get(f'/api/users/{user_id}')  # Bloque ~100ms
    return response.json()

def get_user_data(user_id):
    user = fetch_user(user_id)           # Attend 100ms
    posts = fetch_posts(user_id)         # Attend 100ms
    comments = fetch_comments(user_id)   # Attend 100ms
    return merge(user, posts, comments)
# Total : 300ms (s√©quentiel)

# Code ASYNCHRONE (non-bloquant)
async def fetch_user(user_id):
    response = await http.get(f'/api/users/{user_id}')  # Ne bloque pas
    return response.json()

async def get_user_data(user_id):
    # Les 3 requ√™tes en PARALL√àLE !
    user, posts, comments = await asyncio.gather(
        fetch_user(user_id),
        fetch_posts(user_id),
        fetch_comments(user_id)
    )
    return merge(user, posts, comments)
# Total : 100ms (parall√®le)
```

Cette section explore comment la programmation asynchrone fonctionne, quand l'utiliser, et comment construire des applications r√©seau performantes avec ce paradigme.

## Concepts fondamentaux

### Le probl√®me du blocage

```python
# Serveur HTTP synchrone (un seul thread)
def handle_request(request):
    # 1. Requ√™te base de donn√©es (50ms)
    user = db.query("SELECT * FROM users WHERE id=?", request.user_id)

    # 2. Appel API externe (100ms)
    profile = requests.get(f"https://api.example.com/profiles/{user.id}")

    # 3. Calcul (10ms)
    result = compute(user, profile)

    return result

# Pendant les 150ms d'I/O, le thread ne fait RIEN
# Il attend passivement
```

**Visualisation** :

```
Thread 1 :
[0-50ms]    DB query ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (BLOQU√â)
[50-150ms]  API call ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (BLOQU√â)
[150-160ms] Compute ‚ñà‚ñà (CPU)

Pendant 150ms sur 160ms, le thread est bloqu√© !
‚Üí CPU inutilis√©
‚Üí Ressources gaspill√©es
```

### La solution asynchrone

```python
# Version asynchrone
async def handle_request(request):
    # 1. Lancer la requ√™te DB (non-bloquant)
    user_task = asyncio.create_task(db.query("SELECT * FROM users WHERE id=?", request.user_id))

    # 2. Pendant ce temps, on peut faire autre chose
    # L'event loop peut traiter d'autres requ√™tes

    # 3. Attendre le r√©sultat quand on en a besoin
    user = await user_task

    # 4. Appel API (non-bloquant)
    profile = await http.get(f"https://api.example.com/profiles/{user.id}")

    # 5. Calcul
    result = compute(user, profile)

    return result
```

**Visualisation** :

```
Event Loop :
[0-50ms]    Requ√™te 1 : DB query (lanc√©e)
            Requ√™te 2 : commence pendant que Req1 attend
            Requ√™te 3 : commence pendant que Req1 & Req2 attendent
[50ms]      Requ√™te 1 : DB r√©pond, continue
[50-150ms]  Requ√™te 1 : API call (lanc√©e)
            Requ√™tes 2, 3, 4, 5... peuvent progresser
[150ms]     Requ√™te 1 : API r√©pond, termine

‚Üí UN SEUL THREAD g√®re PLUSIEURS requ√™tes simultan√©ment
‚Üí CPU toujours occup√©
‚Üí Pas d'attente passive
```

### Coroutines : Les fonctions async

Une **coroutine** est une fonction qui peut √™tre suspendue et reprise.

```python
import asyncio

# Fonction normale (ex√©cution continue)
def synchronous_function():
    result = do_something()  # S'ex√©cute jusqu'au bout
    return result

# Coroutine (peut √™tre suspendue)
async def asynchronous_function():
    result = await do_something_async()  # Peut √™tre suspendue ici
    return result

# Appel
# sync :
value = synchronous_function()  # Ex√©cution imm√©diate

# async :
coro = asynchronous_function()  # Retourne une coroutine (pas encore ex√©cut√©e)
value = await coro  # OU asyncio.run(coro)
```

**Points cl√©s** :

```python
# 1. async def cr√©e une coroutine
async def my_coroutine():
    return "hello"

# 2. Appeler une coroutine ne l'ex√©cute PAS
coro = my_coroutine()
print(type(coro))  # <class 'coroutine'>
# Elle n'a pas encore run !

# 3. Il faut await ou run()
result = await my_coroutine()  # Dans un contexte async

# OU
result = asyncio.run(my_coroutine())  # √Ä la racine
```

### await : Le point de suspension

```python
import asyncio

async def fetch_data():
    print("1. D√©but fetch_data")

    # await SUSPEND la coroutine
    await asyncio.sleep(1)  # Point de suspension

    print("2. Apr√®s sleep")
    return "data"

async def main():
    print("A. Avant fetch")

    data = await fetch_data()  # Attend ici

    print("B. Apr√®s fetch")
    print(f"Data: {data}")

asyncio.run(main())

# Output :
# A. Avant fetch
# 1. D√©but fetch_data
# [pause de 1 seconde - event loop traite autres t√¢ches]
# 2. Apr√®s sleep
# B. Apr√®s fetch
# Data: data
```

**Que se passe-t-il lors de await ?**

```
1. Coroutine s'ex√©cute jusqu'√† await
2. await SUSPEND la coroutine
3. Event loop prend le contr√¥le
4. Event loop peut ex√©cuter d'AUTRES coroutines
5. Quand l'op√©ration async est pr√™te, event loop REPREND la coroutine
6. La coroutine continue apr√®s le await
```

## L'Event Loop

### Qu'est-ce qu'un event loop ?

L'**event loop** est le c≈ìur de la programmation asynchrone. C'est une boucle infinie qui :

1. V√©rifie quelles coroutines sont pr√™tes
2. Ex√©cute les coroutines pr√™tes
3. G√®re les √©v√©nements I/O (sockets, timers, etc.)
4. R√©p√®te

```python
# Pseudo-code simplifi√© d'un event loop
class EventLoop:
    def __init__(self):
        self.ready_queue = []      # Coroutines pr√™tes
        self.waiting = {}          # Coroutines en attente
        self.selector = select.epoll()  # Multiplexage I/O

    def run_forever(self):
        while True:
            # 1. Traiter les coroutines pr√™tes
            while self.ready_queue:
                coro = self.ready_queue.pop(0)
                self.step(coro)

            # 2. Attendre des √©v√©nements I/O
            events = self.selector.poll(timeout=0.01)

            # 3. R√©veiller les coroutines dont l'I/O est pr√™te
            for fd, event in events:
                coro = self.waiting.pop(fd)
                self.ready_queue.append(coro)

    def step(self, coro):
        """Ex√©cute une coroutine jusqu'au prochain await"""
        try:
            # Avancer la coroutine
            future = coro.send(None)

            # Si elle attend, la mettre en attente
            if isinstance(future, IOFuture):
                self.waiting[future.fd] = coro
            else:
                # Pr√™te √† continuer
                self.ready_queue.append(coro)

        except StopIteration:
            # Coroutine termin√©e
            pass
```

### Exemple concret d'event loop

```python
import asyncio

async def task1():
    print("Task1: D√©but")
    await asyncio.sleep(0.1)
    print("Task1: Milieu")
    await asyncio.sleep(0.1)
    print("Task1: Fin")

async def task2():
    print("Task2: D√©but")
    await asyncio.sleep(0.15)
    print("Task2: Fin")

async def main():
    # Lancer les deux t√¢ches en parall√®le
    await asyncio.gather(task1(), task2())

asyncio.run(main())

# Output :
# Task1: D√©but
# Task2: D√©but
# Task1: Milieu      (apr√®s 0.1s)
# Task2: Fin         (apr√®s 0.15s)
# Task1: Fin         (apr√®s 0.2s)
```

**Timeline de l'event loop** :

```
t=0ms:
  Event loop d√©marre
  Task1 s'ex√©cute jusqu'√† await sleep(0.1)
  Task2 s'ex√©cute jusqu'√† await sleep(0.15)
  Event loop: Task1 pr√™te √† 100ms, Task2 pr√™te √† 150ms

t=100ms:
  Event loop r√©veille Task1
  Task1 continue jusqu'√† await sleep(0.1)
  Event loop: Task1 pr√™te √† 200ms

t=150ms:
  Event loop r√©veille Task2
  Task2 termine

t=200ms:
  Event loop r√©veille Task1
  Task1 termine
```

## asyncio en Python

### Bases d'asyncio

```python
import asyncio

# 1. Fonction async basique
async def say_hello():
    print("Hello")
    await asyncio.sleep(1)
    print("World")

# 2. Ex√©cuter
asyncio.run(say_hello())

# 3. Cr√©er des t√¢ches
async def main():
    # Cr√©er une t√¢che (s'ex√©cute en arri√®re-plan)
    task = asyncio.create_task(say_hello())

    # Faire autre chose
    await asyncio.sleep(0.5)

    # Attendre la fin de la t√¢che
    await task

asyncio.run(main())
```

### asyncio.gather() - Ex√©cution parall√®le

```python
import asyncio
import time

async def fetch_data(id, delay):
    """Simule une requ√™te r√©seau"""
    print(f"Fetching {id}...")
    await asyncio.sleep(delay)
    print(f"Fetched {id}")
    return f"data_{id}"

async def sequential():
    """Ex√©cution s√©quentielle"""
    start = time.time()

    data1 = await fetch_data(1, 1.0)
    data2 = await fetch_data(2, 1.0)
    data3 = await fetch_data(3, 1.0)

    duration = time.time() - start
    print(f"Sequential: {duration:.2f}s")  # ~3s

async def parallel():
    """Ex√©cution parall√®le avec gather"""
    start = time.time()

    # Toutes en parall√®le !
    results = await asyncio.gather(
        fetch_data(1, 1.0),
        fetch_data(2, 1.0),
        fetch_data(3, 1.0)
    )

    duration = time.time() - start
    print(f"Parallel: {duration:.2f}s")  # ~1s
    print(f"Results: {results}")

# Test
asyncio.run(sequential())  # 3 secondes
asyncio.run(parallel())    # 1 seconde
```

**Cas d'usage r√©el : agr√©gation d'APIs**

```python
import asyncio
import aiohttp

async def fetch_user_data(user_id):
    """R√©cup√®re les donn√©es d'un utilisateur depuis plusieurs APIs"""
    async with aiohttp.ClientSession() as session:
        # Lancer toutes les requ√™tes en parall√®le
        tasks = [
            fetch_profile(session, user_id),
            fetch_posts(session, user_id),
            fetch_followers(session, user_id),
            fetch_stats(session, user_id)
        ]

        # Attendre toutes les r√©ponses
        profile, posts, followers, stats = await asyncio.gather(*tasks)

        return {
            'profile': profile,
            'posts': posts,
            'followers': followers,
            'stats': stats
        }

async def fetch_profile(session, user_id):
    async with session.get(f'https://api.example.com/users/{user_id}') as resp:
        return await resp.json()

async def fetch_posts(session, user_id):
    async with session.get(f'https://api.example.com/users/{user_id}/posts') as resp:
        return await resp.json()

async def fetch_followers(session, user_id):
    async with session.get(f'https://api.example.com/users/{user_id}/followers') as resp:
        return await resp.json()

async def fetch_stats(session, user_id):
    async with session.get(f'https://api.example.com/users/{user_id}/stats') as resp:
        return await resp.json()

# Utilisation
async def main():
    data = await fetch_user_data(123)
    print(data)

asyncio.run(main())

# Au lieu de 4 √ó 100ms = 400ms (s√©quentiel)
# On obtient 1 √ó 100ms = 100ms (parall√®le)
```

### asyncio.create_task() - T√¢ches en arri√®re-plan

```python
import asyncio

async def background_task():
    """T√¢che qui tourne en arri√®re-plan"""
    while True:
        print("Background task running...")
        await asyncio.sleep(2)

async def main_task():
    """T√¢che principale"""
    # Lancer la t√¢che en arri√®re-plan
    bg_task = asyncio.create_task(background_task())

    # Faire le travail principal
    for i in range(5):
        print(f"Main task: {i}")
        await asyncio.sleep(1)

    # Annuler la t√¢che d'arri√®re-plan
    bg_task.cancel()

    try:
        await bg_task
    except asyncio.CancelledError:
        print("Background task cancelled")

asyncio.run(main_task())

# Output :
# Background task running...
# Main task: 0
# Main task: 1
# Background task running...
# Main task: 2
# Main task: 3
# Background task running...
# Main task: 4
# Background task cancelled
```

**Cas d'usage : Heartbeat**

```python
import asyncio

class WebSocketClient:
    """Client WebSocket avec heartbeat automatique"""

    def __init__(self, url):
        self.url = url
        self.ws = None
        self.heartbeat_task = None

    async def connect(self):
        """Connecte et d√©marre le heartbeat"""
        self.ws = await websockets.connect(self.url)

        # Lancer le heartbeat en arri√®re-plan
        self.heartbeat_task = asyncio.create_task(self._heartbeat())

    async def _heartbeat(self):
        """Envoie un ping toutes les 30 secondes"""
        while True:
            try:
                await asyncio.sleep(30)
                await self.ws.ping()
                print("Heartbeat sent")
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"Heartbeat error: {e}")
                break

    async def send(self, message):
        """Envoie un message"""
        await self.ws.send(message)

    async def receive(self):
        """Re√ßoit un message"""
        return await self.ws.recv()

    async def close(self):
        """Ferme la connexion et arr√™te le heartbeat"""
        if self.heartbeat_task:
            self.heartbeat_task.cancel()
            try:
                await self.heartbeat_task
            except asyncio.CancelledError:
                pass

        if self.ws:
            await self.ws.close()

# Utilisation
async def main():
    client = WebSocketClient('ws://example.com')
    await client.connect()

    # Le heartbeat tourne en arri√®re-plan
    await client.send("Hello")
    response = await client.receive()

    await client.close()
```

### Timeouts avec asyncio

```python
import asyncio

async def slow_operation():
    """Op√©ration qui prend du temps"""
    await asyncio.sleep(10)
    return "Done"

async def with_timeout():
    """Op√©ration avec timeout"""
    try:
        # Timeout de 3 secondes
        result = await asyncio.wait_for(slow_operation(), timeout=3.0)
        print(f"Result: {result}")

    except asyncio.TimeoutError:
        print("Operation timed out!")

asyncio.run(with_timeout())
# Output : Operation timed out! (apr√®s 3s)
```

**Cas d'usage : Requ√™te HTTP avec timeout**

```python
import asyncio
import aiohttp

async def fetch_with_timeout(url, timeout=5.0):
    """Requ√™te HTTP avec timeout"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=timeout)) as resp:
                return await resp.text()

    except asyncio.TimeoutError:
        print(f"Request to {url} timed out")
        return None

    except Exception as e:
        print(f"Error: {e}")
        return None

# Utilisation
async def main():
    # Ces requ√™tes s'ex√©cutent en parall√®le
    results = await asyncio.gather(
        fetch_with_timeout('https://fast-api.com', timeout=2.0),
        fetch_with_timeout('https://slow-api.com', timeout=2.0),
        fetch_with_timeout('https://medium-api.com', timeout=2.0)
    )

    for i, result in enumerate(results):
        if result:
            print(f"API {i+1}: Success ({len(result)} bytes)")
        else:
            print(f"API {i+1}: Failed")

asyncio.run(main())
```

## Serveur asynchrone complet

### Serveur TCP echo asynchrone

```python
import asyncio

class AsyncTCPServer:
    """Serveur TCP asynchrone"""

    def __init__(self, host='0.0.0.0', port=8888):
        self.host = host
        self.port = port
        self.clients = set()

    async def handle_client(self, reader, writer):
        """G√®re un client (coroutine par client)"""
        addr = writer.get_extra_info('peername')
        print(f"[+] Client connect√©: {addr}")

        self.clients.add(writer)

        try:
            while True:
                # Lire des donn√©es (non-bloquant)
                data = await reader.read(1024)

                if not data:
                    # Client d√©connect√©
                    break

                message = data.decode('utf-8')
                print(f"[{addr}] Re√ßu: {message.strip()}")

                # Echo (non-bloquant)
                writer.write(b"Echo: " + data)
                await writer.drain()

        except Exception as e:
            print(f"Erreur avec {addr}: {e}")

        finally:
            print(f"[-] Client d√©connect√©: {addr}")
            self.clients.remove(writer)
            writer.close()
            await writer.wait_closed()

    async def start(self):
        """D√©marre le serveur"""
        server = await asyncio.start_server(
            self.handle_client,
            self.host,
            self.port
        )

        addr = server.sockets[0].getsockname()
        print(f"Serveur async d√©marr√© sur {addr}")

        async with server:
            await server.serve_forever()

# Lancement
async def main():
    server = AsyncTCPServer()
    await server.start()

if __name__ == "__main__":
    asyncio.run(main())
```

**Points cl√©s** :

```python
# 1. asyncio.start_server() g√®re l'accept() automatiquement
server = await asyncio.start_server(handler, host, port)

# 2. Une coroutine par client (pas de thread !)
async def handle_client(reader, writer):
    # Chaque client a sa propre coroutine
    pass

# 3. reader/writer abstractions
data = await reader.read(1024)    # Lecture async
writer.write(data)                # √âcriture bufferis√©e
await writer.drain()              # Flush async
```

### Serveur de chat asynchrone

```python
import asyncio

class AsyncChatServer:
    """Serveur de chat asynchrone multi-clients"""

    def __init__(self, host='0.0.0.0', port=9999):
        self.host = host
        self.port = port
        self.clients = {}  # {writer: {'addr': ..., 'name': ...}}

    async def handle_client(self, reader, writer):
        """G√®re un client"""
        addr = writer.get_extra_info('peername')

        # Demander le pseudo
        writer.write(b"Entrez votre pseudo: ")
        await writer.drain()

        name_data = await reader.readline()
        name = name_data.decode('utf-8').strip()

        # Enregistrer le client
        self.clients[writer] = {'addr': addr, 'name': name}

        # Message de bienvenue
        welcome = f"Bienvenue {name}! {len(self.clients)} utilisateurs connect√©s.\n"
        writer.write(welcome.encode('utf-8'))
        await writer.drain()

        # Annoncer l'arriv√©e
        await self.broadcast(f"{name} a rejoint le chat!\n", exclude=writer)

        try:
            while True:
                # Lire un message
                data = await reader.readline()

                if not data:
                    break

                message = data.decode('utf-8').strip()

                if message:
                    # Diffuser le message
                    formatted = f"[{name}] {message}\n"
                    await self.broadcast(formatted, exclude=writer)

        except Exception as e:
            print(f"Erreur avec {name}: {e}")

        finally:
            # Retirer le client
            del self.clients[writer]
            writer.close()
            await writer.wait_closed()

            # Annoncer le d√©part
            await self.broadcast(f"{name} a quitt√© le chat.\n")
            print(f"{name} ({addr}) d√©connect√©")

    async def broadcast(self, message, exclude=None):
        """Diffuse un message √† tous les clients"""
        # Liste des t√¢ches d'envoi
        tasks = []

        for writer in self.clients:
            if writer != exclude:
                # Cr√©er une t√¢che pour chaque client
                tasks.append(self.send_to_client(writer, message))

        # Envoyer √† tous en parall√®le
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)

    async def send_to_client(self, writer, message):
        """Envoie un message √† un client"""
        try:
            writer.write(message.encode('utf-8'))
            await writer.drain()
        except Exception as e:
            # Erreur d'envoi (client d√©connect√© ?)
            print(f"Erreur d'envoi: {e}")

    async def start(self):
        """D√©marre le serveur"""
        server = await asyncio.start_server(
            self.handle_client,
            self.host,
            self.port
        )

        print(f"Serveur de chat sur {self.host}:{self.port}")

        async with server:
            await server.serve_forever()

# Lancement
if __name__ == "__main__":
    server = AsyncChatServer()
    asyncio.run(server.start())
```

**Test** :

```bash
# Terminal 1 : Serveur
python async_chat_server.py

# Terminal 2 : Client 1
telnet localhost 9999
Entrez votre pseudo: Alice
Bienvenue Alice! 1 utilisateurs connect√©s.
Hello everyone!

# Terminal 3 : Client 2
telnet localhost 9999
Entrez votre pseudo: Bob
Bienvenue Bob! 2 utilisateurs connect√©s.
Alice a rejoint le chat!
[Alice] Hello everyone!
Hi Alice!

# Dans Terminal 2 (Alice voit) :
Bob a rejoint le chat!
[Bob] Hi Alice!
```

## Gestion des erreurs en async

### try/except avec async

```python
import asyncio

async def risky_operation():
    """Op√©ration qui peut √©chouer"""
    await asyncio.sleep(0.1)
    raise ValueError("Something went wrong!")

async def safe_caller():
    """Appel s√©curis√©"""
    try:
        result = await risky_operation()
    except ValueError as e:
        print(f"Caught error: {e}")
        result = None

    return result

asyncio.run(safe_caller())
```

### Gestion d'erreurs avec gather()

```python
import asyncio

async def task_success():
    await asyncio.sleep(0.1)
    return "success"

async def task_failure():
    await asyncio.sleep(0.1)
    raise ValueError("failed")

async def handle_multiple_tasks():
    """G√®re plusieurs t√¢ches avec erreurs potentielles"""

    # Option 1 : Propager la premi√®re erreur (d√©faut)
    try:
        results = await asyncio.gather(
            task_success(),
            task_failure(),
            task_success()
        )
    except ValueError as e:
        print(f"Une t√¢che a √©chou√©: {e}")

    # Option 2 : Retourner les exceptions (ne pas lever)
    results = await asyncio.gather(
        task_success(),
        task_failure(),
        task_success(),
        return_exceptions=True  # Important !
    )

    # Traiter les r√©sultats et erreurs
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"T√¢che {i} a √©chou√©: {result}")
        else:
            print(f"T√¢che {i} a r√©ussi: {result}")

asyncio.run(handle_multiple_tasks())

# Output :
# Une t√¢che a √©chou√©: failed
# T√¢che 0 a r√©ussi: success
# T√¢che 1 a √©chou√©: failed
# T√¢che 2 a r√©ussi: success
```

**Cas d'usage : Agr√©gation d'APIs avec fallback**

```python
import asyncio
import aiohttp

async def fetch_with_fallback(urls):
    """Essaie plusieurs URLs, retourne la premi√®re qui r√©ussit"""

    async def try_url(url):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=2)) as resp:
                    return await resp.json()
        except Exception as e:
            return e  # Retourner l'exception

    # Essayer toutes les URLs en parall√®le
    results = await asyncio.gather(
        *[try_url(url) for url in urls],
        return_exceptions=True
    )

    # Retourner le premier succ√®s
    for result in results:
        if not isinstance(result, Exception):
            return result

    # Toutes ont √©chou√©
    raise Exception("All APIs failed")

# Utilisation
async def main():
    urls = [
        'https://api1.example.com/data',
        'https://api2.example.com/data',
        'https://api3.example.com/data'
    ]

    try:
        data = await fetch_with_fallback(urls)
        print(f"Success: {data}")
    except Exception as e:
        print(f"All failed: {e}")
```

### Annulation de t√¢ches

```python
import asyncio

async def long_running_task():
    """T√¢che longue qui peut √™tre annul√©e"""
    try:
        for i in range(100):
            print(f"Working... {i}")
            await asyncio.sleep(0.5)
    except asyncio.CancelledError:
        print("Task was cancelled!")
        # Cleanup
        raise  # Important : propager l'annulation

async def main():
    # Cr√©er la t√¢che
    task = asyncio.create_task(long_running_task())

    # Laisser tourner 2 secondes
    await asyncio.sleep(2)

    # Annuler
    task.cancel()

    try:
        await task
    except asyncio.CancelledError:
        print("Confirmed: task cancelled")

asyncio.run(main())
```

**Cas d'usage : Timeout avec cleanup**

```python
import asyncio

async def download_file(url, filepath):
    """T√©l√©charge un fichier avec possibilit√© d'annulation"""
    file = None
    try:
        file = open(filepath, 'wb')

        async with aiohttp.ClientSession() as session:
            async with session.get(url) as resp:
                async for chunk in resp.content.iter_chunked(8192):
                    file.write(chunk)
                    # Permet l'annulation entre les chunks
                    await asyncio.sleep(0)

        print(f"Download complete: {filepath}")

    except asyncio.CancelledError:
        print(f"Download cancelled: {filepath}")
        # Cleanup : supprimer le fichier partiel
        if file:
            file.close()
            os.remove(filepath)
        raise

    finally:
        if file:
            file.close()

async def download_with_timeout(url, filepath, timeout=30.0):
    """T√©l√©charge avec timeout"""
    try:
        await asyncio.wait_for(
            download_file(url, filepath),
            timeout=timeout
        )
    except asyncio.TimeoutError:
        print(f"Download timeout: {url}")
```

## Async vs Sync vs Threads : Comparaison

### Benchmark : 1000 requ√™tes HTTP

```python
import time
import asyncio
import aiohttp
import requests
from concurrent.futures import ThreadPoolExecutor

# 1. SYNCHRONE (s√©quentiel)
def sync_fetch(url):
    response = requests.get(url, timeout=5)
    return len(response.content)

def benchmark_sync(urls):
    start = time.time()

    results = []
    for url in urls:
        result = sync_fetch(url)
        results.append(result)

    duration = time.time() - start
    print(f"Sync: {duration:.2f}s")
    return results

# 2. THREADS
def benchmark_threads(urls):
    start = time.time()

    with ThreadPoolExecutor(max_workers=50) as executor:
        results = list(executor.map(sync_fetch, urls))

    duration = time.time() - start
    print(f"Threads: {duration:.2f}s")
    return results

# 3. ASYNC
async def async_fetch(session, url):
    async with session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as resp:
        content = await resp.read()
        return len(content)

async def benchmark_async(urls):
    start = time.time()

    async with aiohttp.ClientSession() as session:
        tasks = [async_fetch(session, url) for url in urls]
        results = await asyncio.gather(*tasks)

    duration = time.time() - start
    print(f"Async: {duration:.2f}s")
    return results

# Test
urls = ['http://httpbin.org/delay/0.1'] * 100

# Sync : ~10s (s√©quentiel, 100 √ó 0.1s)
benchmark_sync(urls)

# Threads : ~2s (parall√®le, mais overhead threads)
benchmark_threads(urls)

# Async : ~0.5s (parall√®le, pas d'overhead)
asyncio.run(benchmark_async(urls))
```

**R√©sultats typiques** :

```
100 requ√™tes de 100ms chacune :

Synchrone : 10.0s
  - S√©quentiel : 100 √ó 0.1s
  - 1 CPU core utilis√©

Threads (50 workers) : 2.5s
  - Parall√®le : 100 / 50 √ó 0.1s
  - Overhead : context switching, m√©moire
  - 50 threads √ó 8 MB = 400 MB RAM

Async : 0.5s
  - Parall√®le : toutes en m√™me temps
  - Pas d'overhead
  - 1 thread, ~10 MB RAM
```

### Tableau comparatif

| Aspect | Synchrone | Threads | Async |
|--------|-----------|---------|-------|
| **Simplicit√© code** | ‚úÖ Tr√®s simple | ‚ö†Ô∏è Moyen | ‚ùå Plus complexe |
| **Scalabilit√©** | ‚ùå 1 requ√™te/fois | ‚ö†Ô∏è ~1000 threads max | ‚úÖ 10000+ |
| **RAM** | ‚úÖ Minimal | ‚ùå ~8 MB/thread | ‚úÖ Minimal |
| **CPU usage** | ‚ö†Ô∏è 1 core max | ‚úÖ Multi-core | ‚ö†Ô∏è 1 core |
| **I/O-bound** | ‚ùå Tr√®s lent | ‚ö†Ô∏è OK | ‚úÖ Excellent |
| **CPU-bound** | ‚úÖ OK | ‚úÖ Excellent | ‚ùå Bloque event loop |
| **Debugging** | ‚úÖ Facile | ‚ùå Race conditions | ‚ö†Ô∏è Moyen |

### Quand utiliser quoi ?

#### Synchrone

‚úÖ **Scripts simples** : One-shot, prototypes

```python
# Script simple de scraping
def scrape_page(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content)
    return soup.find_all('a')

# Acceptable pour quelques pages
for url in urls:
    links = scrape_page(url)
```

‚úÖ **Peu de requ√™tes** : < 10 appels r√©seau

‚úÖ **Code legacy** : Int√©gration avec biblioth√®ques bloquantes

#### Threads

‚úÖ **CPU-bound + I/O** : Calculs lourds + r√©seau

```python
def process_image(url):
    # 1. Download (I/O-bound)
    response = requests.get(url)

    # 2. Process (CPU-bound)
    image = Image.open(BytesIO(response.content))
    processed = apply_filters(image)  # CPU intensif

    return processed

# Threads utilisent plusieurs cores pour CPU-bound
with ThreadPoolExecutor(max_workers=8) as executor:
    results = executor.map(process_image, urls)
```

‚úÖ **Biblioth√®ques bloquantes** : Pas de version async disponible

‚úÖ **< 1000 op√©rations parall√®les**

#### Async

‚úÖ **I/O-bound pur** : Appels r√©seau, base de donn√©es

```python
async def fetch_all_data():
    # Des milliers de requ√™tes r√©seau
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session, url) for url in 10000_urls]
        return await asyncio.gather(*tasks)
```

‚úÖ **Serveurs haute performance** : Web servers, APIs

‚úÖ **WebSockets, temps r√©el** : Beaucoup de connexions longues

‚úÖ **Microservices** : Communication service-to-service

## Cas d'usage r√©els d√©taill√©s

### 1. Web Scraper asynchrone

```python
import asyncio
import aiohttp
from bs4 import BeautifulSoup

class AsyncWebScraper:
    """Web scraper asynchrone haute performance"""

    def __init__(self, max_concurrent=100):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)

    async def fetch_page(self, session, url):
        """R√©cup√®re une page"""
        async with self.semaphore:  # Limite la concurrence
            try:
                async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                    if resp.status == 200:
                        html = await resp.text()
                        return url, html
                    else:
                        return url, None
            except Exception as e:
                print(f"Error fetching {url}: {e}")
                return url, None

    async def parse_page(self, url, html):
        """Parse une page HTML"""
        if not html:
            return []

        # Parser avec BeautifulSoup (CPU-bound, mais rapide)
        soup = BeautifulSoup(html, 'html.parser')

        # Extraire les liens
        links = []
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.startswith('http'):
                links.append(href)

        return links

    async def scrape(self, start_urls, max_pages=1000):
        """Scrape des pages web"""
        to_visit = set(start_urls)
        visited = set()
        results = {}

        async with aiohttp.ClientSession() as session:
            while to_visit and len(visited) < max_pages:
                # Prendre un batch d'URLs
                batch_size = min(self.max_concurrent, len(to_visit))
                batch = [to_visit.pop() for _ in range(batch_size)]

                # Fetch en parall√®le
                fetch_tasks = [
                    self.fetch_page(session, url)
                    for url in batch
                ]
                pages = await asyncio.gather(*fetch_tasks)

                # Parse en parall√®le
                parse_tasks = [
                    self.parse_page(url, html)
                    for url, html in pages
                ]
                all_links = await asyncio.gather(*parse_tasks)

                # Traiter les r√©sultats
                for (url, html), links in zip(pages, all_links):
                    visited.add(url)
                    results[url] = {
                        'success': html is not None,
                        'links_found': len(links)
                    }

                    # Ajouter les nouveaux liens
                    for link in links:
                        if link not in visited and link not in to_visit:
                            to_visit.add(link)

                print(f"Scraped: {len(visited)}/{max_pages}, Queue: {len(to_visit)}")

        return results

# Utilisation
async def main():
    scraper = AsyncWebScraper(max_concurrent=50)

    results = await scraper.scrape(
        start_urls=['https://example.com'],
        max_pages=500
    )

    print(f"Total pages scraped: {len(results)}")
    successful = sum(1 for r in results.values() if r['success'])
    print(f"Successful: {successful}")

asyncio.run(main())

# Performance :
# 500 pages en ~30 secondes
# vs 500+ secondes en synchrone
```

### 2. Proxy HTTP asynchrone

```python
import asyncio

class AsyncHTTPProxy:
    """Proxy HTTP asynchrone"""

    def __init__(self, host='0.0.0.0', port=8080):
        self.host = host
        self.port = port

    async def handle_client(self, client_reader, client_writer):
        """G√®re une connexion client"""
        client_addr = client_writer.get_extra_info('peername')
        print(f"[+] Client: {client_addr}")

        try:
            # Lire la requ√™te HTTP du client
            request = await client_reader.read(4096)

            if not request:
                return

            # Parser la requ√™te pour extraire l'h√¥te
            lines = request.decode('utf-8').split('\r\n')
            first_line = lines[0]  # GET /path HTTP/1.1

            # Trouver le Host header
            host = None
            for line in lines[1:]:
                if line.lower().startswith('host:'):
                    host = line.split(':', 1)[1].strip()
                    break

            if not host:
                client_writer.write(b'HTTP/1.1 400 Bad Request\r\n\r\n')
                await client_writer.drain()
                return

            # Connecter au serveur distant
            print(f"  ‚Üí Connecting to {host}")
            remote_reader, remote_writer = await asyncio.open_connection(
                host, 80
            )

            # Forward la requ√™te
            remote_writer.write(request)
            await remote_writer.drain()

            # Bidirectional proxy
            await asyncio.gather(
                self.forward(client_reader, remote_writer, 'client‚Üíremote'),
                self.forward(remote_reader, client_writer, 'remote‚Üíclient')
            )

        except Exception as e:
            print(f"Error: {e}")

        finally:
            client_writer.close()
            await client_writer.wait_closed()

    async def forward(self, reader, writer, direction):
        """Forward les donn√©es dans une direction"""
        try:
            while True:
                data = await reader.read(8192)

                if not data:
                    break

                writer.write(data)
                await writer.drain()

                print(f"  {direction}: {len(data)} bytes")

        except Exception as e:
            print(f"Forward error ({direction}): {e}")

        finally:
            writer.close()
            await writer.wait_closed()

    async def start(self):
        """D√©marre le proxy"""
        server = await asyncio.start_server(
            self.handle_client,
            self.host,
            self.port
        )

        print(f"Proxy HTTP on {self.host}:{self.port}")

        async with server:
            await server.serve_forever()

# Lancement
if __name__ == "__main__":
    proxy = AsyncHTTPProxy()
    asyncio.run(proxy.start())

# Test :
# curl -x http://localhost:8080 http://example.com
```

### 3. Task Queue asynchrone (type Celery)

```python
import asyncio
import json
from datetime import datetime

class AsyncTaskQueue:
    """File de t√¢ches asynchrone"""

    def __init__(self):
        self.queue = asyncio.Queue()
        self.results = {}
        self.workers = []

    async def enqueue(self, task_id, task_func, *args, **kwargs):
        """Ajoute une t√¢che √† la queue"""
        await self.queue.put({
            'id': task_id,
            'func': task_func,
            'args': args,
            'kwargs': kwargs,
            'enqueued_at': datetime.now()
        })
        print(f"[QUEUE] Task {task_id} enqueued")

    async def worker(self, worker_id):
        """Worker qui traite les t√¢ches"""
        print(f"[WORKER-{worker_id}] Started")

        while True:
            # R√©cup√©rer une t√¢che
            task = await self.queue.get()

            task_id = task['id']
            print(f"[WORKER-{worker_id}] Processing task {task_id}")

            try:
                # Ex√©cuter la t√¢che
                start = datetime.now()
                result = await task['func'](*task['args'], **task['kwargs'])
                duration = (datetime.now() - start).total_seconds()

                # Stocker le r√©sultat
                self.results[task_id] = {
                    'status': 'success',
                    'result': result,
                    'duration': duration,
                    'worker': worker_id
                }

                print(f"[WORKER-{worker_id}] Task {task_id} completed in {duration:.2f}s")

            except Exception as e:
                # Erreur
                self.results[task_id] = {
                    'status': 'error',
                    'error': str(e),
                    'worker': worker_id
                }

                print(f"[WORKER-{worker_id}] Task {task_id} failed: {e}")

            finally:
                self.queue.task_done()

    async def start_workers(self, num_workers=4):
        """D√©marre les workers"""
        for i in range(num_workers):
            worker = asyncio.create_task(self.worker(i))
            self.workers.append(worker)

    async def wait_completion(self):
        """Attend que toutes les t√¢ches soient termin√©es"""
        await self.queue.join()

    def get_result(self, task_id):
        """R√©cup√®re le r√©sultat d'une t√¢che"""
        return self.results.get(task_id)

# Exemples de t√¢ches
async def send_email(to, subject, body):
    """Simule l'envoi d'email"""
    await asyncio.sleep(1)  # Simule latence SMTP
    return f"Email sent to {to}"

async def process_image(image_url):
    """Simule traitement d'image"""
    await asyncio.sleep(2)  # Simule traitement
    return f"Image processed: {image_url}"

async def fetch_data(api_url):
    """Simule fetch API"""
    await asyncio.sleep(0.5)
    return f"Data from {api_url}"

# Utilisation
async def main():
    # Cr√©er la queue
    queue = AsyncTaskQueue()

    # D√©marrer 4 workers
    await queue.start_workers(num_workers=4)

    # Enqueue des t√¢ches
    await queue.enqueue('email-1', send_email, 'user@example.com', 'Hello', 'Body')
    await queue.enqueue('image-1', process_image, 'http://example.com/img.jpg')
    await queue.enqueue('api-1', fetch_data, 'https://api.example.com/data')
    await queue.enqueue('email-2', send_email, 'admin@example.com', 'Report', 'Stats')

    # En ajouter plus
    for i in range(10):
        await queue.enqueue(f'task-{i}', fetch_data, f'http://api.com/{i}')

    # Attendre la fin
    await queue.wait_completion()

    # Afficher les r√©sultats
    print("\n=== Results ===")
    for task_id, result in queue.results.items():
        print(f"{task_id}: {result['status']} ({result.get('duration', 0):.2f}s)")

asyncio.run(main())
```

### 4. Rate Limiter asynchrone

```python
import asyncio
import time
from collections import deque

class AsyncRateLimiter:
    """Rate limiter asynchrone (Token Bucket)"""

    def __init__(self, rate, per):
        """
        rate: nombre de requ√™tes
        per: p√©riode en secondes

        Exemple: RateLimiter(10, 1.0) = 10 req/sec
        """
        self.rate = rate
        self.per = per
        self.allowance = rate
        self.last_check = time.time()
        self.lock = asyncio.Lock()

    async def acquire(self):
        """Acquiert un token (bloque si rate limit atteint)"""
        async with self.lock:
            current = time.time()
            time_passed = current - self.last_check
            self.last_check = current

            # Ajouter des tokens bas√© sur le temps pass√©
            self.allowance += time_passed * (self.rate / self.per)

            if self.allowance > self.rate:
                self.allowance = self.rate  # Cap

            if self.allowance < 1.0:
                # Pas de token disponible, calculer le temps d'attente
                sleep_time = (1.0 - self.allowance) * (self.per / self.rate)
                await asyncio.sleep(sleep_time)
                self.allowance = 0.0
            else:
                self.allowance -= 1.0

# Utilisation
async def api_call(limiter, api_id):
    """Fait un appel API avec rate limiting"""
    await limiter.acquire()

    print(f"[{time.time():.2f}] API call {api_id}")
    # Faire l'appel r√©el
    await asyncio.sleep(0.1)

async def main():
    # 5 requ√™tes par seconde max
    limiter = AsyncRateLimiter(rate=5, per=1.0)

    # Faire 20 requ√™tes
    tasks = [api_call(limiter, i) for i in range(20)]
    await asyncio.gather(*tasks)

asyncio.run(main())

# Output :
# [0.00] API call 0
# [0.00] API call 1
# [0.00] API call 2
# [0.00] API call 3
# [0.00] API call 4
# [0.20] API call 5  ‚Üê Attend pour respecter le rate
# [0.40] API call 6
# [0.60] API call 7
# ...
```

## Patterns avanc√©s

### 1. Async Context Managers

```python
import asyncio

class AsyncDatabaseConnection:
    """Connexion DB asynchrone avec context manager"""

    def __init__(self, host, port):
        self.host = host
        self.port = port
        self.connection = None

    async def __aenter__(self):
        """Appel√© lors du 'async with'"""
        print(f"Connecting to {self.host}:{self.port}...")
        await asyncio.sleep(0.1)  # Simule connexion
        self.connection = f"Connection to {self.host}"
        print("Connected!")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Appel√© √† la sortie du 'async with'"""
        print("Closing connection...")
        await asyncio.sleep(0.1)  # Simule fermeture
        self.connection = None
        print("Connection closed!")

    async def query(self, sql):
        """Execute une requ√™te"""
        print(f"Executing: {sql}")
        await asyncio.sleep(0.1)
        return [{"id": 1, "name": "John"}]

# Utilisation
async def main():
    # Le context manager g√®re automatiquement ouverture/fermeture
    async with AsyncDatabaseConnection('localhost', 5432) as db:
        results = await db.query("SELECT * FROM users")
        print(f"Results: {results}")

    # Connection automatiquement ferm√©e ici

asyncio.run(main())
```

### 2. Async Iterators

```python
import asyncio

class AsyncRange:
    """Range asynchrone (async iterator)"""

    def __init__(self, start, end):
        self.current = start
        self.end = end

    def __aiter__(self):
        return self

    async def __anext__(self):
        if self.current >= self.end:
            raise StopAsyncIteration

        # Simule une op√©ration I/O
        await asyncio.sleep(0.1)

        value = self.current
        self.current += 1
        return value

# Utilisation
async def main():
    async for i in AsyncRange(0, 5):
        print(f"Value: {i}")

asyncio.run(main())

# Cas d'usage r√©el : pagination d'API
class AsyncAPIPaginator:
    """It√®re sur les pages d'une API"""

    def __init__(self, base_url, page_size=100):
        self.base_url = base_url
        self.page_size = page_size
        self.current_page = 1
        self.has_more = True

    def __aiter__(self):
        return self

    async def __anext__(self):
        if not self.has_more:
            raise StopAsyncIteration

        # Fetch page
        async with aiohttp.ClientSession() as session:
            url = f"{self.base_url}?page={self.current_page}&size={self.page_size}"
            async with session.get(url) as resp:
                data = await resp.json()

        # Check si il y a plus de pages
        self.has_more = len(data['items']) == self.page_size
        self.current_page += 1

        return data['items']

# Usage
async def process_all_items():
    async for page in AsyncAPIPaginator('https://api.example.com/items'):
        for item in page:
            await process_item(item)
```

### 3. Async Generators

```python
import asyncio

async def async_generator():
    """G√©n√©rateur asynchrone"""
    for i in range(5):
        await asyncio.sleep(0.1)  # Simule I/O
        yield i * 2

# Utilisation
async def main():
    async for value in async_generator():
        print(f"Got: {value}")

asyncio.run(main())

# Cas d'usage : Streaming de fichier
async def stream_file(filepath, chunk_size=8192):
    """Streame un fichier de mani√®re asynchrone"""
    async with aiofiles.open(filepath, 'rb') as f:
        while True:
            chunk = await f.read(chunk_size)
            if not chunk:
                break
            yield chunk

# Streaming HTTP response
async def stream_http_response(url):
    """Streame une r√©ponse HTTP"""
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as resp:
            async for chunk in resp.content.iter_chunked(8192):
                yield chunk
```

## D√©bogage et profiling

### Debugging async

```python
import asyncio
import logging

# Activer le logging asyncio
logging.basicConfig(level=logging.DEBUG)
asyncio.get_event_loop().set_debug(True)

async def slow_task():
    """T√¢che qui bloque l'event loop (MAUVAIS)"""
    import time
    time.sleep(1)  # ‚ö†Ô∏è Bloque l'event loop !

async def main():
    await slow_task()
# Warning: Executing <Task> took 1.001 seconds
```

### D√©tection de t√¢ches bloquantes

```python
import asyncio

async def detect_blocking():
    """D√©tecte les op√©rations bloquantes"""
    loop = asyncio.get_running_loop()
    loop.slow_callback_duration = 0.1  # Warn si > 100ms

    # Mauvais : bloque
    import time
    time.sleep(0.5)  # ‚ö†Ô∏è Warning !

# asyncio g√©n√®re un warning automatiquement
```

### Profiling avec aiomonitor

```python
import asyncio
import aiomonitor

async def my_app():
    while True:
        await asyncio.sleep(1)
        # Votre app

# Avec monitoring
async def main():
    # D√©marre un serveur de monitoring sur port 50101
    with aiomonitor.start_monitor(loop=asyncio.get_running_loop()):
        await my_app()

# Connexion au monitor :
# telnet localhost 50101
# > ps        # Liste des t√¢ches
# > where <task_id>  # Stack trace d'une t√¢che
```

## R√©capitulatif

### Points cl√©s

üéØ **Async = Concurrence sans threads**
- Un seul thread
- Plusieurs op√©rations en parall√®le
- Event loop g√®re l'ordonnancement

üéØ **async/await = Syntaxe claire**
- `async def` : d√©finit une coroutine
- `await` : suspend la coroutine
- Lisibilit√© proche du code synchrone

üéØ **Id√©al pour I/O-bound**
- Requ√™tes r√©seau
- Bases de donn√©es
- APIs externes
- WebSockets

üéØ **√âviter pour CPU-bound**
- Calculs lourds bloquent l'event loop
- Utiliser threads ou processus

### Quand utiliser async ?

| Cas d'usage | Async | Alt. |
|-------------|-------|------|
| Serveur web HP | ‚úÖ | Threads |
| Scraping massif | ‚úÖ | Threads |
| API calls multiples | ‚úÖ | Threads |
| WebSockets | ‚úÖ | - |
| Microservices | ‚úÖ | - |
| Scripts simples | ‚ùå | Sync |
| Calculs lourds | ‚ùå | Threads/Processes |
| Legacy libs | ‚ùå | Sync/Threads |

### Best Practices

‚úÖ **Ne jamais bloquer l'event loop**
```python
# MAUVAIS
import time
time.sleep(1)

# BON
await asyncio.sleep(1)
```

‚úÖ **Utiliser aiohttp, pas requests**
```python
# MAUVAIS (bloque)
import requests
response = requests.get(url)

# BON (async)
import aiohttp
async with aiohttp.ClientSession() as session:
    async with session.get(url) as resp:
        data = await resp.read()
```

‚úÖ **G√©rer les erreurs avec gather(return_exceptions=True)**

‚úÖ **Limiter la concurrence avec Semaphore**

‚úÖ **Utiliser create_task() pour background tasks**

## Prochaines √©tapes

Maintenant que vous ma√Ætrisez la programmation asynchrone :

- **Section 8.8** : R√©silience et fiabilit√© applicative (timeouts, retry, circuit breakers, keep-alive)
- **Section 8.9** : Communication temps r√©el (WebSockets, SSE, long-polling)
- **Section 8.10** : Architectures de communication modernes (REST, gRPC, GraphQL)

---


‚è≠Ô∏è [R√©silience et fiabilit√© applicative](/08-programmation-reseau/08-resilience-fiabilite.md)
