ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 8.8.4 Keep-Alive et connexions persistantes

## Introduction

Ã‰tablir une connexion TCP est coÃ»teux : **3-way handshake** (1 RTT), **TLS handshake** (2 RTT pour TLS 1.2, 1 RTT pour TLS 1.3), nÃ©gociation des paramÃ¨tres. Pour une simple requÃªte HTTP, ce surcoÃ»t peut reprÃ©senter **plus de 80% du temps total** !

**ProblÃ¨me** : CrÃ©er une nouvelle connexion pour chaque requÃªte gaspille du temps et des ressources.

**Solution** : Les connexions persistantes (keep-alive) permettent de **rÃ©utiliser une mÃªme connexion TCP** pour plusieurs requÃªtes/rÃ©ponses, Ã©liminant le coÃ»t du handshake rÃ©pÃ©tÃ©.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SANS KEEP-ALIVE (HTTP/1.0 par dÃ©faut)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  RequÃªte 1:                                             â”‚
â”‚    TCP handshake (100ms)                                â”‚
â”‚    TLS handshake (200ms)                                â”‚
â”‚    HTTP request/response (50ms)                         â”‚
â”‚    Close connection                                     â”‚
â”‚  Total: 350ms                                           â”‚
â”‚                                                         â”‚
â”‚  RequÃªte 2:                                             â”‚
â”‚    TCP handshake (100ms)                                â”‚
â”‚    TLS handshake (200ms)                                â”‚
â”‚    HTTP request/response (50ms)                         â”‚
â”‚    Close connection                                     â”‚
â”‚  Total: 350ms                                           â”‚
â”‚                                                         â”‚
â”‚  Total pour 2 requÃªtes: 700ms                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            AVEC KEEP-ALIVE (HTTP/1.1 par dÃ©faut)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  Connexion initiale:                                    â”‚
â”‚    TCP handshake (100ms)                                â”‚
â”‚    TLS handshake (200ms)                                â”‚
â”‚                                                         â”‚
â”‚  RequÃªte 1:                                             â”‚
â”‚    HTTP request/response (50ms)                         â”‚
â”‚                                                         â”‚
â”‚  RequÃªte 2: (rÃ©utilise la connexion)                    â”‚
â”‚    HTTP request/response (50ms)                         â”‚
â”‚                                                         â”‚
â”‚  Close connection aprÃ¨s timeout d'inactivitÃ©            â”‚
â”‚                                                         â”‚
â”‚  Total pour 2 requÃªtes: 400ms                           â”‚
â”‚  Gain: 43% plus rapide !                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Distinction fondamentale : TCP Keep-Alive vs HTTP Keep-Alive

**âš ï¸ Attention : Ce sont deux mÃ©canismes diffÃ©rents avec des objectifs diffÃ©rents !**

### TCP Keep-Alive

**Objectif** : DÃ©tecter les connexions mortes (broken connections) au niveau TCP.

**Comment Ã§a fonctionne** :
- Envoie pÃ©riodiquement des paquets TCP vides (probes)
- Si pas de rÃ©ponse aprÃ¨s N probes â†’ connexion considÃ©rÃ©e morte
- **Intervalle typique : 2 heures** (!)

**Configuration au niveau socket** :

```python
import socket

sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

# Activer TCP keep-alive
sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)

# Linux : configurer les paramÃ¨tres (optionnel)
if hasattr(socket, 'TCP_KEEPIDLE'):
    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPIDLE, 60)    # DÃ©lai avant premiÃ¨re probe (60s)
    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPINTVL, 10)   # Intervalle entre probes (10s)
    sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPCNT, 6)      # Nombre de probes (6)

# Total : 60s + (6 Ã— 10s) = 120s pour dÃ©tecter une connexion morte
```

**Valeurs systÃ¨me par dÃ©faut (Linux)** :

```bash
# Afficher les paramÃ¨tres
$ sysctl -a | grep tcp_keepalive
net.ipv4.tcp_keepalive_time = 7200    # 2 heures !
net.ipv4.tcp_keepalive_intvl = 75     # 75 secondes
net.ipv4.tcp_keepalive_probes = 9     # 9 probes

# Total par dÃ©faut : 7200 + (9 Ã— 75) = ~2h15min pour dÃ©tecter une connexion morte
```

**Quand l'utiliser** :
- Connexions long-lived (WebSocket, SSH, database)
- DÃ©tecter les connexions zombies
- Traverser les NAT/firewalls qui timeout les connexions inactives

**Quand NE PAS l'utiliser** :
- HTTP courte durÃ©e (overhead inutile)
- Quand le protocole applicatif a son propre ping/pong

### HTTP Keep-Alive (Connexions persistantes)

**Objectif** : RÃ©utiliser une connexion TCP pour plusieurs requÃªtes HTTP.

**Comment Ã§a fonctionne** :
- Client et serveur gardent la connexion ouverte aprÃ¨s chaque rÃ©ponse
- Header `Connection: keep-alive` (HTTP/1.0) ou comportement par dÃ©faut (HTTP/1.1)
- Timeout cÃ´tÃ© serveur (gÃ©nÃ©ralement 5-60 secondes)

**Headers HTTP** :

```http
# Client â†’ Serveur
GET /api/users HTTP/1.1
Host: api.example.com
Connection: keep-alive

# Serveur â†’ Client
HTTP/1.1 200 OK
Connection: keep-alive
Keep-Alive: timeout=30, max=100

# Signifie :
# - timeout=30 : le serveur fermera la connexion aprÃ¨s 30s d'inactivitÃ©
# - max=100 : maximum 100 requÃªtes sur cette connexion
```

**DiffÃ©rences clÃ©s** :

| | TCP Keep-Alive | HTTP Keep-Alive |
|---|---------------|-----------------|
| **Couche** | Transport (TCP) | Application (HTTP) |
| **Objectif** | DÃ©tecter connexions mortes | RÃ©utiliser connexions |
| **Timing** | Heures (par dÃ©faut) | Secondes/minutes |
| **Overhead** | Paquets TCP vides | Headers HTTP |
| **Configuration** | Socket options | Headers HTTP |
| **UtilisÃ© pour** | Long-lived connections | Optimisation HTTP |

**Peuvent Ãªtre combinÃ©s** : Une connexion HTTP persistante avec TCP keep-alive pour dÃ©tecter les connexions mortes.

---

## HTTP Keep-Alive en dÃ©tail

### Ã‰volution historique

```
HTTP/0.9 (1991)
â””â”€ Pas de keep-alive, fermeture aprÃ¨s chaque requÃªte

HTTP/1.0 (1996)
â””â”€ Keep-alive optionnel via header "Connection: keep-alive"

HTTP/1.1 (1999)
â””â”€ Keep-alive par dÃ©faut, fermeture via "Connection: close"

HTTP/2 (2015)
â””â”€ Multiplexage sur une seule connexion, keep-alive implicite

HTTP/3 (2020)
â””â”€ QUIC, connexions persistantes native avec migration de connexion
```

### HTTP/1.1 : Connexions persistantes par dÃ©faut

**Comportement** :
- Connexion reste ouverte par dÃ©faut
- Client peut envoyer plusieurs requÃªtes sÃ©quentiellement
- Serveur rÃ©pond dans l'ordre (pas de parallÃ©lisme)

**Exemple de session** :

```http
# Connexion Ã©tablie
Client: TCP SYN â†’
Server: â† SYN-ACK
Client: ACK â†’

# RequÃªte 1
Client: GET /index.html HTTP/1.1
        Host: example.com

Server: HTTP/1.1 200 OK
        Content-Length: 1024
        Connection: keep-alive

        [contenu de index.html]

# RequÃªte 2 (mÃªme connexion)
Client: GET /style.css HTTP/1.1
        Host: example.com

Server: HTTP/1.1 200 OK
        Content-Length: 512
        Connection: keep-alive

        [contenu de style.css]

# RequÃªte 3 (mÃªme connexion)
Client: GET /script.js HTTP/1.1
        Host: example.com

Server: HTTP/1.1 200 OK
        Content-Length: 2048
        Connection: keep-alive

        [contenu de script.js]

# AprÃ¨s timeout d'inactivitÃ© ou limite de requÃªtes
Server: [ferme la connexion]
```

### Limitation : Head-of-Line Blocking

**ProblÃ¨me** : En HTTP/1.1, les requÃªtes doivent Ãªtre traitÃ©es sÃ©quentiellement.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         HTTP/1.1 Pipelining (rarement utilisÃ©) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Client envoie :                               â”‚
â”‚    GET /file1.js                               â”‚
â”‚    GET /file2.css                              â”‚
â”‚    GET /file3.png                              â”‚
â”‚                                                â”‚
â”‚  Serveur doit rÃ©pondre dans l'ordre :          â”‚
â”‚    Response /file1.js (lent, 2s)               â”‚
â”‚    Response /file2.css (bloquÃ©, attend file1)  â”‚
â”‚    Response /file3.png (bloquÃ©, attend file2)  â”‚
â”‚                                                â”‚
â”‚  Si file1 est lent, tout est bloquÃ© !          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Solution HTTP/1.1** : Ouvrir **6 connexions parallÃ¨les** par domaine.

```javascript
// Le navigateur ouvre automatiquement plusieurs connexions
// pour contourner le head-of-line blocking
Connection 1: GET /file1.js
Connection 2: GET /file2.css
Connection 3: GET /file3.png
Connection 4: GET /file4.jpg
Connection 5: GET /file5.json
Connection 6: GET /file6.xml
```

**Limites** :
- Surcharge serveur (6Ã— plus de connexions)
- 6Ã— overhead de handshake
- ComplexitÃ© de gestion

### HTTP/2 : Multiplexage

**Solution moderne** : Une seule connexion, plusieurs streams parallÃ¨les.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HTTP/2 Multiplexing               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Une seule connexion TCP :                     â”‚
â”‚                                                â”‚
â”‚  Stream 1: GET /file1.js    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           â”‚
â”‚  Stream 2: GET /file2.css   â–ˆâ–ˆâ–ˆâ–ˆ               â”‚
â”‚  Stream 3: GET /file3.png   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         â”‚
â”‚  Stream 4: GET /file4.jpg   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       â”‚
â”‚                                                â”‚
â”‚  Toutes les requÃªtes/rÃ©ponses en parallÃ¨le !   â”‚
â”‚  Pas de head-of-line blocking au niveau HTTP   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Avantages** :
- Une seule connexion = un seul handshake
- Pas de limite de 6 connexions
- Priorisation des streams
- Server push

**Note** : HTTP/2 a toujours du head-of-line blocking au niveau TCP (perte d'un paquet bloque tous les streams).

### HTTP/3 : QUIC et Keep-Alive natif

**BasÃ© sur UDP**, Ã©limine le head-of-line blocking TCP :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         HTTP/3 (QUIC) - Sur UDP                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                â”‚
â”‚  Stream 1: [Paquet perdu] âœ—                    â”‚
â”‚  Stream 2: [OK] âœ“                              â”‚
â”‚  Stream 3: [OK] âœ“                              â”‚
â”‚  Stream 4: [OK] âœ“                              â”‚
â”‚                                                â”‚
â”‚  La perte d'un paquet n'affecte QUE son stream â”‚
â”‚  Migration de connexion (changement d'IP/port) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Connection Pooling

**Concept** : Maintenir un pool de connexions rÃ©utilisables pour Ã©viter de recrÃ©er les connexions.

### Architecture d'un Connection Pool

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Application Threads                    â”‚
â”‚     Thread1  Thread2  Thread3  Thread4          â”‚
â”‚        â”‚       â”‚       â”‚       â”‚                â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                 â”‚                               â”‚
â”‚                 â–¼                               â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚        â”‚ Connection Pool   â”‚                    â”‚
â”‚        â”‚                   â”‚                    â”‚
â”‚        â”‚  [Conn1] [Conn2]  â”‚  â† Idle            â”‚
â”‚        â”‚  [Conn3] [Conn4]  â”‚  â† Idle            â”‚
â”‚        â”‚  [Conn5]          â”‚  â† In use          â”‚
â”‚        â”‚  [Conn6]          â”‚  â† In use          â”‚
â”‚        â”‚                   â”‚                    â”‚
â”‚        â”‚  Max: 10 conns    â”‚                    â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                 â”‚                               â”‚
â”‚                 â–¼                               â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚        â”‚  Remote Server    â”‚                    â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ParamÃ¨tres clÃ©s

```python
class ConnectionPoolConfig:
    min_size: int = 2         # Minimum de connexions maintenues
    max_size: int = 10        # Maximum de connexions
    max_idle_time: int = 300  # Fermer aprÃ¨s 5min d'inactivitÃ© (secondes)
    max_lifetime: int = 3600  # Recycler aprÃ¨s 1h max (secondes)
    connect_timeout: int = 5  # Timeout pour nouvelle connexion
    acquire_timeout: int = 10 # Timeout pour obtenir une connexion du pool

    # Keep-alive
    keep_alive: bool = True
    keep_alive_idle: int = 60    # TCP keep-alive aprÃ¨s 60s inactivitÃ©
    keep_alive_interval: int = 10
    keep_alive_count: int = 6
```

### ImplÃ©mentation Python (requests + urllib3)

```python
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from urllib3.poolmanager import PoolManager

class KeepAliveHTTPAdapter(HTTPAdapter):
    """
    Adapter HTTP avec connection pooling et keep-alive optimisÃ©.
    """

    def __init__(
        self,
        pool_connections=10,
        pool_maxsize=20,
        max_retries=3,
        pool_block=False,
        **kwargs
    ):
        # Configuration des retries
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS", "POST"]
        )

        super().__init__(
            pool_connections=pool_connections,
            pool_maxsize=pool_maxsize,
            max_retries=retry_strategy,
            pool_block=pool_block,
            **kwargs
        )

    def init_poolmanager(self, *args, **kwargs):
        """Personnalise le pool manager avec keep-alive."""
        kwargs['socket_options'] = [
            # TCP keep-alive
            (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1),
        ]

        # Linux : paramÃ¨tres keep-alive affinÃ©s
        if hasattr(socket, 'TCP_KEEPIDLE'):
            kwargs['socket_options'].extend([
                (socket.IPPROTO_TCP, socket.TCP_KEEPIDLE, 60),
                (socket.IPPROTO_TCP, socket.TCP_KEEPINTVL, 10),
                (socket.IPPROTO_TCP, socket.TCP_KEEPCNT, 6),
            ])

        super().init_poolmanager(*args, **kwargs)

# CrÃ©er une session rÃ©utilisable
session = requests.Session()

# Monter l'adapter avec pooling
adapter = KeepAliveHTTPAdapter(
    pool_connections=10,    # 10 pools (un par host)
    pool_maxsize=20,        # 20 connexions max par pool
    max_retries=3
)

session.mount('http://', adapter)
session.mount('https://', adapter)

# Configurer les timeouts globaux
session.timeout = (5, 30)  # (connect, read)

# Utilisation : les connexions sont rÃ©utilisÃ©es automatiquement
for i in range(100):
    response = session.get('https://api.example.com/data')
    print(f"Request {i}: {response.status_code}")

# Les connexions restent ouvertes dans le pool
# et sont rÃ©utilisÃ©es pour les requÃªtes suivantes
```

**MÃ©triques du pool** :

```python
from urllib3 import PoolManager

# AccÃ©der aux stats du pool
pool = session.adapters['https://'].poolmanager

print(f"Pools actifs: {len(pool.pools)}")

for key, pool_instance in pool.pools.items():
    print(f"\nPool {key}:")
    print(f"  Connexions totales: {pool_instance.num_connections}")
    print(f"  Connexions en cours: {pool_instance.num_requests}")
    print(f"  Queue size: {pool_instance.pool.qsize()}")
```

### ImplÃ©mentation Go (net/http)

```go
package main

import (
    "net"
    "net/http"
    "time"
)

// CreateHTTPClient crÃ©e un client HTTP optimisÃ© avec pooling
func CreateHTTPClient() *http.Client {
    // Custom Dialer avec keep-alive
    dialer := &net.Dialer{
        Timeout:   5 * time.Second,  // Connection timeout
        KeepAlive: 30 * time.Second, // TCP keep-alive
    }

    // Custom Transport avec pooling
    transport := &http.Transport{
        // Connection pooling
        MaxIdleConns:        100,              // Total connexions idle max
        MaxIdleConnsPerHost: 20,               // Connexions idle max par host
        MaxConnsPerHost:     0,                // 0 = illimitÃ©

        // Timeouts
        DialContext:           dialer.DialContext,
        IdleConnTimeout:       90 * time.Second,  // Fermer aprÃ¨s 90s idle
        TLSHandshakeTimeout:   10 * time.Second,
        ResponseHeaderTimeout: 10 * time.Second,
        ExpectContinueTimeout: 1 * time.Second,

        // Keep-alive HTTP
        DisableKeepAlives: false,

        // Compression
        DisableCompression: false,

        // Force close aprÃ¨s chaque requÃªte (pour debugging)
        // DisableKeepAlives: true,
    }

    return &http.Client{
        Transport: transport,
        Timeout:   30 * time.Second, // Timeout global
    }
}

// Utilisation
func main() {
    client := CreateHTTPClient()

    // RÃ©utiliser le mÃªme client pour toutes les requÃªtes
    for i := 0; i < 100; i++ {
        resp, err := client.Get("https://api.example.com/data")
        if err != nil {
            log.Printf("Request %d failed: %v", i, err)
            continue
        }
        resp.Body.Close()

        log.Printf("Request %d: %s", i, resp.Status)
    }

    // Les connexions sont automatiquement poolÃ©es et rÃ©utilisÃ©es
    // Transport ferme automatiquement les connexions idle aprÃ¨s timeout
}

// Monitoring du pool
func MonitorConnectionPool(transport *http.Transport) {
    ticker := time.NewTicker(10 * time.Second)
    defer ticker.Stop()

    for range ticker.C {
        // Note: Ces mÃ©triques ne sont pas exposÃ©es directement par http.Transport
        // Il faut utiliser des outils comme pprof ou instrumenter manuellement
        log.Println("Connection pool stats:")
        // Dans une vraie application, utiliser des mÃ©triques Prometheus
    }
}
```

**MÃ©triques avec Prometheus** :

```go
import (
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promhttp"
)

var (
    httpPoolConnsGauge = prometheus.NewGaugeVec(
        prometheus.GaugeOpts{
            Name: "http_pool_connections",
            Help: "Number of connections in HTTP pool",
        },
        []string{"state", "host"},
    )
)

func init() {
    prometheus.MustRegister(httpPoolConnsGauge)
}

// Wrapper de Transport pour tracking
type InstrumentedTransport struct {
    *http.Transport
}

func (t *InstrumentedTransport) RoundTrip(req *http.Request) (*http.Response, error) {
    // Enregistrer les mÃ©triques avant/aprÃ¨s
    start := time.Now()

    resp, err := t.Transport.RoundTrip(req)

    duration := time.Since(start)

    // Mise Ã  jour des mÃ©triques
    // (NÃ©cessite d'accÃ©der aux champs privÃ©s ou utiliser reflection)

    return resp, err
}
```

### ImplÃ©mentation JavaScript (Node.js)

```javascript
const http = require('http');
const https = require('https');

/**
 * CrÃ©e des agents HTTP/HTTPS avec pooling et keep-alive
 */
class HTTPClientPool {
    constructor(options = {}) {
        const defaultOptions = {
            keepAlive: true,
            keepAliveMsecs: 30000,      // 30s entre keep-alive probes
            maxSockets: 20,             // Max connexions par host
            maxFreeSockets: 10,         // Max connexions idle par host
            timeout: 30000,             // Socket timeout
            scheduling: 'lifo'          // 'lifo' ou 'fifo'
        };

        const config = { ...defaultOptions, ...options };

        // Agent HTTP
        this.httpAgent = new http.Agent(config);

        // Agent HTTPS
        this.httpsAgent = new https.Agent(config);
    }

    /**
     * Effectue une requÃªte avec pooling
     */
    async request(url, options = {}) {
        const urlObj = new URL(url);
        const isHttps = urlObj.protocol === 'https:';

        const requestOptions = {
            ...options,
            hostname: urlObj.hostname,
            port: urlObj.port || (isHttps ? 443 : 80),
            path: urlObj.pathname + urlObj.search,
            agent: isHttps ? this.httpsAgent : this.httpAgent
        };

        return new Promise((resolve, reject) => {
            const req = (isHttps ? https : http).request(requestOptions, (res) => {
                let data = '';

                res.on('data', chunk => data += chunk);
                res.on('end', () => {
                    resolve({
                        status: res.statusCode,
                        headers: res.headers,
                        data: data
                    });
                });
            });

            req.on('error', reject);
            req.on('timeout', () => {
                req.destroy();
                reject(new Error('Request timeout'));
            });

            req.end();
        });
    }

    /**
     * Statistiques du pool
     */
    getStats() {
        return {
            http: {
                sockets: Object.keys(this.httpAgent.sockets).length,
                freeSockets: Object.keys(this.httpAgent.freeSockets).length,
                requests: Object.keys(this.httpAgent.requests).length
            },
            https: {
                sockets: Object.keys(this.httpsAgent.sockets).length,
                freeSockets: Object.keys(this.httpsAgent.freeSockets).length,
                requests: Object.keys(this.httpsAgent.requests).length
            }
        };
    }

    /**
     * Ferme toutes les connexions
     */
    destroy() {
        this.httpAgent.destroy();
        this.httpsAgent.destroy();
    }
}

// Utilisation avec axios
const axios = require('axios');

const httpAgent = new http.Agent({
    keepAlive: true,
    maxSockets: 20,
    maxFreeSockets: 10
});

const httpsAgent = new https.Agent({
    keepAlive: true,
    maxSockets: 20,
    maxFreeSockets: 10
});

const client = axios.create({
    httpAgent: httpAgent,
    httpsAgent: httpsAgent,
    timeout: 30000
});

// Toutes les requÃªtes rÃ©utilisent les connexions
async function makeRequests() {
    for (let i = 0; i < 100; i++) {
        try {
            const response = await client.get('https://api.example.com/data');
            console.log(`Request ${i}: ${response.status}`);
        } catch (error) {
            console.error(`Request ${i} failed:`, error.message);
        }
    }
}

// Monitoring
setInterval(() => {
    console.log('HTTP Agent sockets:', Object.keys(httpAgent.sockets).length);
    console.log('HTTP Agent free sockets:', Object.keys(httpAgent.freeSockets).length);
    console.log('HTTPS Agent sockets:', Object.keys(httpsAgent.sockets).length);
    console.log('HTTPS Agent free sockets:', Object.keys(httpsAgent.freeSockets).length);
}, 10000);

makeRequests();
```

### ImplÃ©mentation Java (Apache HttpClient)

```java
import org.apache.http.impl.client.CloseableHttpClient;
import org.apache.http.impl.client.HttpClients;
import org.apache.http.impl.conn.PoolingHttpClientConnectionManager;
import org.apache.http.client.config.RequestConfig;

public class HTTPClientPool {

    private final CloseableHttpClient httpClient;
    private final PoolingHttpClientConnectionManager connManager;

    public HTTPClientPool() {
        // Configuration du pool de connexions
        connManager = new PoolingHttpClientConnectionManager();

        // ParamÃ¨tres du pool
        connManager.setMaxTotal(100);                  // Total max connexions
        connManager.setDefaultMaxPerRoute(20);         // Max par route (host)
        connManager.setValidateAfterInactivity(2000);  // Valider aprÃ¨s 2s inactivitÃ©

        // Configuration des timeouts
        RequestConfig requestConfig = RequestConfig.custom()
            .setConnectTimeout(5000)              // Connection timeout: 5s
            .setSocketTimeout(30000)              // Read timeout: 30s
            .setConnectionRequestTimeout(10000)   // Pool acquisition timeout: 10s
            .build();

        // CrÃ©er le client HTTP
        httpClient = HttpClients.custom()
            .setConnectionManager(connManager)
            .setDefaultRequestConfig(requestConfig)
            .setKeepAliveStrategy((response, context) -> {
                // Keep-alive strategy personnalisÃ©e
                HeaderElementIterator it = new BasicHeaderElementIterator(
                    response.headerIterator(HTTP.CONN_KEEP_ALIVE));

                while (it.hasNext()) {
                    HeaderElement he = it.nextElement();
                    String param = he.getName();
                    String value = he.getValue();

                    if (value != null && param.equalsIgnoreCase("timeout")) {
                        return Long.parseLong(value) * 1000;
                    }
                }

                // Par dÃ©faut : 30 secondes
                return 30 * 1000;
            })
            .evictIdleConnections(60, TimeUnit.SECONDS)  // Fermer idle > 60s
            .evictExpiredConnections()
            .build();

        // Thread de monitoring
        startMonitoring();
    }

    public CloseableHttpResponse execute(HttpUriRequest request) throws IOException {
        return httpClient.execute(request);
    }

    private void startMonitoring() {
        ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(1);

        scheduler.scheduleAtFixedRate(() -> {
            PoolStats totalStats = connManager.getTotalStats();

            System.out.println("Connection Pool Stats:");
            System.out.println("  Total connections: " + totalStats.getLeased());
            System.out.println("  Available: " + totalStats.getAvailable());
            System.out.println("  Pending: " + totalStats.getPending());
            System.out.println("  Max: " + totalStats.getMax());
        }, 0, 10, TimeUnit.SECONDS);
    }

    public void close() throws IOException {
        httpClient.close();
    }

    // Exemple d'utilisation
    public static void main(String[] args) throws Exception {
        HTTPClientPool pool = new HTTPClientPool();

        // Faire 100 requÃªtes
        for (int i = 0; i < 100; i++) {
            HttpGet request = new HttpGet("https://api.example.com/data");

            try (CloseableHttpResponse response = pool.execute(request)) {
                System.out.println("Request " + i + ": " + response.getStatusLine());
                EntityUtils.consume(response.getEntity());
            } catch (Exception e) {
                System.err.println("Request " + i + " failed: " + e.getMessage());
            }
        }

        pool.close();
    }
}
```

---

## Optimisation des performances

### Benchmarks : Impact du keep-alive

**Test** : 1000 requÃªtes vers un serveur distant (latence 50ms).

```python
import time
import requests

def benchmark_without_keepalive():
    """Nouvelle connexion pour chaque requÃªte."""
    start = time.time()

    for i in range(1000):
        # CrÃ©er une nouvelle session Ã  chaque fois
        response = requests.get('https://httpbin.org/uuid')
        response.close()

    duration = time.time() - start
    print(f"Sans keep-alive: {duration:.2f}s ({1000/duration:.1f} req/s)")

def benchmark_with_keepalive():
    """RÃ©utiliser la mÃªme session."""
    start = time.time()

    # Une seule session rÃ©utilisÃ©e
    session = requests.Session()

    for i in range(1000):
        response = session.get('https://httpbin.org/uuid')
        response.close()

    duration = time.time() - start
    print(f"Avec keep-alive: {duration:.2f}s ({1000/duration:.1f} req/s)")

# RÃ©sultats typiques (50ms latency):
# Sans keep-alive: 180.45s (5.5 req/s)
# Avec keep-alive: 62.12s (16.1 req/s)
# Gain: 3x plus rapide !
```

**Analyse** :

```
Sans keep-alive (par requÃªte):
- TCP handshake: 50ms (1 RTT)
- TLS handshake: 100ms (2 RTT)
- HTTP request/response: 100ms (2 RTT)
- Total: 250ms
- 1000 requÃªtes = 250s

Avec keep-alive:
- PremiÃ¨re requÃªte: 250ms (handshakes)
- RequÃªtes suivantes: 100ms (juste HTTP)
- Total: 250 + (999 Ã— 100) = 100s
- Gain: 2.5x plus rapide

Avec HTTP/2 (multiplexage):
- Une connexion: 250ms (handshakes)
- 1000 requÃªtes en parallÃ¨le: 100ms
- Total: 350ms
- Gain: 700x plus rapide !
```

### Dimensionnement du pool

**Formule de base** :

```
pool_size = (requests_per_second Ã— average_response_time) + buffer

Exemple:
- 100 req/s
- Temps de rÃ©ponse moyen: 200ms (0.2s)
- Buffer: 20%

pool_size = (100 Ã— 0.2) Ã— 1.2 = 24 connexions
```

**RÃ¨gles empiriques** :

| ScÃ©nario | Min | Max | Justification |
|----------|-----|-----|---------------|
| **API REST faible charge** | 2 | 10 | Quelques connexions suffisent |
| **API REST charge moyenne** | 5 | 50 | Balance performance/ressources |
| **API REST haute charge** | 10 | 200 | Absorber les pics |
| **Base de donnÃ©es** | 5 | 20 | BDD limitÃ©e en connexions |
| **Microservices** | 10 | 100 | Haute frÃ©quence d'appels |
| **Batch processing** | 20 | 500 | ParallÃ©lisme Ã©levÃ© |

### StratÃ©gies de recyclage

**Pourquoi recycler les connexions** :
- Ã‰viter les memory leaks cÃ´tÃ© serveur
- Forcer la reconnexion pour load balancing
- Ã‰viter les connexions "stale"

```python
class ConnectionLifecycleManager:
    """
    GÃ¨re le cycle de vie des connexions dans un pool.
    """

    def __init__(
        self,
        max_idle_time=300,     # 5 minutes
        max_lifetime=3600,     # 1 heure
        max_requests=1000      # Max requÃªtes par connexion
    ):
        self.max_idle_time = max_idle_time
        self.max_lifetime = max_lifetime
        self.max_requests = max_requests

    def should_recycle(self, connection):
        """DÃ©termine si une connexion doit Ãªtre recyclÃ©e."""
        now = time.time()

        # Trop vieille (Ã¢ge total)
        if now - connection.created_at > self.max_lifetime:
            return True, "max_lifetime_exceeded"

        # Inactive trop longtemps
        if now - connection.last_used > self.max_idle_time:
            return True, "idle_timeout"

        # Trop de requÃªtes
        if connection.request_count >= self.max_requests:
            return True, "max_requests_exceeded"

        # Connexion saine
        return False, None

# ImplÃ©mentation dans un pool
class ManagedConnectionPool:
    def __init__(self, *args, **kwargs):
        self.pool = []
        self.lifecycle_manager = ConnectionLifecycleManager()
        self.lock = threading.Lock()

    def acquire(self):
        """Acquiert une connexion, crÃ©e-en une nouvelle si nÃ©cessaire."""
        with self.lock:
            # Nettoyer les connexions expirÃ©es
            self._evict_expired()

            # Chercher une connexion idle
            for conn in self.pool:
                if not conn.in_use:
                    should_recycle, reason = self.lifecycle_manager.should_recycle(conn)

                    if should_recycle:
                        print(f"Recycling connection: {reason}")
                        conn.close()
                        self.pool.remove(conn)
                        continue

                    conn.in_use = True
                    conn.last_used = time.time()
                    return conn

            # CrÃ©er une nouvelle connexion
            conn = self._create_connection()
            self.pool.append(conn)
            return conn

    def release(self, conn):
        """LibÃ¨re une connexion dans le pool."""
        with self.lock:
            conn.in_use = False
            conn.request_count += 1
            conn.last_used = time.time()

    def _evict_expired(self):
        """Supprime les connexions expirÃ©es."""
        to_remove = []

        for conn in self.pool:
            if conn.in_use:
                continue

            should_recycle, reason = self.lifecycle_manager.should_recycle(conn)
            if should_recycle:
                to_remove.append(conn)

        for conn in to_remove:
            conn.close()
            self.pool.remove(conn)
```

### Warm-up du pool

**ProblÃ¨me** : Ã€ froid, le pool est vide. Les premiÃ¨res requÃªtes subissent le coÃ»t du handshake.

**Solution** : PrÃ©-crÃ©er des connexions au dÃ©marrage.

```go
// Warm-up du pool au dÃ©marrage
func WarmUpConnectionPool(client *http.Client, urls []string, connections int) error {
    var wg sync.WaitGroup
    errChan := make(chan error, connections)

    for i := 0; i < connections; i++ {
        wg.Add(1)

        go func(idx int) {
            defer wg.Done()

            // Effectuer une requÃªte pour crÃ©er la connexion
            url := urls[idx % len(urls)]
            resp, err := client.Get(url)

            if err != nil {
                errChan <- err
                return
            }

            // Lire et fermer le body pour rÃ©utiliser la connexion
            io.Copy(io.Discard, resp.Body)
            resp.Body.Close()

            log.Printf("Warmed up connection %d to %s", idx, url)
        }(i)
    }

    wg.Wait()
    close(errChan)

    // VÃ©rifier les erreurs
    var errors []error
    for err := range errChan {
        errors = append(errors, err)
    }

    if len(errors) > 0 {
        return fmt.Errorf("warm-up failed with %d errors", len(errors))
    }

    return nil
}

// Utilisation
func main() {
    client := CreateHTTPClient()

    urls := []string{
        "https://api.example.com/health",
        "https://api2.example.com/health",
    }

    // Warm-up: crÃ©er 20 connexions
    if err := WarmUpConnectionPool(client, urls, 20); err != nil {
        log.Fatalf("Warm-up failed: %v", err)
    }

    log.Println("Pool warmed up, ready to serve traffic")

    // Les requÃªtes suivantes rÃ©utilisent les connexions
}
```

---

## Cas d'usage spÃ©cifiques

### 1. WebSocket : Keep-alive essentiel

**WebSocket nÃ©cessite une connexion persistante par nature.**

```javascript
const WebSocket = require('ws');

class ResilientWebSocket {
    constructor(url, options = {}) {
        this.url = url;
        this.reconnectInterval = options.reconnectInterval || 5000;
        this.maxReconnectAttempts = options.maxReconnectAttempts || 10;
        this.pingInterval = options.pingInterval || 30000;  // Ping toutes les 30s

        this.reconnectAttempts = 0;
        this.pingTimer = null;

        this.connect();
    }

    connect() {
        this.ws = new WebSocket(this.url);

        this.ws.on('open', () => {
            console.log('WebSocket connected');
            this.reconnectAttempts = 0;

            // DÃ©marrer les pings keep-alive
            this.startPing();
        });

        this.ws.on('pong', () => {
            console.log('Received pong');
        });

        this.ws.on('close', () => {
            console.log('WebSocket closed');
            this.stopPing();
            this.reconnect();
        });

        this.ws.on('error', (error) => {
            console.error('WebSocket error:', error);
        });
    }

    startPing() {
        this.pingTimer = setInterval(() => {
            if (this.ws.readyState === WebSocket.OPEN) {
                this.ws.ping();
                console.log('Sent ping');
            }
        }, this.pingInterval);
    }

    stopPing() {
        if (this.pingTimer) {
            clearInterval(this.pingTimer);
            this.pingTimer = null;
        }
    }

    reconnect() {
        if (this.reconnectAttempts >= this.maxReconnectAttempts) {
            console.error('Max reconnect attempts reached');
            return;
        }

        this.reconnectAttempts++;

        console.log(`Reconnecting in ${this.reconnectInterval}ms (attempt ${this.reconnectAttempts})`);

        setTimeout(() => {
            this.connect();
        }, this.reconnectInterval);
    }

    send(data) {
        if (this.ws.readyState === WebSocket.OPEN) {
            this.ws.send(data);
        } else {
            console.error('WebSocket not connected');
        }
    }

    close() {
        this.stopPing();
        this.ws.close();
    }
}

// Utilisation
const ws = new ResilientWebSocket('wss://api.example.com/ws', {
    pingInterval: 30000,      // Ping toutes les 30s
    reconnectInterval: 5000,
    maxReconnectAttempts: 10
});

ws.send(JSON.stringify({ type: 'subscribe', channel: 'updates' }));
```

### 2. Database connection pooling

**Les bases de donnÃ©es bÃ©nÃ©ficient Ã©normÃ©ment du pooling.**

```python
# PostgreSQL avec psycopg2
from psycopg2 import pool
import contextlib

class DatabasePool:
    def __init__(self, minconn=2, maxconn=20, **db_params):
        self.pool = pool.ThreadedConnectionPool(
            minconn,
            maxconn,
            **db_params
        )

    @contextlib.contextmanager
    def get_connection(self):
        """Context manager pour acquÃ©rir/libÃ©rer une connexion."""
        conn = self.pool.getconn()
        try:
            yield conn
        finally:
            self.pool.putconn(conn)

    def execute(self, query, params=None):
        """Execute une query avec gestion automatique de la connexion."""
        with self.get_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(query, params)
                result = cur.fetchall() if cur.description else None
            conn.commit()
            return result

    def close_all(self):
        """Ferme toutes les connexions."""
        self.pool.closeall()

# Utilisation
db = DatabasePool(
    minconn=5,
    maxconn=20,
    host='localhost',
    database='mydb',
    user='user',
    password='pass'
)

# Les connexions sont automatiquement gÃ©rÃ©es
users = db.execute("SELECT * FROM users WHERE active = %s", (True,))

# Plusieurs requÃªtes rÃ©utilisent les connexions du pool
for i in range(100):
    db.execute("INSERT INTO logs (message) VALUES (%s)", (f"Log {i}",))

db.close_all()
```

**SQLAlchemy (ORM)** :

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# Engine avec pooling
engine = create_engine(
    'postgresql://user:pass@localhost/mydb',

    # Configuration du pool
    pool_size=10,              # Connexions persistantes
    max_overflow=20,           # Connexions supplÃ©mentaires temporaires
    pool_timeout=30,           # Timeout pour acquisition
    pool_recycle=3600,         # Recycler aprÃ¨s 1h
    pool_pre_ping=True,        # VÃ©rifier la connexion avant utilisation

    # Logging
    echo_pool=True
)

Session = sessionmaker(bind=engine)

# Utilisation
session = Session()
try:
    users = session.query(User).filter_by(active=True).all()
    session.commit()
finally:
    session.close()  # Retourne la connexion au pool

# MÃ©triques du pool
pool = engine.pool
print(f"Pool size: {pool.size()}")
print(f"Checked in: {pool.checkedin()}")
print(f"Checked out: {pool.checkedout()}")
print(f"Overflow: {pool.overflow()}")
```

### 3. Microservices : Service mesh et keep-alive

**Dans Kubernetes avec Istio/Linkerd** :

```yaml
# Envoy (proxy sidecar) configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: envoy-config
data:
  envoy.yaml: |
    static_resources:
      clusters:
      - name: backend_service
        connect_timeout: 5s
        type: STRICT_DNS
        lb_policy: ROUND_ROBIN

        # Configuration keep-alive
        upstream_connection_options:
          tcp_keepalive:
            keepalive_time: 60
            keepalive_interval: 10
            keepalive_probes: 6

        # Connection pooling
        circuit_breakers:
          thresholds:
          - max_connections: 100
            max_pending_requests: 100
            max_requests: 1000
            max_retries: 3

        # Connexions persistantes HTTP/2
        http2_protocol_options: {}

        load_assignment:
          cluster_name: backend_service
          endpoints:
          - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: backend-service
                    port_value: 8080
```

**gRPC avec keep-alive** :

```go
import (
    "google.golang.org/grpc"
    "google.golang.org/grpc/keepalive"
)

// Client gRPC avec keep-alive
func CreateGRPCClient(target string) (*grpc.ClientConn, error) {
    kacp := keepalive.ClientParameters{
        Time:                10 * time.Second, // Ping aprÃ¨s 10s inactivitÃ©
        Timeout:             5 * time.Second,  // Timeout pour pong
        PermitWithoutStream: true,             // Ping mÃªme sans stream actif
    }

    conn, err := grpc.Dial(
        target,
        grpc.WithKeepaliveParams(kacp),
        grpc.WithInsecure(),
    )

    return conn, err
}

// Serveur gRPC avec keep-alive
func CreateGRPCServer() *grpc.Server {
    kaep := keepalive.EnforcementPolicy{
        MinTime:             5 * time.Second, // Min temps entre pings
        PermitWithoutStream: true,
    }

    kasp := keepalive.ServerParameters{
        Time:    10 * time.Second,
        Timeout: 5 * time.Second,
    }

    server := grpc.NewServer(
        grpc.KeepaliveEnforcementPolicy(kaep),
        grpc.KeepaliveParams(kasp),
    )

    return server
}
```

### 4. CDN et edge caching avec keep-alive

**Configuration Nginx (reverse proxy)** :

```nginx
http {
    # Connection pooling vers upstream
    upstream backend {
        server backend1.example.com:80 max_fails=3 fail_timeout=30s;
        server backend2.example.com:80 max_fails=3 fail_timeout=30s;

        # Keep-alive vers backend
        keepalive 32;  # 32 connexions idle par worker
    }

    server {
        listen 80;

        # Keep-alive client â†’ Nginx
        keepalive_timeout 65s;      # Timeout idle
        keepalive_requests 100;     # Max requÃªtes par connexion

        location / {
            proxy_pass http://backend;

            # Keep-alive Nginx â†’ backend
            proxy_http_version 1.1;
            proxy_set_header Connection "";

            # Timeouts
            proxy_connect_timeout 5s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;
        }
    }
}
```

---

## Monitoring et mÃ©triques

### MÃ©triques essentielles

```python
from prometheus_client import Gauge, Counter, Histogram

# Pool de connexions
conn_pool_size = Gauge(
    'http_connection_pool_size',
    'Number of connections in pool',
    ['state', 'host']  # state: idle, active, total
)

conn_pool_wait_time = Histogram(
    'http_connection_pool_wait_seconds',
    'Time waiting to acquire a connection',
    ['host'],
    buckets=[0.001, 0.01, 0.1, 0.5, 1.0, 5.0]
)

conn_pool_errors = Counter(
    'http_connection_pool_errors_total',
    'Connection pool errors',
    ['host', 'error_type']  # timeout, max_connections, etc.
)

# Connexions recyclÃ©es
conn_recycled = Counter(
    'http_connections_recycled_total',
    'Total connections recycled',
    ['host', 'reason']  # max_lifetime, idle_timeout, max_requests
)

# Connexions keep-alive
conn_keepalive_reused = Counter(
    'http_keepalive_reused_total',
    'Connections reused via keep-alive',
    ['host']
)

# Exemple d'instrumentation
class InstrumentedConnectionPool:
    def acquire(self, host):
        start = time.time()

        try:
            conn = self._acquire_from_pool(host)

            # Enregistrer le temps d'attente
            wait_time = time.time() - start
            conn_pool_wait_time.labels(host=host).observe(wait_time)

            # Si connexion rÃ©utilisÃ©e
            if conn.request_count > 0:
                conn_keepalive_reused.labels(host=host).inc()

            return conn

        except PoolTimeout:
            conn_pool_errors.labels(host=host, error_type='timeout').inc()
            raise
        except PoolExhausted:
            conn_pool_errors.labels(host=host, error_type='max_connections').inc()
            raise

    def _update_metrics(self, host):
        """Met Ã  jour les mÃ©triques du pool."""
        idle = len([c for c in self.pool if not c.in_use])
        active = len([c for c in self.pool if c.in_use])
        total = len(self.pool)

        conn_pool_size.labels(state='idle', host=host).set(idle)
        conn_pool_size.labels(state='active', host=host).set(active)
        conn_pool_size.labels(state='total', host=host).set(total)
```

### Dashboard Grafana

**RequÃªtes PromQL utiles** :

```promql
# Taux d'utilisation du pool
(http_connection_pool_size{state="active"}
/
http_connection_pool_size{state="total"}) * 100

# Connexions rÃ©utilisÃ©es (efficacitÃ© du keep-alive)
rate(http_keepalive_reused_total[5m])

# Temps d'attente P95 pour acquisition
histogram_quantile(0.95,
  rate(http_connection_pool_wait_seconds_bucket[5m])
)

# Taux d'erreurs du pool
rate(http_connection_pool_errors_total[5m])

# Connexions recyclÃ©es par raison
sum by (reason) (rate(http_connections_recycled_total[5m]))
```

### Alertes

```yaml
groups:
  - name: connection_pool_alerts
    rules:
      # Pool saturÃ©
      - alert: ConnectionPoolSaturated
        expr: |
          (http_connection_pool_size{state="active"}
          /
          http_connection_pool_size{state="total"}) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Connection pool nearly full for {{ $labels.host }}"

      # Temps d'attente Ã©levÃ©
      - alert: HighConnectionAcquisitionTime
        expr: |
          histogram_quantile(0.95,
            rate(http_connection_pool_wait_seconds_bucket[5m])
          ) > 1.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High connection acquisition time for {{ $labels.host }}"

      # Taux d'erreurs Ã©levÃ©
      - alert: HighConnectionPoolErrorRate
        expr: |
          rate(http_connection_pool_errors_total[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High connection pool error rate for {{ $labels.host }}"
```

---

## Anti-patterns et piÃ¨ges

### âŒ 1. Pool trop petit

```python
# Anti-pattern : pool trop petit pour la charge
session = requests.Session()
adapter = HTTPAdapter(pool_maxsize=2)  # Seulement 2 connexions !
session.mount('https://', adapter)

# Avec 100 threads concurrents, 98 vont attendre
with ThreadPoolExecutor(max_workers=100) as executor:
    futures = [
        executor.submit(session.get, 'https://api.example.com/data')
        for _ in range(100)
    ]
```

**ConsÃ©quence** : Goulot d'Ã©tranglement, threads bloquÃ©s en attente de connexion.

**Solution** : Dimensionner selon la concurrence attendue.

```python
adapter = HTTPAdapter(
    pool_connections=10,   # Pools par host
    pool_maxsize=100       # Connexions par pool
)
```

### âŒ 2. Oublier de fermer les rÃ©ponses

```python
# Anti-pattern : ne pas fermer les rÃ©ponses
session = requests.Session()

for i in range(1000):
    response = session.get('https://api.example.com/data')
    data = response.json()
    # Oops! response.close() jamais appelÃ©
    # La connexion reste "in use" et n'est jamais retournÃ©e au pool
```

**ConsÃ©quence** : Fuite de connexions, pool Ã©puisÃ©.

**Solution** : Toujours fermer ou utiliser context manager.

```python
# MÃ©thode 1 : with statement
with session.get('https://api.example.com/data') as response:
    data = response.json()

# MÃ©thode 2 : try/finally
response = session.get('https://api.example.com/data')
try:
    data = response.json()
finally:
    response.close()

# MÃ©thode 3 : lire complÃ¨tement le body (ferme automatiquement)
response = session.get('https://api.example.com/data')
data = response.json()  # Lit tout le body
# Connexion automatiquement retournÃ©e au pool
```

### âŒ 3. Keep-alive avec load balancer mal configurÃ©

```
Client â”€â”€[keep-alive]â”€â”€> Load Balancer â”€â”€> Backend 1
                                        â”€â”€> Backend 2
                                        â”€â”€> Backend 3

ProblÃ¨me : Le LB route toutes les requÃªtes d'une connexion
vers le mÃªme backend â†’ dÃ©sÃ©quilibre de charge !
```

**Solution** : Configurer le LB pour HTTP/2 ou fermer pÃ©riodiquement les connexions.

```nginx
# Nginx : limiter la durÃ©e de vie des connexions keep-alive
upstream backend {
    server backend1:80;
    server backend2:80;
    server backend3:80;

    keepalive 32;
}

server {
    location / {
        proxy_pass http://backend;

        # Limiter pour forcer redistribution
        keepalive_requests 100;    # Max 100 requÃªtes
        keepalive_timeout 30s;     # Max 30s
    }
}
```

### âŒ 4. Connexions zombies (stale connections)

```python
# ProblÃ¨me : connexion fermÃ©e cÃ´tÃ© serveur mais client ne le sait pas
session = requests.Session()

# Connexion Ã©tablie
response = session.get('https://api.example.com/data')

# Serveur ferme la connexion aprÃ¨s timeout
time.sleep(120)  # InactivitÃ© 2 minutes

# Tentative d'utilisation â†’ erreur !
response = session.get('https://api.example.com/data')
# ConnectionError: Connection broken
```

**Solution** : Configurer pool_pre_ping ou retry automatique.

```python
# SQLAlchemy example
engine = create_engine(
    'postgresql://...',
    pool_pre_ping=True  # VÃ©rifie la connexion avant utilisation
)

# Requests : retry automatique sur connection errors
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

retry_strategy = Retry(
    total=3,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=["HEAD", "GET", "OPTIONS"],
    raise_on_status=False
)

adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount("http://", adapter)
session.mount("https://", adapter)
```

### âŒ 5. MÃ©langer sessions courtes et longues

```python
# Anti-pattern : mÃªme pool pour requÃªtes courtes et longues
pool = ConnectionPool(maxsize=10)

# Thread 1 : requÃªte courte (50ms)
response = pool.request('GET', '/quick')

# Thread 2 : requÃªte longue (30s)
response = pool.request('GET', '/slow')  # Bloque une connexion 30s !

# Threads 3-12 : requÃªtes courtes
# ProblÃ¨me : doivent attendre que /slow libÃ¨re la connexion
```

**Solution** : Pools sÃ©parÃ©s par type de requÃªte.

```python
quick_pool = ConnectionPool(maxsize=20, timeout=5)
slow_pool = ConnectionPool(maxsize=5, timeout=60)

# RequÃªtes courtes
response = quick_pool.request('GET', '/quick')

# RequÃªtes longues
response = slow_pool.request('GET', '/slow')
```

---

## Checklist pour production

- [ ] **Keep-alive activÃ©** pour HTTP (Connection: keep-alive)
- [ ] **Pool dimensionnÃ©** selon charge et concurrence
- [ ] **Limites configurÃ©es** : min_size, max_size, max_idle_time
- [ ] **Timeouts dÃ©finis** : connect, read, pool acquisition
- [ ] **Recyclage configurÃ©** : max_lifetime, max_requests
- [ ] **TCP keep-alive** pour connexions long-lived (WebSocket, DB)
- [ ] **RÃ©ponses fermÃ©es** correctement (context managers)
- [ ] **Monitoring** : mÃ©triques du pool exposÃ©es
- [ ] **Alertes** : saturation, temps d'attente, erreurs
- [ ] **Load balancer compatible** avec keep-alive
- [ ] **Warm-up** du pool au dÃ©marrage si nÃ©cessaire
- [ ] **Tests de charge** validant le comportement du pool
- [ ] **Documentation** des paramÃ¨tres et leur justification

---

## Conclusion

Les connexions persistantes (keep-alive) et le connection pooling sont des **optimisations fondamentales** pour les applications rÃ©seau modernes. Bien implÃ©mentÃ©s, ils peuvent multiplier les performances par 3-10Ã— tout en rÃ©duisant la charge serveur.

**Points clÃ©s Ã  retenir** :

1. **Deux mÃ©canismes distincts** :
   - TCP keep-alive : dÃ©tecter connexions mortes (heures)
   - HTTP keep-alive : rÃ©utiliser connexions (secondes/minutes)

2. **Connection pooling essentiel** : RÃ©utiliser plutÃ´t que recrÃ©er

3. **Dimensionnement critique** : Ni trop petit (goulot) ni trop grand (gaspillage)

4. **Recyclage nÃ©cessaire** : Ã‰viter les connexions stale et memory leaks

5. **Monitoring indispensable** : Surveiller utilisation, attente, erreurs

6. **Ã‰volution HTTP** :
   - HTTP/1.1 : connexions persistantes sÃ©quentielles
   - HTTP/2 : multiplexage sur une connexion
   - HTTP/3 : QUIC avec migration de connexion

**Triangle de l'optimisation rÃ©seau** :
```
    Keep-Alive
   (RÃ©utilisation)
        /\
       /  \
      /    \
     /      \
Connection   Multiplexage
 Pooling     (HTTP/2)
(Gestion)   (ParallÃ©lisme)
```

**Gains typiques** :
- Latence : **-50% Ã  -80%** (Ã©conomie de handshakes)
- Throughput : **+200% Ã  +1000%** (parallÃ©lisme)
- Charge serveur : **-60% Ã  -90%** (moins de connexions)

Les connexions persistantes, combinÃ©es avec les timeouts, retries et circuit breakers explorÃ©s dans les sections prÃ©cÃ©dentes, forment la base d'une architecture rÃ©seau rÃ©siliente et performante.

---


â­ï¸ [Communication temps rÃ©el : panorama complet](/08-programmation-reseau/09-communication-temps-reel.md)
