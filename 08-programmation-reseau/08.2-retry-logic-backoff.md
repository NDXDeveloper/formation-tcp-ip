üîù Retour au [Sommaire](/SOMMAIRE.md)

# 8.8.2 Retry logic et backoff exponentiel

## Introduction

Les timeouts d√©tectent les probl√®mes, mais **que faire ensuite ?** Une erreur r√©seau ne signifie pas n√©cessairement un √©chec d√©finitif. De nombreuses d√©faillances sont **transitoires** : un serveur temporairement surcharg√©, un paquet perdu, une connexion r√©seau instable qui se r√©tablit. **La retry logic transforme des √©checs transitoires en succ√®s.**

Mais attention : **retenter stupidement peut aggraver la situation**. Une mauvaise strat√©gie de retry peut transformer une panne locale en catastrophe distribu√©e. Cette section explore comment impl√©menter des retries intelligents qui am√©liorent la r√©silience sans cr√©er de nouveaux probl√®mes.

---

## Pourquoi retrier ?

### Le r√©seau est intrins√®quement non fiable

Les d√©faillances r√©seau temporaires sont la norme, pas l'exception :

```
Sc√©nario 1 : Paquet perdu (taux de perte ~0.1-1%)
Client ‚îÄ‚îÄ[SYN]‚îÄ‚îÄX (perdu)
Client ‚îÄ‚îÄ[SYN]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> Serveur ‚úì (retry automatique par TCP)

Sc√©nario 2 : Serveur temporairement surcharg√©
Client ‚îÄ‚îÄ[Request]‚îÄ‚îÄ> Serveur [503 Service Unavailable]
  [attend 2s]
Client ‚îÄ‚îÄ[Request]‚îÄ‚îÄ> Serveur [200 OK] ‚úì

Sc√©nario 3 : Basculement d'instance (rolling deployment)
Client ‚îÄ‚îÄ[Request]‚îÄ‚îÄ> Instance-1 [Connection refused]
  [attend 1s]
Client ‚îÄ‚îÄ[Request]‚îÄ‚îÄ> Instance-2 [200 OK] ‚úì
```

### Statistiques r√©elles

**√âtude Google (2017)** sur les requ√™tes inter-datacenter :
- **~0.5%** des requ√™tes √©chouent au premier essai
- **~98%** de ces √©checs r√©ussissent au deuxi√®me essai
- **R√©sultat** : Un simple retry am√©liore la fiabilit√© de 0.995 √† 0.99999

**AWS Best Practices** :
- Les SDK AWS impl√©mentent par d√©faut des retries avec exponential backoff
- Recommandation : 3 tentatives minimum pour les op√©rations critiques
- Timeouts sugg√©r√©s : 2s, 4s, 8s (backoff exponentiel)

---

## Erreurs retriable vs non-retriable

**R√®gle d'or** : Ne pas retrier aveugl√©ment toutes les erreurs.

### Classification des erreurs

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              ERREURS R√âSEAU                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                 ‚îÇ
‚îÇ  RETRIABLE (erreurs transitoires)               ‚îÇ
‚îÇ  ‚úì Timeout (connection, read, write)            ‚îÇ
‚îÇ  ‚úì Connection refused (serveur red√©marre)       ‚îÇ
‚îÇ  ‚úì Connection reset                             ‚îÇ
‚îÇ  ‚úì DNS temporary failure                        ‚îÇ
‚îÇ  ‚úì HTTP 429 (Too Many Requests)                 ‚îÇ
‚îÇ  ‚úì HTTP 500 (Internal Server Error)             ‚îÇ
‚îÇ  ‚úì HTTP 502 (Bad Gateway)                       ‚îÇ
‚îÇ  ‚úì HTTP 503 (Service Unavailable)               ‚îÇ
‚îÇ  ‚úì HTTP 504 (Gateway Timeout)                   ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  NON-RETRIABLE (erreurs permanentes)            ‚îÇ
‚îÇ  ‚úó HTTP 400 (Bad Request)                       ‚îÇ
‚îÇ  ‚úó HTTP 401 (Unauthorized)                      ‚îÇ
‚îÇ  ‚úó HTTP 403 (Forbidden)                         ‚îÇ
‚îÇ  ‚úó HTTP 404 (Not Found)                         ‚îÇ
‚îÇ  ‚úó HTTP 405 (Method Not Allowed)                ‚îÇ
‚îÇ  ‚úó Erreurs de validation                        ‚îÇ
‚îÇ  ‚úó Certificat SSL invalide (si non-expir√©)      ‚îÇ
‚îÇ                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Cas particuliers et nuances

#### HTTP 408 (Request Timeout)

**Ambigu** : Peut √™tre retriable ou non selon le contexte.

```python
def should_retry_408(response, attempt):
    # Si c'est un timeout c√¥t√© serveur pendant le traitement
    if response.headers.get('X-Timeout-Stage') == 'processing':
        return False  # Le serveur a re√ßu et commenc√© √† traiter

    # Si c'est un timeout d'idle connection
    if attempt == 1:
        return True  # Probablement une connexion qui a expir√©

    return False
```

#### HTTP 429 (Rate Limit)

**Retriable**, mais n√©cessite un traitement sp√©cial avec le header `Retry-After` :

```python
import time

def handle_rate_limit(response):
    if response.status_code == 429:
        # Respecter le Retry-After header
        retry_after = response.headers.get('Retry-After')

        if retry_after:
            if retry_after.isdigit():
                # Nombre de secondes
                wait_time = int(retry_after)
            else:
                # Date HTTP
                retry_date = parse_http_date(retry_after)
                wait_time = (retry_date - datetime.now()).total_seconds()

            # Limiter √† un maximum raisonnable
            wait_time = min(wait_time, 60)

            print(f"Rate limited, waiting {wait_time}s")
            time.sleep(wait_time)
            return True

    return False
```

#### Erreurs de base de donn√©es

```python
# PostgreSQL error codes
RETRIABLE_DB_ERRORS = {
    '40001',  # serialization_failure
    '40P01',  # deadlock_detected
    '08000',  # connection_exception
    '08003',  # connection_does_not_exist
    '08006',  # connection_failure
    '57P03',  # cannot_connect_now
}

def should_retry_db_error(error):
    if hasattr(error, 'pgcode'):
        return error.pgcode in RETRIABLE_DB_ERRORS

    # Pour les timeouts g√©n√©riques
    if 'timeout' in str(error).lower():
        return True

    return False
```

---

## Strat√©gies de retry

### 1. Immediate Retry (Retry imm√©diat)

**Le plus simple, rarement le meilleur.**

```python
def immediate_retry(func, max_attempts=3):
    for attempt in range(1, max_attempts + 1):
        try:
            return func()
        except Exception as e:
            if attempt == max_attempts:
                raise
            print(f"Attempt {attempt} failed, retrying immediately...")
            # Pas de d√©lai !
```

**Quand l'utiliser** :
- Erreurs tr√®s transitoires (paquet perdu)
- Premi√®re tentative apr√®s une erreur connue comme transitoire

**Inconv√©nients** :
- Peut saturer un serveur d√©j√† surcharg√©
- Gaspille des ressources si le probl√®me persiste
- Risque de "retry storm"

### 2. Fixed Delay Retry

**Attendre un temps fixe entre chaque tentative.**

```python
import time

def fixed_delay_retry(func, max_attempts=3, delay=2.0):
    for attempt in range(1, max_attempts + 1):
        try:
            return func()
        except Exception as e:
            if attempt == max_attempts:
                raise
            print(f"Attempt {attempt} failed, waiting {delay}s...")
            time.sleep(delay)
```

**Timing** :
```
Tentative 1:  0s
Tentative 2:  2s  (d√©lai fixe)
Tentative 3:  4s  (d√©lai fixe)
Total: 4s
```

**Quand l'utiliser** :
- Probl√®mes avec timing pr√©visible (rate limits connus)
- Syst√®mes batch moins sensibles √† la latence

**Inconv√©nients** :
- Tous les clients sont synchronis√©s (thundering herd)
- Ne s'adapte pas √† la gravit√© du probl√®me

### 3. Exponential Backoff (Backoff exponentiel)

**La strat√©gie recommand√©e pour la plupart des cas.**

Le d√©lai augmente exponentiellement : 1s, 2s, 4s, 8s, 16s...

```python
import time

def exponential_backoff_retry(
    func,
    max_attempts=5,
    base_delay=1.0,
    max_delay=32.0,
    exponential_base=2
):
    for attempt in range(1, max_attempts + 1):
        try:
            return func()
        except Exception as e:
            if attempt == max_attempts:
                raise

            # Calcul du d√©lai : base_delay * (exponential_base ^ (attempt - 1))
            delay = min(
                base_delay * (exponential_base ** (attempt - 1)),
                max_delay
            )

            print(f"Attempt {attempt} failed, waiting {delay}s...")
            time.sleep(delay)
```

**Timing avec base=2, base_delay=1s** :
```
Tentative 1:  0s
Tentative 2:  1s  (1 * 2^0 = 1s)
Tentative 3:  3s  (1 * 2^1 = 2s)
Tentative 4:  7s  (1 * 2^2 = 4s)
Tentative 5: 15s  (1 * 2^3 = 8s)
Total: 15s
```

**Avantages** :
- ‚úÖ Donne du temps au syst√®me pour r√©cup√©rer
- ‚úÖ R√©duit la charge progressive sur le serveur
- ‚úÖ S'adapte automatiquement aux probl√®mes persistants

**Formule g√©n√©rique** :
```
delay = min(base_delay * (base ^ attempts), max_delay)
```

### 4. Exponential Backoff avec Jitter

**La version am√©lior√©e : ajouter de l'al√©atoire pour √©viter la synchronisation.**

#### Le probl√®me du thundering herd

Sans jitter, tous les clients qui √©chouent simultan√©ment vont retrier simultan√©ment :

```
10 000 clients √©chouent √† t=0
    ‚îÇ
    ‚îú‚îÄ‚îÄ> Tous attendent 1s
    ‚îÇ
    ‚îî‚îÄ‚îÄ> Tous retentent √† t=1s ‚îÄ‚îÄ> Surcharge serveur
              ‚îÇ
              ‚îú‚îÄ‚îÄ> Tous attendent 2s
              ‚îÇ
              ‚îî‚îÄ‚îÄ> Tous retentent √† t=3s ‚îÄ‚îÄ> Surcharge encore
```

**Avec jitter**, les retries sont distribu√©s dans le temps :

```
10 000 clients √©chouent √† t=0
    ‚îÇ
    ‚îú‚îÄ‚îÄ> Attendent 0.5s - 1.5s (al√©atoire)
    ‚îÇ
    ‚îî‚îÄ‚îÄ> Retentent entre t=0.5s et t=1.5s ‚îÄ‚îÄ> Charge √©tal√©e ‚úì
```

#### Impl√©mentation : Full Jitter (recommand√© par AWS)

```python
import random
import time

def exponential_backoff_with_jitter(
    func,
    max_attempts=5,
    base_delay=1.0,
    max_delay=32.0
):
    for attempt in range(1, max_attempts + 1):
        try:
            return func()
        except Exception as e:
            if attempt == max_attempts:
                raise

            # Calcul du d√©lai maximal pour cette tentative
            max_wait = min(base_delay * (2 ** (attempt - 1)), max_delay)

            # Full jitter : d√©lai al√©atoire entre 0 et max_wait
            delay = random.uniform(0, max_wait)

            print(f"Attempt {attempt} failed, waiting {delay:.2f}s...")
            time.sleep(delay)
```

**Variantes de jitter** :

```python
# 1. Full Jitter (recommand√©)
delay = random.uniform(0, calculated_delay)

# 2. Equal Jitter (50% fixe + 50% al√©atoire)
delay = calculated_delay / 2 + random.uniform(0, calculated_delay / 2)

# 3. Decorrelated Jitter (chaque retry d√©pend du pr√©c√©dent)
delay = random.uniform(base_delay, previous_delay * 3)
```

**Comparaison visuelle** (5000 clients sur 5 tentatives) :

```
Sans jitter:
t=0s     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000 requ√™tes
t=1s     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000 requ√™tes
t=3s     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000 requ√™tes

Avec full jitter:
t=0-1s   |‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë| ~2500 requ√™tes
t=1-2s   |‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë| ~1000 requ√™tes
t=2-4s   |‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë| ~1500 requ√™tes
```

---

## Impl√©mentation par langage

### Python

#### Version simple avec d√©corateur

```python
import time
import random
from functools import wraps

def retry_with_backoff(
    max_attempts=3,
    base_delay=1.0,
    max_delay=32.0,
    exponential_base=2,
    jitter=True,
    retriable_exceptions=(Exception,)
):
    """
    D√©corateur pour retry avec exponential backoff et jitter.

    Usage:
        @retry_with_backoff(max_attempts=5, jitter=True)
        def call_api():
            return requests.get('https://api.example.com/data')
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(1, max_attempts + 1):
                try:
                    return func(*args, **kwargs)

                except retriable_exceptions as e:
                    if attempt == max_attempts:
                        raise

                    # Calcul du d√©lai
                    delay = min(
                        base_delay * (exponential_base ** (attempt - 1)),
                        max_delay
                    )

                    # Appliquer le jitter
                    if jitter:
                        delay = random.uniform(0, delay)

                    print(f"Attempt {attempt}/{max_attempts} failed: {e}")
                    print(f"Retrying in {delay:.2f}s...")
                    time.sleep(delay)

        return wrapper
    return decorator

# Utilisation
@retry_with_backoff(
    max_attempts=5,
    base_delay=1.0,
    jitter=True,
    retriable_exceptions=(requests.Timeout, requests.ConnectionError)
)
def fetch_user_data(user_id):
    response = requests.get(
        f'https://api.example.com/users/{user_id}',
        timeout=5
    )
    response.raise_for_status()
    return response.json()
```

#### Version avanc√©e avec classification des erreurs

```python
import requests
from enum import Enum

class RetryDecision(Enum):
    RETRY = "retry"
    FAIL = "fail"
    RETRY_AFTER = "retry_after"

def classify_error(exception, response=None):
    """D√©termine si une erreur est retriable."""

    # Erreurs r√©seau : toujours retriable
    if isinstance(exception, (requests.Timeout, requests.ConnectionError)):
        return RetryDecision.RETRY

    # Erreurs HTTP
    if isinstance(exception, requests.HTTPError):
        status_code = exception.response.status_code

        # Rate limiting avec Retry-After
        if status_code == 429:
            return RetryDecision.RETRY_AFTER

        # Erreurs serveur : retriable
        if 500 <= status_code < 600:
            return RetryDecision.RETRY

        # Erreurs client : non retriable
        if 400 <= status_code < 500:
            return RetryDecision.FAIL

    # Par d√©faut : non retriable
    return RetryDecision.FAIL

def smart_retry(
    func,
    max_attempts=3,
    base_delay=1.0,
    max_delay=60.0
):
    """Retry intelligent avec classification des erreurs."""
    for attempt in range(1, max_attempts + 1):
        try:
            return func()

        except requests.RequestException as e:
            decision = classify_error(e, getattr(e, 'response', None))

            if decision == RetryDecision.FAIL:
                raise  # Erreur non retriable

            if attempt == max_attempts:
                raise  # Derni√®re tentative

            # Calcul du d√©lai
            if decision == RetryDecision.RETRY_AFTER:
                # Respecter le Retry-After header
                retry_after = e.response.headers.get('Retry-After', '1')
                delay = min(int(retry_after), max_delay)
            else:
                # Exponential backoff avec jitter
                max_wait = min(base_delay * (2 ** (attempt - 1)), max_delay)
                delay = random.uniform(0, max_wait)

            print(f"Attempt {attempt} failed, retrying in {delay:.2f}s...")
            time.sleep(delay)
```

### Go

```go
package main

import (
    "context"
    "errors"
    "fmt"
    "math"
    "math/rand"
    "net/http"
    "time"
)

// RetryConfig configure le comportement du retry
type RetryConfig struct {
    MaxAttempts     int
    BaseDelay       time.Duration
    MaxDelay        time.Duration
    ExponentialBase float64
    Jitter          bool
}

// DefaultRetryConfig retourne une configuration par d√©faut
func DefaultRetryConfig() RetryConfig {
    return RetryConfig{
        MaxAttempts:     3,
        BaseDelay:       1 * time.Second,
        MaxDelay:        32 * time.Second,
        ExponentialBase: 2.0,
        Jitter:          true,
    }
}

// IsRetriable d√©termine si une erreur est retriable
func IsRetriable(err error, resp *http.Response) bool {
    // Timeout : retriable
    if errors.Is(err, context.DeadlineExceeded) {
        return true
    }

    // Erreur r√©seau : retriable
    if netErr, ok := err.(net.Error); ok && netErr.Timeout() {
        return true
    }

    // Codes HTTP retriables
    if resp != nil {
        switch resp.StatusCode {
        case 429, 500, 502, 503, 504:
            return true
        }
    }

    return false
}

// RetryWithBackoff ex√©cute une fonction avec retry et backoff
func RetryWithBackoff(
    ctx context.Context,
    config RetryConfig,
    fn func() (*http.Response, error),
) (*http.Response, error) {

    var lastErr error

    for attempt := 1; attempt <= config.MaxAttempts; attempt++ {
        // V√©rifier le contexte
        if err := ctx.Err(); err != nil {
            return nil, err
        }

        // Ex√©cuter la fonction
        resp, err := fn()

        // Succ√®s
        if err == nil && resp.StatusCode < 400 {
            return resp, nil
        }

        // √âchec non retriable
        if err != nil && !IsRetriable(err, resp) {
            return resp, err
        }

        // Derni√®re tentative
        if attempt == config.MaxAttempts {
            if err != nil {
                return resp, err
            }
            return resp, fmt.Errorf("max attempts reached, status: %d", resp.StatusCode)
        }

        // Calculer le d√©lai
        delay := calculateDelay(attempt, config)

        fmt.Printf("Attempt %d/%d failed, retrying in %v...\n",
            attempt, config.MaxAttempts, delay)

        // Attendre avec possibilit√© d'annulation
        select {
        case <-time.After(delay):
            // Continue
        case <-ctx.Done():
            return nil, ctx.Err()
        }

        lastErr = err
    }

    return nil, lastErr
}

// calculateDelay calcule le d√©lai avec exponential backoff et jitter
func calculateDelay(attempt int, config RetryConfig) time.Duration {
    // Exponential backoff
    delay := float64(config.BaseDelay) * math.Pow(
        config.ExponentialBase,
        float64(attempt-1),
    )

    // Limiter au max
    if delay > float64(config.MaxDelay) {
        delay = float64(config.MaxDelay)
    }

    // Appliquer le jitter (full jitter)
    if config.Jitter {
        delay = rand.Float64() * delay
    }

    return time.Duration(delay)
}

// Exemple d'utilisation
func main() {
    ctx := context.Background()
    config := DefaultRetryConfig()

    resp, err := RetryWithBackoff(ctx, config, func() (*http.Response, error) {
        return http.Get("https://api.example.com/data")
    })

    if err != nil {
        fmt.Printf("Request failed: %v\n", err)
        return
    }
    defer resp.Body.Close()

    fmt.Printf("Success: %d\n", resp.StatusCode)
}
```

### JavaScript (Node.js)

```javascript
/**
 * Retry avec exponential backoff et jitter
 */
class RetryStrategy {
    constructor(options = {}) {
        this.maxAttempts = options.maxAttempts || 3;
        this.baseDelay = options.baseDelay || 1000; // ms
        this.maxDelay = options.maxDelay || 32000;
        this.exponentialBase = options.exponentialBase || 2;
        this.jitter = options.jitter !== false; // true par d√©faut
    }

    /**
     * D√©termine si une erreur est retriable
     */
    isRetriable(error) {
        // Erreurs r√©seau
        if (error.code === 'ECONNREFUSED' ||
            error.code === 'ETIMEDOUT' ||
            error.code === 'ENOTFOUND') {
            return true;
        }

        // Codes HTTP retriables
        if (error.response) {
            const status = error.response.status;
            return status === 429 || status >= 500;
        }

        return false;
    }

    /**
     * Calcule le d√©lai avec backoff et jitter
     */
    calculateDelay(attempt) {
        // Exponential backoff
        let delay = this.baseDelay * Math.pow(this.exponentialBase, attempt - 1);

        // Limiter au max
        delay = Math.min(delay, this.maxDelay);

        // Appliquer le jitter (full jitter)
        if (this.jitter) {
            delay = Math.random() * delay;
        }

        return delay;
    }

    /**
     * Ex√©cute une fonction avec retry
     */
    async execute(fn) {
        let lastError;

        for (let attempt = 1; attempt <= this.maxAttempts; attempt++) {
            try {
                return await fn();
            } catch (error) {
                lastError = error;

                // Erreur non retriable
                if (!this.isRetriable(error)) {
                    throw error;
                }

                // Derni√®re tentative
                if (attempt === this.maxAttempts) {
                    throw error;
                }

                // Calculer et attendre
                const delay = this.calculateDelay(attempt);
                console.log(`Attempt ${attempt}/${this.maxAttempts} failed, retrying in ${delay}ms...`);

                await new Promise(resolve => setTimeout(resolve, delay));
            }
        }

        throw lastError;
    }
}

// Utilisation avec fetch
const retry = new RetryStrategy({
    maxAttempts: 5,
    baseDelay: 1000,
    jitter: true
});

async function fetchWithRetry(url) {
    return retry.execute(async () => {
        const response = await fetch(url, {
            timeout: 5000
        });

        if (!response.ok) {
            const error = new Error(`HTTP ${response.status}`);
            error.response = response;
            throw error;
        }

        return response.json();
    });
}

// Utilisation avec axios
const axios = require('axios');

async function callAPIWithRetry(endpoint) {
    return retry.execute(async () => {
        return axios.get(endpoint, {
            timeout: 5000
        });
    });
}

// Export
module.exports = { RetryStrategy };
```

### Java

```java
import java.io.IOException;
import java.net.http.*;
import java.time.Duration;
import java.util.Random;
import java.util.concurrent.CompletableFuture;

public class RetryStrategy {
    private final int maxAttempts;
    private final long baseDelayMs;
    private final long maxDelayMs;
    private final double exponentialBase;
    private final boolean jitter;
    private final Random random;

    public RetryStrategy(int maxAttempts, long baseDelayMs, long maxDelayMs,
                        double exponentialBase, boolean jitter) {
        this.maxAttempts = maxAttempts;
        this.baseDelayMs = baseDelayMs;
        this.maxDelayMs = maxDelayMs;
        this.exponentialBase = exponentialBase;
        this.jitter = jitter;
        this.random = new Random();
    }

    /**
     * D√©termine si une erreur est retriable
     */
    private boolean isRetriable(Throwable error, HttpResponse<?> response) {
        // Timeout ou IOException : retriable
        if (error instanceof java.net.http.HttpTimeoutException ||
            error instanceof IOException) {
            return true;
        }

        // Codes HTTP retriables
        if (response != null) {
            int status = response.statusCode();
            return status == 429 || (status >= 500 && status < 600);
        }

        return false;
    }

    /**
     * Calcule le d√©lai avec backoff et jitter
     */
    private long calculateDelay(int attempt) {
        // Exponential backoff
        long delay = (long) (baseDelayMs * Math.pow(exponentialBase, attempt - 1));

        // Limiter au max
        delay = Math.min(delay, maxDelayMs);

        // Appliquer le jitter (full jitter)
        if (jitter) {
            delay = (long) (random.nextDouble() * delay);
        }

        return delay;
    }

    /**
     * Ex√©cute une requ√™te avec retry
     */
    public <T> HttpResponse<T> execute(
        HttpClient client,
        HttpRequest request,
        HttpResponse.BodyHandler<T> bodyHandler
    ) throws IOException, InterruptedException {

        Exception lastException = null;

        for (int attempt = 1; attempt <= maxAttempts; attempt++) {
            try {
                HttpResponse<T> response = client.send(request, bodyHandler);

                // Succ√®s
                if (response.statusCode() < 400) {
                    return response;
                }

                // Erreur non retriable
                if (!isRetriable(null, response)) {
                    return response;
                }

                // Derni√®re tentative
                if (attempt == maxAttempts) {
                    return response;
                }

                // Calculer et attendre
                long delay = calculateDelay(attempt);
                System.out.printf("Attempt %d/%d failed (status %d), retrying in %dms...%n",
                    attempt, maxAttempts, response.statusCode(), delay);
                Thread.sleep(delay);

            } catch (IOException | InterruptedException e) {
                lastException = e;

                // Erreur non retriable
                if (!isRetriable(e, null)) {
                    throw e;
                }

                // Derni√®re tentative
                if (attempt == maxAttempts) {
                    throw e;
                }

                // Calculer et attendre
                long delay = calculateDelay(attempt);
                System.out.printf("Attempt %d/%d failed (%s), retrying in %dms...%n",
                    attempt, maxAttempts, e.getMessage(), delay);
                Thread.sleep(delay);
            }
        }

        throw new IOException("Max attempts exceeded", lastException);
    }

    // Exemple d'utilisation
    public static void main(String[] args) throws Exception {
        HttpClient client = HttpClient.newBuilder()
            .connectTimeout(Duration.ofSeconds(5))
            .build();

        HttpRequest request = HttpRequest.newBuilder()
            .uri(URI.create("https://api.example.com/data"))
            .timeout(Duration.ofSeconds(10))
            .build();

        RetryStrategy retry = new RetryStrategy(
            5,      // maxAttempts
            1000,   // baseDelayMs
            32000,  // maxDelayMs
            2.0,    // exponentialBase
            true    // jitter
        );

        HttpResponse<String> response = retry.execute(
            client,
            request,
            HttpResponse.BodyHandlers.ofString()
        );

        System.out.println("Response: " + response.body());
    }
}
```

---

## Patterns avanc√©s

### 1. Retry Budget (Budget de retries)

**Concept** : Limiter le nombre total de retries pour √©viter d'amplifier la charge.

```python
import time
from threading import Lock

class RetryBudget:
    """
    Limite le taux de retry global pour prot√©ger le syst√®me.

    Principe : Si trop de requ√™tes √©chouent (>10% par exemple),
    on arr√™te les retries pour ne pas aggraver la situation.
    """
    def __init__(self, window_size=60, max_retry_ratio=0.1):
        self.window_size = window_size  # Secondes
        self.max_retry_ratio = max_retry_ratio  # 10% max
        self.lock = Lock()
        self.events = []  # (timestamp, was_retry)

    def _cleanup_old_events(self):
        """Supprime les √©v√©nements hors de la fen√™tre."""
        now = time.time()
        cutoff = now - self.window_size
        self.events = [e for e in self.events if e[0] > cutoff]

    def can_retry(self):
        """D√©termine si on peut encore retry."""
        with self.lock:
            self._cleanup_old_events()

            if not self.events:
                return True

            total_requests = len(self.events)
            retry_requests = sum(1 for _, was_retry in self.events if was_retry)

            current_ratio = retry_requests / total_requests

            # Si on d√©passe le ratio, bloquer les nouveaux retries
            return current_ratio < self.max_retry_ratio

    def record_request(self, was_retry=False):
        """Enregistre une requ√™te."""
        with self.lock:
            self.events.append((time.time(), was_retry))

# Utilisation
budget = RetryBudget(window_size=60, max_retry_ratio=0.1)

def fetch_with_budget(url):
    for attempt in range(1, 4):
        # Enregistrer la requ√™te
        is_retry = attempt > 1
        budget.record_request(was_retry=is_retry)

        try:
            return requests.get(url, timeout=5)
        except Exception as e:
            if attempt < 3 and budget.can_retry():
                time.sleep(2 ** (attempt - 1))
                continue
            else:
                if not budget.can_retry():
                    print("Retry budget exhausted, failing fast")
                raise
```

### 2. Adaptive Retry (Retry adaptatif)

**Ajuster le nombre de tentatives selon le taux de succ√®s.**

```go
type AdaptiveRetry struct {
    mu              sync.RWMutex
    successCount    int
    failureCount    int
    baseMaxAttempts int
    window          int
}

func NewAdaptiveRetry(baseMaxAttempts int) *AdaptiveRetry {
    return &AdaptiveRetry{
        baseMaxAttempts: baseMaxAttempts,
        window:          100, // Fen√™tre de 100 requ√™tes
    }
}

func (ar *AdaptiveRetry) GetMaxAttempts() int {
    ar.mu.RLock()
    defer ar.mu.RUnlock()

    total := ar.successCount + ar.failureCount
    if total == 0 {
        return ar.baseMaxAttempts
    }

    successRate := float64(ar.successCount) / float64(total)

    // Si taux de succ√®s > 90% : augmenter les tentatives
    if successRate > 0.9 {
        return ar.baseMaxAttempts + 2
    }

    // Si taux de succ√®s < 50% : r√©duire les tentatives
    if successRate < 0.5 {
        return max(1, ar.baseMaxAttempts-1)
    }

    return ar.baseMaxAttempts
}

func (ar *AdaptiveRetry) RecordSuccess() {
    ar.mu.Lock()
    defer ar.mu.Unlock()

    ar.successCount++
    ar.rotateWindow()
}

func (ar *AdaptiveRetry) RecordFailure() {
    ar.mu.Lock()
    defer ar.mu.Unlock()

    ar.failureCount++
    ar.rotateWindow()
}

func (ar *AdaptiveRetry) rotateWindow() {
    total := ar.successCount + ar.failureCount
    if total > ar.window {
        // Garder les proportions mais r√©duire les compteurs
        ratio := float64(ar.window) / float64(total)
        ar.successCount = int(float64(ar.successCount) * ratio)
        ar.failureCount = int(float64(ar.failureCount) * ratio)
    }
}
```

### 3. Circuit Breaker + Retry

**Combiner circuit breaker et retry pour une r√©silience maximale.**

```python
from enum import Enum
import time

class CircuitState(Enum):
    CLOSED = "closed"        # Trafic normal
    OPEN = "open"            # Blocage des requ√™tes
    HALF_OPEN = "half_open"  # Test de r√©cup√©ration

class CircuitBreaker:
    def __init__(self, failure_threshold=5, recovery_timeout=60):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED

    def call(self, func):
        """Ex√©cute une fonction √† travers le circuit breaker."""
        # Si circuit ouvert, v√©rifier si on peut tester la r√©cup√©ration
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = CircuitState.HALF_OPEN
                print("Circuit breaker: OPEN -> HALF_OPEN")
            else:
                raise Exception("Circuit breaker is OPEN")

        try:
            result = func()
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise

    def _on_success(self):
        """Appel√© sur succ√®s."""
        if self.state == CircuitState.HALF_OPEN:
            self.state = CircuitState.CLOSED
            print("Circuit breaker: HALF_OPEN -> CLOSED")
        self.failure_count = 0

    def _on_failure(self):
        """Appel√© sur √©chec."""
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.failure_threshold:
            if self.state == CircuitState.CLOSED:
                self.state = CircuitState.OPEN
                print(f"Circuit breaker: CLOSED -> OPEN ({self.failure_count} failures)")
            elif self.state == CircuitState.HALF_OPEN:
                self.state = CircuitState.OPEN
                print("Circuit breaker: HALF_OPEN -> OPEN (test failed)")

# Combiner circuit breaker et retry
def resilient_call(url, breaker, max_retries=3):
    """
    Appel r√©silient combinant circuit breaker et retry.

    Strat√©gie :
    1. V√©rifier le circuit breaker
    2. Si ferm√©/half-open, tenter avec retry
    3. Enregistrer le r√©sultat dans le circuit breaker
    """
    def make_request():
        for attempt in range(1, max_retries + 1):
            try:
                response = requests.get(url, timeout=5)
                response.raise_for_status()
                return response
            except Exception as e:
                if attempt == max_retries:
                    raise
                delay = 2 ** (attempt - 1) * random.uniform(0.5, 1.5)
                time.sleep(delay)

    # Ex√©cuter √† travers le circuit breaker
    return breaker.call(make_request)

# Utilisation
breaker = CircuitBreaker(failure_threshold=5, recovery_timeout=30)

try:
    response = resilient_call('https://api.example.com/data', breaker)
    print("Success:", response.status_code)
except Exception as e:
    print("Failed:", e)
```

---

## Cas d'usage r√©els

### 1. AWS SDK : Retry automatique

Les SDK AWS impl√©mentent des retries automatiques avec exponential backoff :

```python
import boto3
from botocore.config import Config

# Configuration des retries
retry_config = Config(
    retries={
        'max_attempts': 5,
        'mode': 'adaptive'  # ou 'standard', 'legacy'
    }
)

# Client S3 avec retry configur√©
s3 = boto3.client('s3', config=retry_config)

# Les erreurs 5xx et throttling sont automatiquement retry√©es
response = s3.get_object(Bucket='my-bucket', Key='my-file.txt')
```

**Modes de retry AWS** :
- **Legacy** : Backoff exponentiel simple (2 tentatives par d√©faut)
- **Standard** : 3 tentatives avec backoff exponentiel et jitter
- **Adaptive** : Ajuste dynamiquement selon le taux d'erreur

### 2. Stripe API : Idempotence et retry

Stripe recommande d'utiliser des cl√©s d'idempotence avec retry :

```python
import stripe
import uuid

stripe.api_key = 'sk_test_...'

def create_payment_with_retry(amount, customer):
    """Cr√©er un paiement avec retry s√ªr gr√¢ce √† l'idempotence."""
    # G√©n√©rer une cl√© d'idempotence unique
    idempotency_key = str(uuid.uuid4())

    for attempt in range(1, 4):
        try:
            charge = stripe.Charge.create(
                amount=amount,
                currency='usd',
                customer=customer,
                idempotency_key=idempotency_key  # Cl√© d'idempotence
            )
            return charge

        except stripe.error.RateLimitError:
            # Rate limit : attendre et retry
            delay = 2 ** (attempt - 1)
            time.sleep(delay)

        except stripe.error.APIConnectionError:
            # Erreur r√©seau : retry
            if attempt < 3:
                delay = 2 ** (attempt - 1) * random.uniform(0.5, 1.5)
                time.sleep(delay)
            else:
                raise

        except stripe.error.StripeError:
            # Autres erreurs Stripe : ne pas retry
            raise
```

### 3. Kafka Producer : Retry avec garanties

Kafka offre des retries configurables avec garanties de livraison :

```java
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

// Configuration des retries
props.put("retries", 3);  // Nombre de retries
props.put("retry.backoff.ms", 100);  // D√©lai entre retries
props.put("delivery.timeout.ms", 120000);  // Timeout total (2 minutes)
props.put("enable.idempotence", true);  // Idempotence pour √©viter les doublons

// Garantie de livraison
props.put("acks", "all");  // Attendre tous les r√©plicas

KafkaProducer<String, String> producer = new KafkaProducer<>(props);

// Envoi avec callback pour g√©rer les retries √©puis√©s
producer.send(
    new ProducerRecord<>("my-topic", "key", "value"),
    (metadata, exception) -> {
        if (exception != null) {
            System.err.println("Failed after retries: " + exception);
            // Logique de fallback : dead letter queue, logging, etc.
        } else {
            System.out.println("Sent to partition " + metadata.partition());
        }
    }
);
```

### 4. Database connection pooling avec retry

```python
import psycopg2
from psycopg2 import pool
import time

class ResilientConnectionPool:
    """Pool de connexions DB avec retry sur acquisition."""

    def __init__(self, minconn=1, maxconn=10, **db_params):
        self.pool = psycopg2.pool.ThreadedConnectionPool(
            minconn, maxconn, **db_params
        )

    def get_connection(self, max_retries=3):
        """Obtenir une connexion avec retry."""
        for attempt in range(1, max_retries + 1):
            try:
                conn = self.pool.getconn()
                # Tester la connexion
                with conn.cursor() as cur:
                    cur.execute("SELECT 1")
                return conn

            except psycopg2.OperationalError as e:
                if attempt == max_retries:
                    raise

                print(f"Connection failed (attempt {attempt}), retrying...")
                delay = 2 ** (attempt - 1)
                time.sleep(delay)

    def execute_with_retry(self, query, params=None, max_retries=3):
        """Ex√©cuter une query avec retry sur deadlock/serialization."""
        for attempt in range(1, max_retries + 1):
            conn = None
            try:
                conn = self.get_connection()
                with conn.cursor() as cur:
                    cur.execute(query, params)
                    result = cur.fetchall() if cur.description else None
                conn.commit()
                return result

            except psycopg2.errors.SerializationFailure:
                # Deadlock ou conflit de s√©rialisation : retry
                if conn:
                    conn.rollback()
                if attempt < max_retries:
                    delay = 0.1 * (2 ** (attempt - 1))
                    time.sleep(delay)
                else:
                    raise

            except Exception as e:
                if conn:
                    conn.rollback()
                raise

            finally:
                if conn:
                    self.pool.putconn(conn)

# Utilisation
db_pool = ResilientConnectionPool(
    minconn=2,
    maxconn=20,
    host='localhost',
    database='mydb',
    user='user',
    password='pass'
)

# Ex√©cuter une query avec retry automatique
result = db_pool.execute_with_retry(
    "UPDATE accounts SET balance = balance - %s WHERE id = %s",
    (100, 'user123')
)
```

---

## Monitoring et m√©triques

### M√©triques essentielles √† traquer

```python
from prometheus_client import Counter, Histogram, Gauge

# Compteur de retries par service et raison
retry_counter = Counter(
    'http_request_retries_total',
    'Total number of retries',
    ['service', 'reason']  # timeout, 5xx, connection_error
)

# Histogramme du nombre de tentatives par requ√™te
attempts_histogram = Histogram(
    'http_request_attempts',
    'Number of attempts per request',
    ['service'],
    buckets=[1, 2, 3, 4, 5, 10]
)

# Gauge du retry budget restant
retry_budget_gauge = Gauge(
    'retry_budget_remaining',
    'Remaining retry budget (0-1)',
    ['service']
)

# Taux de succ√®s apr√®s retry
retry_success_counter = Counter(
    'http_request_retry_success_total',
    'Requests that succeeded after retry',
    ['service', 'attempt']
)

def instrumented_retry(service, func, max_attempts=3):
    """Retry avec instrumentation compl√®te."""
    for attempt in range(1, max_attempts + 1):
        try:
            result = func()

            # Enregistrer le nombre de tentatives
            attempts_histogram.labels(service=service).observe(attempt)

            # Succ√®s apr√®s retry ?
            if attempt > 1:
                retry_success_counter.labels(
                    service=service,
                    attempt=str(attempt)
                ).inc()

            return result

        except requests.Timeout as e:
            retry_counter.labels(service=service, reason='timeout').inc()
            if attempt == max_attempts:
                raise

        except requests.HTTPError as e:
            reason = f'http_{e.response.status_code}'
            retry_counter.labels(service=service, reason=reason).inc()
            if attempt == max_attempts:
                raise

        # Backoff
        time.sleep(2 ** (attempt - 1) * random.random())
```

### Dashboard Grafana

**Requ√™tes Prometheus utiles** :

```promql
# Taux de retry par service (sur 5 minutes)
rate(http_request_retries_total[5m])

# Pourcentage de requ√™tes n√©cessitant un retry
rate(http_request_retries_total[5m])
/
rate(http_requests_total[5m]) * 100

# Distribution du nombre de tentatives
histogram_quantile(0.95,
  rate(http_request_attempts_bucket[5m])
)

# Taux de succ√®s apr√®s retry
sum(rate(http_request_retry_success_total[5m])) by (attempt)

# Budget retry restant moyen
avg(retry_budget_remaining) by (service)
```

**Alertes recommand√©es** :

```yaml
groups:
  - name: retry_alerts
    rules:
      # Taux de retry √©lev√© (>20% des requ√™tes)
      - alert: HighRetryRate
        expr: |
          (rate(http_request_retries_total[5m])
          /
          rate(http_requests_total[5m])) > 0.2
        for: 5m
        annotations:
          summary: "High retry rate on {{ $labels.service }}"
          description: "{{ $value | humanizePercentage }} of requests require retry"

      # Retry budget √©puis√©
      - alert: RetryBudgetExhausted
        expr: retry_budget_remaining < 0.1
        for: 2m
        annotations:
          summary: "Retry budget nearly exhausted for {{ $labels.service }}"

      # Taux de succ√®s apr√®s retry faible
      - alert: LowRetrySuccessRate
        expr: |
          (sum(rate(http_request_retry_success_total[5m]))
          /
          sum(rate(http_request_retries_total[5m]))) < 0.5
        for: 10m
        annotations:
          summary: "Low success rate after retry"
```

---

## Anti-patterns et pi√®ges

### ‚ùå 1. Retry sur erreurs non-retriable

```python
# Anti-pattern : retry sur toutes les erreurs
@retry(tries=3)
def create_user(email):
    response = requests.post('/users', json={'email': email})
    response.raise_for_status()
    return response.json()

# Si l'email existe d√©j√† (400 Bad Request), retry 3 fois inutilement !
```

**Solution** :
```python
def should_retry(exception):
    if isinstance(exception, requests.HTTPError):
        # Ne retry QUE les 5xx et 429
        return exception.response.status_code >= 500 or \
               exception.response.status_code == 429
    return isinstance(exception, (requests.Timeout, requests.ConnectionError))

@retry(retry=retry_if_exception(should_retry))
def create_user(email):
    # ...
```

### ‚ùå 2. Retry sans idempotence

```python
# DANGER : Retry sur op√©ration non-idempotente
@retry(tries=3)
def charge_credit_card(amount, card):
    # Si timeout apr√®s que le paiement a r√©ussi c√¥t√© serveur,
    # le retry va charger 2 ou 3 fois la carte !
    return payment_api.charge(amount, card)
```

**Solution** : Utiliser des cl√©s d'idempotence
```python
import uuid

@retry(tries=3)
def charge_credit_card(amount, card):
    idempotency_key = str(uuid.uuid4())
    return payment_api.charge(
        amount,
        card,
        idempotency_key=idempotency_key
    )
```

### ‚ùå 3. Retry sans limite temporelle

```python
# Anti-pattern : peut prendre une √©ternit√©
@retry(tries=10, delay=5)  # Jusqu'√† 50 secondes !
def fetch_data():
    return requests.get(url, timeout=30)
```

**Solution** : Limiter le temps total
```python
import time

def retry_with_deadline(func, max_time=30, max_attempts=5):
    start = time.time()

    for attempt in range(1, max_attempts + 1):
        # V√©rifier le budget temps
        elapsed = time.time() - start
        if elapsed > max_time:
            raise TimeoutError(f"Deadline exceeded after {elapsed:.1f}s")

        try:
            return func()
        except Exception:
            if attempt == max_attempts:
                raise
            # Calculer d√©lai sans d√©passer le deadline
            delay = min(2 ** (attempt - 1), max_time - elapsed - 1)
            if delay > 0:
                time.sleep(delay)
```

### ‚ùå 4. Retry sans jitter (thundering herd)

```python
# Anti-pattern : tous les clients retryent en m√™me temps
for i in range(3):
    try:
        return api_call()
    except:
        time.sleep(2)  # D√©lai fixe sans jitter
```

**Impact** : Lors d'une panne, des milliers de clients retryent simultan√©ment toutes les 2 secondes, cr√©ant des pics de charge.

**Solution** : Toujours ajouter du jitter
```python
import random

for i in range(3):
    try:
        return api_call()
    except:
        # Jitter : d√©lai al√©atoire entre 1s et 3s
        time.sleep(1 + random.random() * 2)
```

### ‚ùå 5. Ignorer les headers Retry-After

```python
# Anti-pattern : ignorer les instructions du serveur
@retry(tries=5, delay=2)
def call_api():
    response = requests.get('/api/data')
    response.raise_for_status()
    # Le serveur envoie "Retry-After: 60" mais on retry apr√®s 2s !
```

**Solution** : Respecter Retry-After
```python
def smart_retry(url, max_attempts=3):
    for attempt in range(1, max_attempts + 1):
        try:
            response = requests.get(url)
            response.raise_for_status()
            return response

        except requests.HTTPError as e:
            if e.response.status_code == 429:
                # Respecter Retry-After
                retry_after = e.response.headers.get('Retry-After', '60')
                delay = int(retry_after)
                print(f"Rate limited, waiting {delay}s as instructed")
                time.sleep(delay)
            elif attempt < max_attempts:
                delay = 2 ** (attempt - 1) * random.random()
                time.sleep(delay)
            else:
                raise
```

---

## Checklist pour production

Avant de d√©ployer une logique de retry :

- [ ] **Classification des erreurs** : Liste des erreurs retriable vs non-retriable
- [ ] **Nombre de tentatives** : D√©fini selon la criticit√© (3-5 g√©n√©ralement)
- [ ] **Exponential backoff** : Impl√©ment√© avec base 2
- [ ] **Jitter** : Ajout√© pour √©viter la synchronisation
- [ ] **Max delay** : Limit√© (30-60s g√©n√©ralement)
- [ ] **Deadline global** : Temps total limit√©
- [ ] **Idempotence** : Garantie pour op√©rations de mutation
- [ ] **Retry-After** : Headers respect√©s pour rate limiting
- [ ] **Monitoring** : M√©triques de retry expos√©es
- [ ] **Alertes** : Configur√©es pour taux de retry √©lev√©
- [ ] **Logging** : Contexte complet (tentative, raison, d√©lai)
- [ ] **Tests** : Sc√©narios de retry test√©s
- [ ] **Documentation** : Strat√©gie document√©e pour l'√©quipe
- [ ] **Circuit breaker** : Consid√©r√© pour protection additionnelle
- [ ] **Retry budget** : Impl√©ment√© si charge √©lev√©e

---

## Conclusion

Le retry avec exponential backoff et jitter est une **technique fondamentale** pour construire des syst√®mes distribu√©s fiables. Bien impl√©ment√©, il transforme des √©checs transitoires en succ√®s transparents pour l'utilisateur.

**Points cl√©s √† retenir** :

1. **Classifier les erreurs** : Ne retry QUE les erreurs retriable (timeouts, 5xx, 429)

2. **Exponential backoff + jitter** : Toujours combiner les deux pour √©viter les retry storms

3. **Limites multiples** : Max attempts ET max total time

4. **Idempotence** : Indispensable pour les op√©rations de mutation

5. **Monitoring** : Traquer taux de retry, nombre de tentatives, succ√®s apr√®s retry

6. **Respecter le serveur** : Headers Retry-After, rate limits, circuit breaker

**Formule magique** :
```
Retry r√©ussi = Exponential Backoff + Jitter + Classification + Idempotence + Monitoring
```

**Prochaine √©tape** : Les retries permettent de r√©cup√©rer des √©checs transitoires, mais que faire quand un service √©choue de mani√®re persistante ? La section suivante explore les **circuit breakers**, un pattern qui prot√®ge votre syst√®me en arr√™tant temporairement les appels vers des services d√©faillants.

---


‚è≠Ô∏è [Circuit breakers](/08-programmation-reseau/08.3-circuit-breakers.md)
