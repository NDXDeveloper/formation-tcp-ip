üîù Retour au [Sommaire](/SOMMAIRE.md)

# 7.1 M√©thodologie de diagnostic r√©seau

## Introduction

Face √† un probl√®me r√©seau, la tentation est grande de se pr√©cipiter sur Wireshark ou de taper fr√©n√©tiquement des commandes en esp√©rant tomber sur la solution. Cette approche improvis√©e conduit g√©n√©ralement √† perdre du temps, √† n√©gliger des pistes importantes, et parfois m√™me √† aggraver le probl√®me.

Une m√©thodologie de diagnostic structur√©e est la cl√© pour r√©soudre efficacement les probl√®mes r√©seau. Elle permet de :
- **Gagner du temps** en √©vitant les tentatives al√©atoires
- **Identifier la cause racine** plut√¥t que les sympt√¥mes
- **Communiquer efficacement** avec les √©quipes concern√©es
- **Documenter** pour √©viter que le probl√®me ne se reproduise
- **Maintenir son calme** m√™me sous pression

Cette section pr√©sente une approche m√©thodique √©prouv√©e, inspir√©e des meilleures pratiques de l'industrie et applicable √† tout environnement, du petit r√©seau local au datacenter d'entreprise.

---

## Le cadre g√©n√©ral : les 6 √©tapes du diagnostic

Tout diagnostic r√©seau efficace suit un processus en six √©tapes :

```
1. D√âFINIR le probl√®me
   ‚Üì
2. COLLECTER les informations
   ‚Üì
3. ANALYSER les sympt√¥mes
   ‚Üì
4. FORMULER des hypoth√®ses
   ‚Üì
5. TESTER les hypoth√®ses
   ‚Üì
6. R√âSOUDRE et DOCUMENTER
```

Chacune de ces √©tapes est essentielle. Sauter une √©tape ou les ex√©cuter dans le d√©sordre compromet l'efficacit√© du diagnostic.

---

## √âtape 1 : D√©finir le probl√®me

### Pourquoi c'est crucial

Un probl√®me mal d√©fini conduit √† des recherches inefficaces. "Le r√©seau ne marche pas" n'est pas une d√©finition de probl√®me, c'est un sympt√¥me vague qui peut avoir des centaines de causes.

### Les bonnes questions

Posez syst√©matiquement ces questions :

#### **Qui est affect√© ?**
- Un seul utilisateur ou tous ?
- Un d√©partement sp√©cifique ?
- Une localisation g√©ographique ?
- Un type de terminal (mobiles, PC, serveurs) ?

**Exemple :**
```
‚ùå Vague : "Les utilisateurs ne peuvent pas acc√©der √† l'application"
‚úÖ Pr√©cis : "Les 50 utilisateurs du bureau de Lyon ne peuvent pas acc√©der
            √† l'application CRM depuis leurs PC Windows, mais les utilisateurs
            de Paris y acc√®dent normalement"
```

#### **Quoi exactement ne fonctionne pas ?**
- Quelle application/service est concern√© ?
- Quelle fonctionnalit√© sp√©cifique √©choue ?
- Quel message d'erreur appara√Æt (verbatim) ?

**Exemple :**
```
‚ùå Vague : "Le site web est lent"
‚úÖ Pr√©cis : "La page de checkout affiche 'ERR_CONNECTION_TIMED_OUT' apr√®s
            30 secondes, uniquement sur HTTPS (port 443). HTTP (port 80)
            fonctionne normalement"
```

#### **Quand le probl√®me se produit-il ?**
- En permanence ou de mani√®re intermittente ?
- √Ä certaines heures de la journ√©e ?
- Depuis quand exactement ?
- Y a-t-il un pattern temporel ?

**Exemple :**
```
‚ùå Vague : "Parfois l'API ne r√©pond pas"
‚úÖ Pr√©cis : "Depuis lundi 14h, l'API r√©pond avec HTTP 504 Gateway Timeout
            tous les jours entre 9h et 11h (heures de pointe).
            Le reste de la journ√©e, tout fonctionne normalement"
```

#### **O√π le probl√®me se situe-t-il ?**
- Sur le r√©seau local ou distant ?
- Entre quels points (client ‚Üí serveur) ?
- Sur quel segment r√©seau ?

#### **Le probl√®me est-il nouveau ?**
- Cela a-t-il d√©j√† fonctionn√© auparavant ?
- Y a-t-il eu un changement r√©cent (configuration, mise √† jour, nouveau d√©ploiement) ?

### Formaliser le probl√®me

R√©digez une d√©claration de probl√®me en une phrase claire :

**Template :**
```
[QUI] ne peut pas [QUOI] [QUAND],
alors que [COMPORTEMENT ATTENDU]
```

**Exemples :**

```
Probl√®me 1 :
Les utilisateurs du VLAN 10 ne peuvent pas r√©soudre les noms de domaine
externes depuis ce matin 8h, alors que la r√©solution DNS fonctionnait
hier encore.

Probl√®me 2 :
Le serveur d'application backend (10.0.5.20) ne peut pas √©tablir de
connexion TCP vers la base de donn√©es PostgreSQL (10.0.8.50:5432)
depuis le d√©ploiement d'hier 18h, alors que la connexion √©tait stable
depuis 3 mois.

Probl√®me 3 :
Les clients mobiles iOS perdent leur connexion WebSocket toutes les
5 minutes exactement, depuis la mise √† jour du reverse proxy nginx
ce matin, alors que les clients Android restent connect√©s normalement.
```

---

## √âtape 2 : Collecter les informations

### Informations de contexte

Avant m√™me de toucher √† un outil, rassemblez le contexte :

#### **Architecture r√©seau**
- Sch√©ma du r√©seau (m√™me basique)
- Adresses IP impliqu√©es
- √âquipements interm√©diaires (routeurs, firewalls, proxys)
- Segmentation (VLANs, sous-r√©seaux)

#### **Configuration actuelle**
- Adressage IP des machines concern√©es
- Configuration DNS
- Routes configur√©es
- R√®gles de firewall applicables

#### **Historique**
- Logs syst√®me r√©cents
- Changements r√©cents (change log)
- Incidents similaires pass√©s
- Tickets de support li√©s

### Collecte technique initiale

Rassemblez les donn√©es de base sur les machines concern√©es :

**Sur le client (machine affect√©e) :**
```bash
# Configuration r√©seau
ip addr show          # Linux
ipconfig /all         # Windows
ifconfig -a           # macOS

# Table de routage
ip route              # Linux
route print           # Windows
netstat -rn           # macOS

# Serveurs DNS configur√©s
cat /etc/resolv.conf  # Linux/macOS
ipconfig /all         # Windows

# Connexions actives
ss -tuln              # Linux (ou netstat -tuln)
netstat -ano          # Windows
```

**Sur le serveur (machine cible) :**
```bash
# √âtat du service
systemctl status nginx        # Linux
Get-Service -Name "W3SVC"    # Windows PowerShell

# Ports en √©coute
ss -tlnp | grep :443          # Linux
netstat -ano | findstr :443   # Windows

# Logs r√©cents
journalctl -u nginx -n 50     # Linux
Get-EventLog -LogName System  # Windows
```

### Cr√©er une timeline

√âtablissez une chronologie des √©v√©nements :

```
Timeline Exemple :

Hier 17:45  - D√©ploiement nouvelle version de l'application
Hier 18:00  - Red√©marrage du serveur web
Hier 18:15  - Premiers signalements d'erreurs HTTP 502
Hier 18:30  - 25% des requ√™tes en √©chec selon monitoring
Aujourd'hui 09:00 - 60% des requ√™tes en √©chec
Aujourd'hui 09:15 - Investigation d√©marr√©e
```

---

## √âtape 3 : Analyser les sympt√¥mes

### Utiliser le mod√®le OSI comme guide

Le mod√®le OSI fournit un cadre parfait pour analyser syst√©matiquement les sympt√¥mes :

```
Couche 7 - Application    ‚îÇ Probl√®me dans l'application elle-m√™me ?
Couche 6 - Pr√©sentation   ‚îÇ Probl√®me de chiffrement/encodage ?
Couche 5 - Session        ‚îÇ Probl√®me d'√©tablissement de session ?
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Couche 4 - Transport      ‚îÇ Probl√®me TCP/UDP (ports, connexions) ?
Couche 3 - R√©seau         ‚îÇ Probl√®me IP (routage, adressage) ?
Couche 2 - Liaison        ‚îÇ Probl√®me Ethernet (switching, VLAN) ?
Couche 1 - Physique       ‚îÇ Probl√®me c√¢ble/carte r√©seau ?
```

### Questions par couche

#### **Couche 1 - Physique**
- Les c√¢bles sont-ils branch√©s ?
- Les LEDs des cartes r√©seau sont-elles allum√©es ?
- Y a-t-il des erreurs CRC sur les interfaces ?

**V√©rification :**
```bash
# Linux : v√©rifier l'√©tat du lien
ethtool eth0 | grep "Link detected"

# Statistiques d'erreurs
ethtool -S eth0 | grep -i error
```

#### **Couche 2 - Liaison**
- L'adresse MAC de destination est-elle accessible ?
- Le bon VLAN est-il configur√© ?
- Y a-t-il des probl√®mes de spanning tree ?

**V√©rification :**
```bash
# Afficher le cache ARP
arp -a                    # Tous syst√®mes
ip neigh show            # Linux moderne

# V√©rifier le VLAN (sur un switch)
show vlan brief          # Cisco
```

#### **Couche 3 - R√©seau**
- L'adresse IP est-elle correcte ?
- La passerelle est-elle joignable ?
- Le routage est-il configur√© ?

**V√©rification :**
```bash
# Tester la passerelle
ping 192.168.1.1

# V√©rifier la table de routage
ip route get 8.8.8.8     # Linux : voir quelle route serait utilis√©e
```

#### **Couche 4 - Transport**
- Le port est-il ouvert sur le serveur ?
- Y a-t-il un firewall qui bloque ?
- La connexion TCP s'√©tablit-elle ?

**V√©rification :**
```bash
# Tester l'ouverture d'un port TCP
telnet 10.0.5.20 443
nc -zv 10.0.5.20 443     # netcat
curl -v telnet://10.0.5.20:443

# Voir les connexions √©tablies
ss -tn state established
```

#### **Couches 5-7 - Session/Pr√©sentation/Application**
- Le protocole applicatif fonctionne-t-il ?
- Les certificats TLS sont-ils valides ?
- L'authentification r√©ussit-elle ?

**V√©rification :**
```bash
# Tester HTTP
curl -I https://example.com

# Tester HTTPS avec d√©tails TLS
openssl s_client -connect example.com:443 -servername example.com

# Tester DNS
dig example.com
nslookup example.com
```

### Classification des sympt√¥mes

Cat√©gorisez le sympt√¥me observ√© :

**Probl√®me de connectivit√© pure**
```
Sympt√¥me : Aucun paquet n'atteint la destination
Couches probables : 1, 2, 3 (Physique, Liaison, R√©seau)
```

**Probl√®me de service**
```
Sympt√¥me : Le serveur est joignable mais le service ne r√©pond pas
Couches probables : 4, 7 (Transport, Application)
```

**Probl√®me de performance**
```
Sympt√¥me : √áa fonctionne mais c'est lent
Toutes les couches peuvent √™tre impliqu√©es
```

**Probl√®me intermittent**
```
Sympt√¥me : √áa marche parfois, parfois non
Peut indiquer : saturation, timeout, probl√®me de MTU, load balancing
```

---

## √âtape 4 : Formuler des hypoth√®ses

### Principe de l'hypoth√®se

Une hypoth√®se est une explication **testable** du probl√®me. Elle doit √™tre :
- **Sp√©cifique** : pointer vers une cause pr√©cise
- **Testable** : on doit pouvoir la valider ou l'invalider
- **Fond√©e** : bas√©e sur les sympt√¥mes observ√©s

### G√©n√©rer des hypoth√®ses

Pour chaque probl√®me, listez 3 √† 5 hypoth√®ses plausibles, class√©es par probabilit√©.

**Exemple 1 : Connexion impossible √† une API**

```
Probl√®me :
L'application web ne peut pas se connecter √† l'API backend
(10.0.5.20:8080) depuis ce matin.

Hypoth√®ses (de la plus probable √† la moins probable) :

H1. Le firewall bloque le port 8080
    ‚Üí Test : v√©rifier les r√®gles de firewall, essayer telnet

H2. Le service API n'est pas d√©marr√© sur le serveur
    ‚Üí Test : v√©rifier le statut du processus sur 10.0.5.20

H3. Une route r√©seau est manquante apr√®s un changement
    ‚Üí Test : v√©rifier la table de routage, faire un traceroute

H4. L'IP du serveur API a chang√© et le DNS n'est pas √† jour
    ‚Üí Test : v√©rifier la r√©solution DNS, comparer avec /etc/hosts

H5. Saturation CPU/m√©moire sur le serveur API emp√™chant les connexions
    ‚Üí Test : v√©rifier les m√©triques syst√®me du serveur
```

**Exemple 2 : Site web tr√®s lent**

```
Probl√®me :
Le site web met 30 secondes √† charger alors qu'il √©tait instantan√© hier.

Hypoth√®ses :

H1. Latence r√©seau excessive (WAN, ISP)
    ‚Üí Test : ping, traceroute, mesure du RTT

H2. Serveur web surcharg√© (trop de connexions)
    ‚Üí Test : v√©rifier la charge CPU, nombre de connexions actives

H3. Base de donn√©es lente (requ√™tes non optimis√©es)
    ‚Üí Test : logs de requ√™tes SQL, temps de r√©ponse DB

H4. Probl√®me de MTU causant de la fragmentation
    ‚Üí Test : ping avec DF flag, v√©rifier MTU sur le chemin

H5. Cache CDN expir√©, tous les clients frappent l'origin
    ‚Üí Test : v√©rifier les headers Cache-Control, hit ratio CDN
```

### Utiliser le principe d'Occam

La loi de parcimonie (rasoir d'Occam) s'applique au diagnostic r√©seau :

> **L'explication la plus simple est g√©n√©ralement la bonne**

Privil√©giez les hypoth√®ses simples avant les sc√©narios complexes :

```
‚úÖ D'abord : "Le c√¢ble est d√©branch√©"
‚ùå Pas d'abord : "Il y a une boucle de routage BGP au niveau Tier-1"

‚úÖ D'abord : "Le firewall bloque le port"
‚ùå Pas d'abord : "Il y a une corruption de paquets due √† un bug kernel"
```

### Prioriser les hypoth√®ses

Classez vos hypoth√®ses selon :

1. **Probabilit√©** : bas√©e sur votre exp√©rience et les sympt√¥mes
2. **Impact** : hypoth√®ses critiques en premier
3. **Facilit√© de test** : si deux hypoth√®ses sont √©quiprobables, testez d'abord la plus simple

---

## √âtape 5 : Tester les hypoth√®ses

### Approches de test

Il existe trois approches principales pour tester les hypoth√®ses :

#### **1. Approche Bottom-Up (de bas en haut)**

On commence par la couche physique et on remonte :

```
√âtape 1 : Physique    ‚Üí C√¢ble branch√© ? LED allum√©e ?
√âtape 2 : Liaison     ‚Üí ARP fonctionne ?
√âtape 3 : R√©seau      ‚Üí Ping de la passerelle ?
√âtape 4 : Transport   ‚Üí Port ouvert ? TCP connecte ?
√âtape 5 : Application ‚Üí Service r√©pond ?
```

**Avantages :**
- M√©thodique et exhaustif
- Ne rate aucune couche
- Bon pour les d√©butants

**Inconv√©nients :**
- Peut √™tre long
- Teste parfois des choses qui fonctionnent

**Quand l'utiliser :**
- Probl√®me de connectivit√© totale
- Nouvelle installation
- Environnement inconnu

#### **2. Approche Top-Down (de haut en bas)**

On commence par l'application et on descend :

```
√âtape 1 : Application ‚Üí L'app r√©pond ? Erreur pr√©cise ?
√âtape 2 : Transport   ‚Üí TCP √©tabli ? Timeout ?
√âtape 3 : R√©seau      ‚Üí Routage OK ? IP joignable ?
√âtape 4 : Liaison     ‚Üí ARP r√©solu ?
√âtape 5 : Physique    ‚Üí Lien physique OK ?
```

**Avantages :**
- Rapide pour les probl√®mes applicatifs
- Cible directement le sympt√¥me

**Inconv√©nients :**
- Peut n√©gliger les couches basses
- N√©cessite de l'exp√©rience

**Quand l'utiliser :**
- Erreur applicative claire (HTTP 500, DNS failure)
- Environnement connu et stable
- Probl√®me r√©cent sur syst√®me qui fonctionnait

#### **3. Approche Divide and Conquer (diviser pour r√©gner)**

On teste au milieu et on √©limine la moiti√© du probl√®me :

```
Test initial : Ping du serveur final
  ‚îú‚îÄ Succ√®s ‚Üí Probl√®me dans Transport/Application
  ‚îî‚îÄ √âchec  ‚Üí Probl√®me dans R√©seau/Liaison/Physique
      ‚îî‚îÄ Test : Ping de la passerelle
          ‚îú‚îÄ Succ√®s ‚Üí Probl√®me de routage/r√©seau distant
          ‚îî‚îÄ √âchec  ‚Üí Probl√®me local (liaison/physique)
```

**Avantages :**
- Tr√®s efficace, √©limine rapidement des pistes
- Moins de tests n√©cessaires

**Inconv√©nients :**
- N√©cessite une bonne compr√©hension
- Peut manquer des probl√®mes subtils

**Quand l'utiliser :**
- Probl√®mes complexes avec beaucoup de composants
- Contraintes de temps
- Expert avec intuition d√©velopp√©e

### Exemple de test m√©thodique

**Sc√©nario :** Client (192.168.1.100) ne peut pas acc√©der au serveur web (203.0.113.50:443)

**Approche Bottom-Up :**

```bash
# Couche 1 : Physique
ethtool eth0 | grep "Link detected"
‚Üí Link detected: yes ‚úÖ

# Couche 2 : Liaison (v√©rifier passerelle dans ARP)
ping 192.168.1.1
arp -a | grep 192.168.1.1
‚Üí R√©ponse, MAC pr√©sente ‚úÖ

# Couche 3 : R√©seau (routage vers destination)
ping 203.0.113.50
‚Üí Request timeout ‚ùå

# Affiner : traceroute pour voir o√π √ßa bloque
traceroute 203.0.113.50
‚Üí S'arr√™te √† 192.168.1.1 (passerelle)

# Hypoth√®se affin√©e : probl√®me de routage apr√®s la passerelle
# ou firewall sur la passerelle

# Couche 4 : Transport (si ping marchait)
telnet 203.0.113.50 443
nc -zv 203.0.113.50 443
‚Üí Connection refused (le port r√©pond mais refuse) ‚ùå

# Couche 7 : Application (si TCP marchait)
curl -v https://203.0.113.50
‚Üí D√©tails de la n√©gociation TLS et r√©ponse HTTP
```

### Isolation par substitution

Une technique puissante : remplacer un composant suspect :

**Exemple :**
```
Probl√®me : Client A ne peut pas joindre Serveur B

Test 1 : Client C peut-il joindre Serveur B ?
  ‚Üí Oui ‚Üí Le probl√®me est sur Client A
  ‚Üí Non ‚Üí Le probl√®me pourrait √™tre Serveur B ou le r√©seau

Test 2 : Client A peut-il joindre Serveur D ?
  ‚Üí Oui ‚Üí Le probl√®me est sp√©cifique √† Serveur B
  ‚Üí Non ‚Üí Le probl√®me est sur Client A ou sa connexion r√©seau

Par √©limination : on isole le composant d√©faillant
```

### Utiliser les logs

Les logs sont souvent plus r√©v√©lateurs que les tests actifs :

```bash
# Logs syst√®me r√©cents
journalctl -xe --since "1 hour ago"
tail -f /var/log/syslog

# Logs applicatifs
tail -f /var/log/nginx/error.log
tail -f /var/log/apache2/error.log

# Logs firewall
journalctl -u firewalld -n 100
tail -f /var/log/ufw.log
```

**Rechercher sp√©cifiquement :**
- Mots-cl√©s : `error`, `fail`, `timeout`, `refused`, `denied`
- Adresses IP concern√©es
- Ports concern√©s
- Timestamps correspondant au probl√®me

### Documenter chaque test

Gardez une trace de ce que vous testez :

```
Tests effectu√©s :

[10:15] ping 203.0.113.50 ‚Üí timeout
[10:16] ping 192.168.1.1 ‚Üí OK (5ms)
[10:17] traceroute 203.0.113.50 ‚Üí bloqu√© apr√®s 192.168.1.1
[10:20] telnet 192.168.1.1 22 ‚Üí connexion OK
[10:22] v√©rification firewall ‚Üí r√®gle DROP pour 203.0.113.0/24
[10:25] ajout r√®gle ACCEPT ‚Üí ping OK, probl√®me r√©solu ‚úÖ
```

---

## √âtape 6 : R√©soudre et documenter

### Impl√©menter la solution

Une fois la cause identifi√©e :

#### **1. Planifier le changement**
```
- Quel changement exact ?
- Impact sur quoi/qui ?
- Fen√™tre de maintenance n√©cessaire ?
- Plan de rollback si √ßa √©choue ?
```

#### **2. Tester sur un environnement non-prod** (si possible)
```
- Lab / environnement de test
- Valider que la solution fonctionne
- Identifier les effets de bord
```

#### **3. Appliquer en production**
```
- Suivre la proc√©dure de change management
- Appliquer le changement
- V√©rifier imm√©diatement que √ßa fonctionne
```

#### **4. Valider la r√©solution**
```
- Le sympt√¥me initial a-t-il disparu ?
- Y a-t-il des effets de bord ?
- Les utilisateurs confirment-ils ?
- Les m√©triques sont-elles revenues √† la normale ?
```

### Documenter la r√©solution

La documentation est **critique** pour :
- √âviter que le probl√®me ne se reproduise
- Partager la connaissance avec l'√©quipe
- Cr√©er une base de connaissances

**Template de documentation :**

```markdown
# Incident #12345 - Impossibilit√© d'acc√®s √† l'API de production

## R√©sum√©
Date : 2025-12-07
Dur√©e : 45 minutes (10:15 - 11:00)
Gravit√© : Haute
Impact : 200 utilisateurs, service API indisponible

## Sympt√¥mes
- Application web retournait HTTP 504 Gateway Timeout
- Logs montrant "Connection refused" vers 10.0.5.20:8080
- Monitoring alertant sur 100% d'√©chec des health checks

## Cause racine
R√®gle de firewall trop restrictive ajout√©e lors du d√©ploiement
du 06/12 bloquant le trafic vers le port 8080 depuis le subnet
de l'application (10.0.3.0/24).

## Investigation
1. [10:15] V√©rification connectivit√© : ping OK, telnet port 8080 KO
2. [10:20] Traceroute : trafic bloqu√© au niveau du firewall (10.0.1.1)
3. [10:25] V√©rification r√®gles firewall : r√®gle DROP identifi√©e
4. [10:35] Logs firewall confirmant les DROP packets

## R√©solution
Ajout de la r√®gle firewall autorisant 10.0.3.0/24 ‚Üí 10.0.5.20:8080
Commande : iptables -I INPUT -s 10.0.3.0/24 -p tcp --dport 8080 -j ACCEPT

## Pr√©vention
- Ajouter cette r√®gle au playbook Ansible firewall
- Am√©liorer les tests pr√©-d√©ploiement (inclure connectivity tests)
- Mettre √† jour la documentation d'architecture r√©seau

## Le√ßons apprises
- Les changements firewall doivent inclure une checklist de validation
- Besoin d'un monitoring plus fin au niveau firewall (logs DROP)
```

### Analyse post-mortem

Pour les incidents majeurs, organisez une r√©union post-mortem :

**Objectifs :**
- Comprendre **pourquoi** c'est arriv√© (pas **qui** est responsable)
- Identifier les am√©liorations de processus
- Partager les apprentissages

**Questions √† se poser :**
```
1. Qu'est-ce qui s'est bien pass√© ?
2. Qu'est-ce qui s'est mal pass√© ?
3. Qu'avons-nous eu de la chance de... ?
4. Comment √©viter que cela se reproduise ?
5. Quelles am√©liorations apporter aux outils/processus ?
```

---

## Strat√©gies avanc√©es

### La r√®gle du 80/20 (Pareto)

En diagnostic r√©seau, 80% des probl√®mes viennent de 20% des causes :

**Les classiques qui causent 80% des probl√®mes :**
1. **Firewall/ACL mal configur√©** (bloque le trafic l√©gitime)
2. **DNS qui ne r√©sout pas** (mauvaise config, serveur down)
3. **Mauvaise route** (table de routage incorrecte)
4. **Service pas d√©marr√©** (crash, pas d'auto-restart)
5. **Probl√®me physique** (c√¢ble d√©branch√©, carte r√©seau HS)
6. **Saturation** (bande passante, connexions max atteintes)

**Checkez toujours ces 6 points en premier** avant de chercher des causes exotiques.

### Le principe des 5 Pourquoi

Creusez jusqu'√† la cause racine :

```
Probl√®me : Le site web est down

Pourquoi ? ‚Üí Le serveur web ne r√©pond pas
Pourquoi ? ‚Üí Le processus nginx est arr√™t√©
Pourquoi ? ‚Üí Il a crash√© √† cause d'un out-of-memory
Pourquoi ? ‚Üí Une connexion a leaked et consomm√© toute la RAM
Pourquoi ? ‚Üí Le keep-alive timeout est trop long et accumule les connexions

Cause racine : Configuration keep-alive inadapt√©e au trafic
Solution : Ajuster timeout + monitoring proactif de la RAM
```

### √âlimination par bisection

Pour les probl√®mes complexes avec beaucoup d'√©tapes :

```
Processus en 10 √©tapes : A ‚Üí B ‚Üí C ‚Üí D ‚Üí E ‚Üí F ‚Üí G ‚Üí H ‚Üí I ‚Üí J

Au lieu de tester A, puis B, puis C... (10 tests max)

Testez au milieu (E) :
  ‚îú‚îÄ OK ‚Üí Probl√®me entre F et J (testez H)
  ‚îî‚îÄ KO ‚Üí Probl√®me entre A et E (testez C)

Avec cette m√©thode : ~log‚ÇÇ(10) = 4 tests maximum
```

### Le changement comme indice

Un probl√®me qui appara√Æt soudainement est **presque toujours** li√© √† un changement r√©cent :

**Cherchez les changements des derni√®res 24-48h :**
- D√©ploiement applicatif
- Mise √† jour syst√®me (apt upgrade, yum update)
- Modification de configuration (nginx, firewall, DNS)
- Ajout/retrait d'√©quipement r√©seau
- Changement de fournisseur/ISP
- Migration vers nouveau datacenter

**Question cl√© :** "Qu'est-ce qui a chang√© ?"

### Reproduire le probl√®me

Si possible, reproduisez le probl√®me de mani√®re contr√¥l√©e :

**Avantages :**
- Confirme que vous comprenez la cause
- Permet de tester la solution
- Facilite le debugging (logs, captures)

**M√©thode :**
```
1. Isoler un environnement de test
2. Recr√©er les conditions (m√™me config, m√™me trafic)
3. Observer le probl√®me se produire
4. Appliquer le fix
5. V√©rifier que le probl√®me dispara√Æt
```

---

## Erreurs courantes √† √©viter

### ‚ùå Erreur 1 : Changer plusieurs choses √† la fois

```
Mauvais :
"Je vais red√©marrer le serveur, changer la config firewall,
et mettre √† jour nginx, comme √ßa on verra bien"

Bon :
"Je vais d'abord v√©rifier le firewall. Si ce n'est pas √ßa,
je testerai ensuite le serveur."
```

**Pourquoi ?** Si vous changez 3 choses et que √ßa marche, vous ne saurez pas laquelle a r√©solu le probl√®me.

### ‚ùå Erreur 2 : Ignorer les logs

Les logs contiennent souvent LA r√©ponse, mais beaucoup ne les consultent qu'en dernier recours.

```
‚úÖ Workflow correct :
1. D√©finir le probl√®me
2. Consulter les logs
3. Tester hypoth√®ses bas√©es sur les logs
```

### ‚ùå Erreur 3 : Partir du principe que c'est complexe

```
Syndrome de l'expert : "Ce doit √™tre un probl√®me de BGP
dans le backbone de notre ISP"

R√©alit√© : Le c√¢ble Ethernet √©tait mal branch√©.
```

### ‚ùå Erreur 4 : Ne pas documenter

```
Sc√©nario :
"J'ai r√©solu le probl√®me en changeant un truc...
mais je ne me souviens plus quoi exactement"

3 mois plus tard : Le m√™me probl√®me se reproduit,
et personne ne sait comment le r√©soudre.
```

### ‚ùå Erreur 5 : N√©gliger la communication

```
Mauvais :
[Silence pendant 2h d'investigation]
R√©sultat : Utilisateurs frustr√©s, management stress√©

Bon :
[Update toutes les 30 min]
"Investigation en cours, 3 hypoth√®ses test√©es,
estimons r√©solution dans 45 minutes"
```

---

## Checklist diagnostic rapide

Voici une checklist condens√©e pour un diagnostic rapide :

### ‚úÖ Phase 1 : D√©finition (5 min)
```
‚ñ° Probl√®me d√©fini en 1 phrase claire
‚ñ° Qui/Quoi/Quand/O√π identifi√©
‚ñ° Sympt√¥me vs comportement attendu not√©
‚ñ° Timeline des √©v√©nements √©tablie
```

### ‚úÖ Phase 2 : Collecte (10 min)
```
‚ñ° Config r√©seau r√©cup√©r√©e (IP, DNS, routes)
‚ñ° Logs r√©cents consult√©s
‚ñ° Changements r√©cents identifi√©s
‚ñ° Architecture comprise (sch√©ma)
```

### ‚úÖ Phase 3 : Tests de base (10 min)
```
‚ñ° Ping local (127.0.0.1) ‚Üí Pile TCP/IP fonctionne
‚ñ° Ping passerelle ‚Üí R√©seau local OK
‚ñ° Ping destination ‚Üí Routage OK
‚ñ° DNS lookup ‚Üí R√©solution de noms OK
‚ñ° Port check (telnet/nc) ‚Üí Service accessible
```

### ‚úÖ Phase 4 : Hypoth√®ses (5 min)
```
‚ñ° 3-5 hypoth√®ses list√©es
‚ñ° Class√©es par probabilit√©
‚ñ° Tests d√©finis pour chaque hypoth√®se
```

### ‚úÖ Phase 5 : Tests (temps variable)
```
‚ñ° Tests ex√©cut√©s un par un
‚ñ° R√©sultats document√©s
‚ñ° Hypoth√®ses √©limin√©es ou confirm√©es
```

### ‚úÖ Phase 6 : R√©solution (temps variable)
```
‚ñ° Solution identifi√©e et test√©e
‚ñ° Changement appliqu√©
‚ñ° Validation effectu√©e
‚ñ° Documentation r√©dig√©e
‚ñ° Post-mortem si n√©cessaire
```

---

## Cas pratique complet

### Sc√©nario r√©el

**Contexte :**
Vous √™tes d√©veloppeur backend. Votre application Node.js ne peut plus se connecter √† la base de donn√©es PostgreSQL depuis 10 minutes. C'est la panique : le site de production est down.

**Sympt√¥me initial :**
```
Application logs:
Error: connect ETIMEDOUT
    at Connection._handleConnectTimeout
Database: db.prod.example.com:5432
```

### Application de la m√©thodologie

#### **√âtape 1 : D√©finir le probl√®me**

```
Probl√®me :
L'application Node.js (10.0.3.15) ne peut pas √©tablir de connexion
TCP vers la base de donn√©es PostgreSQL (db.prod.example.com / 10.0.8.50:5432)
depuis 10 minutes, alors que √ßa fonctionnait parfaitement depuis 3 mois.

Contexte :
- Seule cette application est affect√©e
- Erreur : ETIMEDOUT (timeout de connexion)
- Pas de changement applicatif r√©cent
- D√©ploiement infrastructure hier soir (mise √† jour firewall)
```

#### **√âtape 2 : Collecter les informations**

```bash
# Sur le serveur d'application (10.0.3.15)
ip addr show
# ‚Üí eth0: 10.0.3.15/24

ip route
# ‚Üí default via 10.0.3.1

cat /etc/resolv.conf
# ‚Üí nameserver 10.0.1.10

# Logs app
tail -100 /var/log/app/error.log
# ‚Üí R√©p√©tition de "connect ETIMEDOUT" toutes les 30s
```

**Timeline :**
```
Hier 22:00 - Mise √† jour firewall planifi√©e
Aujourd'hui 14:35 - Premi√®re alerte monitoring DB timeout
Aujourd'hui 14:40 - Site en erreur 500
Aujourd'hui 14:45 - Investigation d√©marr√©e
```

#### **√âtape 3 : Analyser les sympt√¥mes**

```
Erreur ETIMEDOUT signifie :
- Les paquets SYN envoy√©s mais pas d'ACK re√ßu
- Couche 3 (r√©seau) ou 4 (transport) probablement

Indice fort : Mise √† jour firewall hier
```

#### **√âtape 4 : Formuler des hypoth√®ses**

```
H1. Firewall bloque le trafic vers port 5432 (PROBABILIT√â HAUTE)
    ‚Üí Test : telnet, v√©rifier r√®gles firewall

H2. Serveur PostgreSQL down (PROBABILIT√â MOYENNE)
    ‚Üí Test : ping serveur, v√©rifier si service tourne

H3. Route r√©seau manquante apr√®s changement (PROBABILIT√â MOYENNE)
    ‚Üí Test : traceroute, v√©rifier table de routage

H4. DNS ne r√©sout plus db.prod.example.com (PROBABILIT√â BASSE)
    ‚Üí Test : nslookup, dig

H5. PostgreSQL accepte connexions mais pas depuis cette IP (PROBABILIT√â BASSE)
    ‚Üí Test : v√©rifier pg_hba.conf
```

#### **√âtape 5 : Tester les hypoth√®ses**

```bash
# Test H4 d'abord (le plus rapide)
nslookup db.prod.example.com
# ‚Üí 10.0.8.50 ‚úÖ DNS fonctionne

# Test H2 : Serveur up ?
ping 10.0.8.50
# ‚Üí Reply from 10.0.8.50: time=2ms ‚úÖ Serveur joignable

# Test H3 : Routage OK ?
traceroute 10.0.8.50
# ‚Üí Atteint 10.0.8.50 en 3 hops ‚úÖ Routage OK

# Test H1 : Port 5432 ouvert ?
telnet 10.0.8.50 5432
# ‚Üí Trying 10.0.8.50...
# ‚Üí [timeout apr√®s 30s] ‚ùå PORT BLOQU√â !

# Confirmation : essayer depuis un autre serveur
ssh admin@10.0.2.20
telnet 10.0.8.50 5432
# ‚Üí Connected to 10.0.8.50 ‚úÖ Fonctionne depuis ailleurs

# CONCLUSION : Le firewall bloque 10.0.3.15 ‚Üí 10.0.8.50:5432
```

**Investigation firewall :**

```bash
# Sur le firewall
sudo iptables -L -n -v | grep 5432
# ‚Üí REJECT  tcp  --  *   *   10.0.3.0/24  10.0.8.50  tcp dpt:5432

# BINGO ! R√®gle de rejet ajout√©e hier
```

#### **√âtape 6 : R√©soudre et documenter**

**Solution :**
```bash
# Supprimer la r√®gle erron√©e
sudo iptables -D INPUT -s 10.0.3.0/24 -d 10.0.8.50 -p tcp --dport 5432 -j REJECT

# Ajouter la r√®gle correcte (ACCEPT)
sudo iptables -I INPUT -s 10.0.3.0/24 -d 10.0.8.50 -p tcp --dport 5432 -j ACCEPT

# Sauvegarder
sudo iptables-save > /etc/iptables/rules.v4
```

**Validation :**
```bash
# Test connexion
telnet 10.0.8.50 5432
# ‚Üí Connected ‚úÖ

# Red√©marrer app
systemctl restart node-app

# V√©rifier logs
tail -f /var/log/app/error.log
# ‚Üí Plus d'erreurs ‚úÖ

# V√©rifier site
curl -I https://prod.example.com
# ‚Üí HTTP/2 200 ‚úÖ
```

**Documentation :**
```markdown
# Incident : Application down - DB inaccessible

**Date :** 2025-12-07 14:35-15:05 (30 min)
**Impact :** Site production indisponible

## Cause
R√®gle firewall erron√©e ajout√©e lors de la mise √† jour du 06/12
bloquant le trafic du subnet application (10.0.3.0/24) vers
PostgreSQL (10.0.8.50:5432).

## R√©solution
Suppression de la r√®gle REJECT et ajout de la r√®gle ACCEPT appropri√©e.

## Pr√©vention
- Automatiser les tests de connectivit√© post-d√©ploiement
- Ajouter r√®gles firewall au contr√¥le de version (Infrastructure as Code)
- Peer review obligatoire pour les changements firewall
- Am√©liorer monitoring : alerter sur connexions DB en √©chec
```

**Temps total :** 30 minutes, probl√®me r√©solu m√©thodiquement.

---

## Points cl√©s √† retenir

### üéØ Les 5 principes d'or

1. **D√©finissez avant de chercher** : Un probl√®me bien d√©fini est √† moiti√© r√©solu
2. **Proc√©dez par couches** : Le mod√®le OSI est votre guide
3. **Testez une chose √† la fois** : Changements isol√©s seulement
4. **Documentez tout** : Vos tests, vos r√©sultats, vos solutions
5. **Cherchez les changements** : 90% des probl√®mes suivent un changement

### ‚úÖ Ce qu'il faut faire

- Rester calme et m√©thodique
- Partir du simple vers le complexe
- Utiliser les logs comme premi√®re source d'information
- Valider chaque hypoth√®se par un test
- Communiquer r√©guli√®rement sur l'avancement

### ‚ùå Ce qu'il faut √©viter

- Paniquer et tester au hasard
- Partir de l'hypoth√®se la plus complexe
- Changer plusieurs choses simultan√©ment
- N√©gliger la documentation
- Travailler en silence sans updates

---

## Conclusion

La m√©thodologie de diagnostic r√©seau n'est pas une perte de temps, c'est un **investissement** qui permet de :
- R√©soudre les probl√®mes **plus rapidement**
- Trouver la **vraie cause** plut√¥t qu'un contournement temporaire
- **Apprendre** de chaque incident pour progresser
- **Communiquer** efficacement avec les √©quipes
- **Pr√©venir** les probl√®mes futurs

Avec de la pratique, cette approche deviendra naturelle. Vous d√©velopperez une intuition qui vous permettra d'identifier rapidement o√π chercher. Mais m√™me les experts les plus aguerris suivent cette m√©thodologie pour les probl√®mes complexes.

Dans les sections suivantes, nous allons approfondir chaque outil et technique mentionn√©s ici, en commen√ßant par les outils en ligne de commande indispensables.

---

**Prochaine section :** 7.2 Outils en ligne de commande

Nous explorerons en d√©tail `ping`, `traceroute`, `netstat`, `dig`, et bien d'autres outils essentiels avec de nombreux exemples pratiques.

‚è≠Ô∏è [Outils en ligne de commande : ping, traceroute, netstat, ss, nslookup, dig](/07-analyse-depannage/02-outils-ligne-commande.md)
