üîù Retour au [Sommaire](/SOMMAIRE.md)

# 9.3 Load Balancing : principes et niveaux (L4, L7)

## Introduction

Le **load balancing** (r√©partition de charge) est l'art de **distribuer intelligemment le trafic r√©seau** entre plusieurs serveurs pour optimiser les performances, maximiser la disponibilit√© et √©viter la surcharge d'un serveur unique. C'est l'un des piliers fondamentaux des architectures web modernes.

### L'analogie du supermarch√©

Imaginez un supermarch√© le samedi apr√®s-midi :

**Sans load balancing :**
```
Caisse 1 : 30 personnes qui attendent ‚è∞‚è∞‚è∞
Caisse 2 : Ferm√©e
Caisse 3 : Ferm√©e
Caisse 4 : Ferm√©e

R√©sultat : Clients m√©contents, abandon de panier, mauvaise exp√©rience
```

**Avec load balancing (L4 - round-robin simple) :**
```
Caisse 1 : 8 personnes
Caisse 2 : 7 personnes
Caisse 3 : 8 personnes
Caisse 4 : 7 personnes

R√©sultat : Temps d'attente divis√© par 4, clients satisfaits
```

**Avec load balancing intelligent (L7 - aware context) :**
```
Caisse express (< 10 articles) : 5 personnes ‚Üí rapide ‚ö°
Caisse standard : 8 personnes ‚Üí normal
Caisse prioritaire (personnes √¢g√©es) : 3 personnes ‚Üí service adapt√©
Caisse automate : 4 personnes ‚Üí autonome

R√©sultat : Optimisation maximale selon le profil
```

### Le probl√®me que r√©sout le load balancing

**Sc√©nario typique :** Votre startup d√©colle. Vous passez de 100 √† 10 000 utilisateurs simultan√©s.

```
Avant (serveur unique)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  10 000 requ√™tes/sec                ‚îÇ
‚îÇ         ‚Üì                           ‚îÇ
‚îÇ    Serveur unique                   ‚îÇ
‚îÇ    CPU: 100% üî•                     ‚îÇ
‚îÇ    RAM: Satur√©e üí•                  ‚îÇ
‚îÇ    Latence: 5000ms üò±               ‚îÇ
‚îÇ         ‚Üì                           ‚îÇ
‚îÇ    Site down ‚ò†Ô∏è                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Apr√®s (avec load balancer)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  10 000 requ√™tes/sec                ‚îÇ
‚îÇ         ‚Üì                           ‚îÇ
‚îÇ    Load Balancer                    ‚îÇ
‚îÇ    (distribue intelligemment)       ‚îÇ
‚îÇ         ‚Üì                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ S1   ‚îÇ S2   ‚îÇ S3   ‚îÇ S4   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ2500  ‚îÇ2500  ‚îÇ2500  ‚îÇ2500  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇreq/s ‚îÇreq/s ‚îÇreq/s ‚îÇreq/s ‚îÇ      ‚îÇ
‚îÇ  ‚îÇCPU:  ‚îÇCPU:  ‚îÇCPU:  ‚îÇCPU:  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ25%‚úÖ ‚îÇ25%‚úÖ ‚îÇ25%‚úÖ ‚îÇ25%‚úÖ ‚îÇ      ‚îÇ
‚îÇ  ‚îÇLat:  ‚îÇLat:  ‚îÇLat:  ‚îÇLat:  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ50ms  ‚îÇ50ms  ‚îÇ50ms  ‚îÇ50ms  ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  Site rapide et stable üöÄ           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Types de load balancing : L4 vs L7

La diff√©rence fondamentale r√©side dans la **couche OSI** √† laquelle le load balancer op√®re.

### Rappel du mod√®le OSI/TCP-IP

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Couche 7 - Application  ‚îÇ HTTP, HTTPS, DNS      ‚îÇ ‚Üê L7 Load Balancer
‚îÇ  Couche 6 - Pr√©sentation ‚îÇ SSL/TLS, Encoding     ‚îÇ
‚îÇ  Couche 5 - Session      ‚îÇ Sessions              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Couche 4 - Transport    ‚îÇ TCP, UDP, Ports       ‚îÇ ‚Üê L4 Load Balancer
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Couche 3 - R√©seau       ‚îÇ IP, ICMP, Routing     ‚îÇ
‚îÇ  Couche 2 - Liaison      ‚îÇ Ethernet, MAC, Switch ‚îÇ
‚îÇ  Couche 1 - Physique     ‚îÇ C√¢bles, signaux       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Load Balancing L4 (Transport Layer)

Le load balancer L4 op√®re au niveau **TCP/UDP**. Il prend des d√©cisions bas√©es uniquement sur :
- **Adresse IP source/destination**
- **Port source/destination**
- **Protocole** (TCP ou UDP)

Il **ne regarde PAS** le contenu des paquets (headers HTTP, cookies, etc.).

```
D√©cision L4 bas√©e sur :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  IP Source : 192.168.1.100      ‚îÇ
‚îÇ  IP Dest   : 10.0.0.10          ‚îÇ
‚îÇ  Port Src  : 54321              ‚îÇ
‚îÇ  Port Dest : 443                ‚îÇ
‚îÇ  Protocole : TCP                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
   [D√©cision de routage]
         ‚Üì
   Serveur backend choisi
```

**Caract√©ristiques :**
- ‚ö° **Tr√®s rapide** (pas de d√©chiffrement SSL, pas d'analyse HTTP)
- üìä **Scalable** (peut g√©rer des millions de connexions)
- üîí **Transparent** (le client ne voit qu'une seule IP)
- ‚ö†Ô∏è **Limit√©** (pas de routing intelligent bas√© sur le contenu)

**Cas d'usage :**
- Load balancing de bases de donn√©es
- Proxying de connexions TCP brutes
- Situations o√π la performance brute est critique
- Trafic non-HTTP (SMTP, PostgreSQL, Redis, etc.)

### Load Balancing L7 (Application Layer)

Le load balancer L7 op√®re au niveau **applicatif** (HTTP/HTTPS). Il peut inspecter :
- **Headers HTTP** (Host, User-Agent, Cookie, etc.)
- **URL et query parameters**
- **M√©thode HTTP** (GET, POST, etc.)
- **Contenu du payload** (JSON, XML)
- **Cookies et sessions**

```
D√©cision L7 bas√©e sur :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GET /api/users/123 HTTP/1.1             ‚îÇ
‚îÇ  Host: api.example.com                   ‚îÇ
‚îÇ  User-Agent: Mobile App/2.0              ‚îÇ
‚îÇ  Cookie: session=abc123                  ‚îÇ
‚îÇ  X-API-Version: v2                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
   [D√©cision intelligente]
   ‚Ä¢ API v2 ‚Üí Backend pool 2
   ‚Ä¢ Mobile ‚Üí Cache-friendly servers
   ‚Ä¢ Session abc123 ‚Üí Server 3 (sticky)
         ‚Üì
   Serveur backend optimal
```

**Caract√©ristiques :**
- üß† **Intelligent** (routing bas√© sur le contenu)
- üîê **SSL termination** (d√©chiffre HTTPS une fois)
- üéØ **Granulaire** (routing par URL, headers, etc.)
- ‚öôÔ∏è **Manipulation** (modifier headers, rewrite URLs)
- üêå **Plus lent** que L4 (analyse approfondie)

**Cas d'usage :**
- Applications web (HTTP/HTTPS)
- Microservices avec routing complexe
- A/B testing
- Canary deployments
- API gateways

### Comparaison L4 vs L7

| Crit√®re | L4 (Transport) | L7 (Application) |
|---------|----------------|------------------|
| **Couche OSI** | Transport (TCP/UDP) | Application (HTTP/HTTPS) |
| **Visibilit√©** | IP + Port | Contenu complet (headers, body) |
| **Performance** | Excellent (ns) | Bon (¬µs-ms) |
| **Throughput** | Tr√®s √©lev√© (10M+ conn/s) | √âlev√© (100k-1M req/s) |
| **Complexit√©** | Faible | √âlev√©e |
| **Routage** | Round-robin, least conn | URL, headers, cookies, g√©o |
| **SSL** | Passthrough | Termination possible |
| **Session persistence** | IP-based | Cookie-based |
| **Health checks** | TCP handshake | HTTP status codes |
| **Use cases** | DB, cache, TCP apps | Web apps, APIs, microservices |
| **Exemples** | AWS NLB, HAProxy (mode TCP) | AWS ALB, Nginx, HAProxy (mode HTTP) |

### Illustration : Flux de traitement

**Load Balancer L4 :**
```
Client                  LB L4                Backend
  ‚îÇ                       ‚îÇ                     ‚îÇ
  ‚îÇ‚îÄ‚îÄ‚îÄ[SYN]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                     ‚îÇ
  ‚îÇ                       ‚îÇ‚îÄ‚îÄ[SYN]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
  ‚îÇ                       ‚îÇ<‚îÄ[SYN-ACK]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
  ‚îÇ<‚îÄ‚îÄ[SYN-ACK]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                     ‚îÇ
  ‚îÇ‚îÄ‚îÄ‚îÄ[ACK]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ‚îÄ‚îÄ‚îÄ[ACK]‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
  ‚îÇ                       ‚îÇ                     ‚îÇ
  ‚îÇ‚îÄ‚îÄ‚îÄ[Data (encrypted)]‚îÄ>‚îÇ‚îÄ‚îÄ[Data (encrypted)]>‚îÇ
  ‚îÇ                       ‚îÇ  (passthrough)      ‚îÇ
  ‚îÇ                       ‚îÇ                     ‚îÇ
         LB ne regarde PAS le contenu
```

**Load Balancer L7 :**
```
Client                  LB L7                Backend
  ‚îÇ                       ‚îÇ                     ‚îÇ
  ‚îÇ‚îÄ‚îÄ‚îÄ[HTTPS Request]‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                     ‚îÇ
  ‚îÇ                       ‚îÇ [D√©chiffre SSL]     ‚îÇ
  ‚îÇ                       ‚îÇ [Analyse HTTP]      ‚îÇ
  ‚îÇ                       ‚îÇ [D√©cision routing]  ‚îÇ
  ‚îÇ                       ‚îÇ‚îÄ‚îÄ[HTTP Request]‚îÄ‚îÄ‚îÄ> ‚îÇ
  ‚îÇ                       ‚îÇ   (peut modifier)   ‚îÇ
  ‚îÇ                       ‚îÇ<‚îÄ[HTTP Response]‚îÄ‚îÄ‚îÄ ‚îÇ
  ‚îÇ                       ‚îÇ [Analyse response]  ‚îÇ
  ‚îÇ                       ‚îÇ [Re-chiffre SSL]    ‚îÇ
  ‚îÇ<‚îÄ‚îÄ[HTTPS Response]‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                     ‚îÇ
  ‚îÇ                       ‚îÇ                     ‚îÇ
      LB inspecte et peut modifier le contenu
```

## Algorithmes de distribution

Le choix de l'algorithme d√©termine **comment** les requ√™tes sont distribu√©es entre les serveurs.

### 1. Round Robin (RR)

**Principe :** Distribution circulaire, chaque serveur re√ßoit √† tour de r√¥le.

```
Requ√™te 1 ‚Üí Serveur 1
Requ√™te 2 ‚Üí Serveur 2
Requ√™te 3 ‚Üí Serveur 3
Requ√™te 4 ‚Üí Serveur 1
Requ√™te 5 ‚Üí Serveur 2
...
```

**Avantages :**
- Simple √† impl√©menter
- Distribution √©quitable si tous les serveurs sont identiques

**Inconv√©nients :**
- Ne tient pas compte de la charge r√©elle des serveurs
- Inefficace si les serveurs ont des capacit√©s diff√©rentes

**Cas d'usage :** Serveurs homog√®nes, charges uniformes.

**Impl√©mentation simple en Python :**

```python
class RoundRobinLoadBalancer:
    def __init__(self, servers):
        self.servers = servers
        self.current = 0

    def get_next_server(self):
        server = self.servers[self.current]
        self.current = (self.current + 1) % len(self.servers)
        return server

# Usage
lb = RoundRobinLoadBalancer(['server1:8080', 'server2:8080', 'server3:8080'])

for i in range(10):
    print(f"Request {i+1} ‚Üí {lb.get_next_server()}")

# Output:
# Request 1 ‚Üí server1:8080
# Request 2 ‚Üí server2:8080
# Request 3 ‚Üí server3:8080
# Request 4 ‚Üí server1:8080
# Request 5 ‚Üí server2:8080
# ...
```

### 2. Weighted Round Robin (WRR)

**Principe :** Round robin pond√©r√© selon la capacit√© des serveurs.

```
Serveur 1 (poids: 3) ‚Üí re√ßoit 3 requ√™tes
Serveur 2 (poids: 2) ‚Üí re√ßoit 2 requ√™tes
Serveur 3 (poids: 1) ‚Üí re√ßoit 1 requ√™te

S√©quence : S1, S1, S1, S2, S2, S3, S1, S1, S1, ...
```

**Cas d'usage :** Serveurs de capacit√©s diff√©rentes (ex: 2√ó8 cores, 1√ó16 cores).

**Impl√©mentation :**

```python
class WeightedRoundRobinLoadBalancer:
    def __init__(self, servers_with_weights):
        """
        servers_with_weights: list of (server, weight) tuples
        Example: [('server1', 3), ('server2', 2), ('server3', 1)]
        """
        self.servers = []
        for server, weight in servers_with_weights:
            self.servers.extend([server] * weight)
        self.current = 0

    def get_next_server(self):
        server = self.servers[self.current]
        self.current = (self.current + 1) % len(self.servers)
        return server

# Usage
lb = WeightedRoundRobinLoadBalancer([
    ('powerful-server', 5),   # Serveur puissant
    ('medium-server', 3),     # Serveur moyen
    ('weak-server', 1)        # Serveur faible
])

for i in range(15):
    print(f"Request {i+1} ‚Üí {lb.get_next_server()}")

# Output distribue 5:3:1
```

### 3. Least Connections (LC)

**Principe :** Envoyer la requ√™te au serveur ayant le **moins de connexions actives**.

```
√âtat actuel :
Serveur 1 : 10 connexions
Serveur 2 : 5 connexions  ‚Üê Choisi (minimum)
Serveur 3 : 8 connexions

Nouvelle requ√™te ‚Üí Serveur 2
```

**Avantages :**
- Adapt√© aux requ√™tes de dur√©e variable
- Meilleure distribution que RR pour charges h√©t√©rog√®nes

**Cas d'usage :** Applications avec temps de traitement variables (uploads, long-polling, WebSockets).

**Impl√©mentation :**

```python
import threading
from collections import defaultdict

class LeastConnectionsLoadBalancer:
    def __init__(self, servers):
        self.servers = servers
        self.connections = defaultdict(int)
        self.lock = threading.Lock()

    def get_next_server(self):
        with self.lock:
            # Trouver le serveur avec le moins de connexions
            server = min(self.servers, key=lambda s: self.connections[s])
            self.connections[server] += 1
            return server

    def release_connection(self, server):
        with self.lock:
            self.connections[server] = max(0, self.connections[server] - 1)

    def get_stats(self):
        return dict(self.connections)

# Usage
lb = LeastConnectionsLoadBalancer(['server1', 'server2', 'server3'])

# Simuler des requ√™tes
import random
import time

def handle_request(request_id):
    server = lb.get_next_server()
    print(f"Request {request_id} ‚Üí {server} (active: {lb.get_stats()})")

    # Simuler traitement variable
    time.sleep(random.uniform(0.1, 1.0))

    lb.release_connection(server)
    print(f"Request {request_id} completed on {server}")

# Test avec threads
threads = []
for i in range(10):
    t = threading.Thread(target=handle_request, args=(i,))
    t.start()
    threads.append(t)
    time.sleep(0.1)

for t in threads:
    t.join()
```

### 4. Weighted Least Connections (WLC)

Combine least connections + poids.

```python
class WeightedLeastConnectionsLoadBalancer:
    def __init__(self, servers_with_weights):
        self.servers = {server: weight for server, weight in servers_with_weights}
        self.connections = defaultdict(int)
        self.lock = threading.Lock()

    def get_next_server(self):
        with self.lock:
            # Ratio = connexions_actives / poids
            # Plus le ratio est bas, plus le serveur est disponible
            server = min(self.servers.keys(),
                        key=lambda s: self.connections[s] / self.servers[s])
            self.connections[server] += 1
            return server

    def release_connection(self, server):
        with self.lock:
            self.connections[server] = max(0, self.connections[server] - 1)

# Usage
lb = WeightedLeastConnectionsLoadBalancer([
    ('powerful-server', 10),
    ('medium-server', 5),
    ('weak-server', 2)
])
```

### 5. IP Hash (Source Hash)

**Principe :** Hash de l'IP client d√©termine le serveur backend.

```
hash(client_ip) % nb_servers = serveur_destination

Client 192.168.1.10 ‚Üí hash ‚Üí Serveur 2 (toujours)
Client 192.168.1.20 ‚Üí hash ‚Üí Serveur 1 (toujours)
```

**Avantages :**
- **Session persistence** naturelle (m√™me client ‚Üí m√™me serveur)
- Pas besoin de sticky sessions

**Inconv√©nients :**
- Redistribution si un serveur tombe
- Mauvaise distribution si peu de clients

**Impl√©mentation :**

```python
import hashlib

class IPHashLoadBalancer:
    def __init__(self, servers):
        self.servers = servers

    def get_server_for_ip(self, client_ip):
        # Hash MD5 de l'IP
        hash_value = int(hashlib.md5(client_ip.encode()).hexdigest(), 16)
        index = hash_value % len(self.servers)
        return self.servers[index]

# Usage
lb = IPHashLoadBalancer(['server1', 'server2', 'server3'])

clients = ['192.168.1.10', '192.168.1.20', '10.0.0.5', '172.16.0.1']

for client in clients:
    server = lb.get_server_for_ip(client)
    print(f"Client {client} ‚Üí {server}")

# M√™me client obtient toujours le m√™me serveur
for _ in range(3):
    print(f"Client 192.168.1.10 ‚Üí {lb.get_server_for_ip('192.168.1.10')}")
```

### 6. Consistent Hashing

**Principe :** Am√©lioration de IP Hash qui minimise la redistribution quand un serveur est ajout√©/retir√©.

```
Ring hash :
         server1
           ‚Üì
    ‚óã‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óã‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óã
    ‚Üë               ‚Üì
  server3        server2
```

**Impl√©mentation :**

```python
import hashlib
import bisect

class ConsistentHashLoadBalancer:
    def __init__(self, servers, replicas=100):
        """
        replicas: nombre de points virtuels par serveur (pour meilleure distribution)
        """
        self.replicas = replicas
        self.ring = {}
        self.sorted_keys = []

        for server in servers:
            self.add_server(server)

    def _hash(self, key):
        return int(hashlib.md5(key.encode()).hexdigest(), 16)

    def add_server(self, server):
        for i in range(self.replicas):
            # Cr√©er des r√©pliques virtuelles
            key = f"{server}:{i}"
            hash_value = self._hash(key)
            self.ring[hash_value] = server
            bisect.insort(self.sorted_keys, hash_value)

    def remove_server(self, server):
        for i in range(self.replicas):
            key = f"{server}:{i}"
            hash_value = self._hash(key)
            del self.ring[hash_value]
            self.sorted_keys.remove(hash_value)

    def get_server(self, client_key):
        if not self.ring:
            return None

        hash_value = self._hash(client_key)

        # Trouver le premier serveur dans le sens horaire
        index = bisect.bisect(self.sorted_keys, hash_value)

        if index == len(self.sorted_keys):
            index = 0

        return self.ring[self.sorted_keys[index]]

# Usage
lb = ConsistentHashLoadBalancer(['server1', 'server2', 'server3'])

# Tester la distribution
from collections import Counter
distribution = Counter()

for i in range(1000):
    client = f"client-{i}"
    server = lb.get_server(client)
    distribution[server] += 1

print("Distribution avant retrait:")
for server, count in distribution.items():
    print(f"  {server}: {count} requ√™tes ({count/10:.1f}%)")

# Retirer un serveur
print("\nRetrait de server2...")
lb.remove_server('server2')

redistribution = Counter()
for i in range(1000):
    client = f"client-{i}"
    server = lb.get_server(client)
    redistribution[server] += 1

print("\nDistribution apr√®s retrait:")
for server, count in redistribution.items():
    print(f"  {server}: {count} requ√™tes ({count/10:.1f}%)")

# Calculer le pourcentage de redistribution
changed = 0
for i in range(1000):
    client = f"client-{i}"
    if lb.get_server(client) != (distribution.most_common()[0][0] if i % 3 == 1 else 'server1'):
        changed += 1

print(f"\n{changed/10:.1f}% des requ√™tes redistribu√©es (vs 33% avec hash simple)")
```

### 7. Random

Simple mais efficace pour de grands volumes.

```python
import random

class RandomLoadBalancer:
    def __init__(self, servers):
        self.servers = servers

    def get_next_server(self):
        return random.choice(self.servers)
```

### 8. Least Response Time

Combine least connections + latence mesur√©e.

```python
import time
from collections import defaultdict
import threading

class LeastResponseTimeLoadBalancer:
    def __init__(self, servers):
        self.servers = servers
        self.response_times = defaultdict(lambda: [])  # Historique des temps
        self.connections = defaultdict(int)
        self.lock = threading.Lock()

    def get_avg_response_time(self, server):
        times = self.response_times[server]
        if not times:
            return 0
        return sum(times[-10:]) / len(times[-10:])  # Moyenne des 10 derni√®res

    def get_next_server(self):
        with self.lock:
            # Score = connexions_actives * avg_response_time
            server = min(self.servers,
                        key=lambda s: (self.connections[s] + 1) *
                                     (self.get_avg_response_time(s) + 0.001))
            self.connections[server] += 1
            return server

    def record_response(self, server, response_time):
        with self.lock:
            self.response_times[server].append(response_time)
            # Garder seulement les 100 derni√®res mesures
            if len(self.response_times[server]) > 100:
                self.response_times[server].pop(0)
            self.connections[server] = max(0, self.connections[server] - 1)
```

### Tableau r√©capitulatif des algorithmes

| Algorithme | Complexit√© | Cas d'usage | Avantages | Inconv√©nients |
|------------|-----------|-------------|-----------|---------------|
| **Round Robin** | O(1) | Serveurs identiques | Simple, √©quitable | Ignore la charge |
| **Weighted RR** | O(1) | Serveurs h√©t√©rog√®nes | Respecte capacit√©s | Statique |
| **Least Connections** | O(n) | Charges variables | Adaptatif | Overhead tracking |
| **IP Hash** | O(1) | Session persistence | Sticky naturel | Redistribution probl√©matique |
| **Consistent Hash** | O(log n) | Elastic scaling | Minimise redistribution | Complexe |
| **Random** | O(1) | Grand volume | Simple, scalable | In√©quitable √† court terme |
| **Least Response Time** | O(n) | Performance critique | Optimal | Overhead mesures |

## Health Checks

Les health checks permettent de **d√©tecter les serveurs d√©faillants** et de ne plus leur envoyer de trafic.

### Types de health checks

#### 1. TCP Health Check (L4)

V√©rifie simplement si le port est ouvert.

```python
import socket

def tcp_health_check(host, port, timeout=2):
    """
    Health check TCP simple
    Retourne True si le port r√©pond
    """
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        result = sock.connect_ex((host, port))
        sock.close()
        return result == 0
    except Exception as e:
        print(f"Health check failed: {e}")
        return False

# Usage
if tcp_health_check('backend1.example.com', 8080):
    print("‚úÖ Server is healthy")
else:
    print("‚ùå Server is down")
```

#### 2. HTTP Health Check (L7)

Appelle un endpoint sp√©cifique et v√©rifie le status code.

```python
import requests
from datetime import datetime

class HTTPHealthChecker:
    def __init__(self, servers, health_path='/health', interval=5):
        self.servers = servers
        self.health_path = health_path
        self.interval = interval
        self.health_status = {server: True for server in servers}
        self.last_check = {}

    def check_server(self, server):
        """
        V√©rifie la sant√© d'un serveur via HTTP
        """
        url = f"http://{server}{self.health_path}"

        try:
            response = requests.get(url, timeout=2)

            # Crit√®res de sant√©
            is_healthy = (
                response.status_code == 200 and
                response.elapsed.total_seconds() < 1.0  # Latence < 1s
            )

            self.health_status[server] = is_healthy
            self.last_check[server] = datetime.now()

            return is_healthy

        except Exception as e:
            print(f"‚ùå {server} health check failed: {e}")
            self.health_status[server] = False
            self.last_check[server] = datetime.now()
            return False

    def get_healthy_servers(self):
        """Retourne la liste des serveurs en bonne sant√©"""
        return [s for s in self.servers if self.health_status.get(s, False)]

    def run_checks(self):
        """Ex√©cute les health checks sur tous les serveurs"""
        for server in self.servers:
            status = "‚úÖ" if self.check_server(server) else "‚ùå"
            print(f"{status} {server} - Last check: {self.last_check[server]}")

# Usage
checker = HTTPHealthChecker([
    'backend1:8080',
    'backend2:8080',
    'backend3:8080'
])

# Check unique
checker.run_checks()

# Obtenir les serveurs sains
healthy = checker.get_healthy_servers()
print(f"\nHealthy servers: {healthy}")
```

#### 3. Application-Aware Health Check

V√©rifie non seulement que le serveur r√©pond, mais qu'il est **fonctionnellement op√©rationnel**.

```python
import requests
import json

def advanced_health_check(server):
    """
    Health check avanc√© qui v√©rifie :
    - La connectivit√© HTTP
    - La connexion √† la base de donn√©es
    - L'√©tat du cache
    - La m√©moire disponible
    """
    url = f"http://{server}/api/health/detailed"

    try:
        response = requests.get(url, timeout=3)

        if response.status_code != 200:
            return False, "HTTP error"

        health_data = response.json()

        # V√©rifier chaque composant
        checks = {
            'database': health_data.get('database', {}).get('connected', False),
            'cache': health_data.get('cache', {}).get('connected', False),
            'memory': health_data.get('memory', {}).get('available_mb', 0) > 100,
            'cpu': health_data.get('cpu', {}).get('load', 100) < 80
        }

        all_healthy = all(checks.values())

        if not all_healthy:
            failing = [k for k, v in checks.items() if not v]
            return False, f"Failing components: {failing}"

        return True, "All systems operational"

    except Exception as e:
        return False, str(e)

# Endpoint /api/health/detailed c√¥t√© serveur (Flask)
from flask import Flask, jsonify
import psutil

app = Flask(__name__)

@app.route('/api/health/detailed')
def detailed_health():
    """
    Endpoint de health check d√©taill√©
    """
    # V√©rifier la DB
    try:
        # db.session.execute('SELECT 1')
        db_connected = True
    except:
        db_connected = False

    # V√©rifier le cache
    try:
        # redis_client.ping()
        cache_connected = True
    except:
        cache_connected = False

    # M√©triques syst√®me
    memory = psutil.virtual_memory()
    cpu = psutil.cpu_percent(interval=0.1)

    health_status = {
        'status': 'healthy' if (db_connected and cache_connected) else 'degraded',
        'database': {
            'connected': db_connected,
            'latency_ms': 5  # Mesure r√©elle
        },
        'cache': {
            'connected': cache_connected,
            'hit_rate': 0.85
        },
        'memory': {
            'available_mb': memory.available / (1024 * 1024),
            'percent_used': memory.percent
        },
        'cpu': {
            'load': cpu
        },
        'timestamp': datetime.now().isoformat()
    }

    status_code = 200 if health_status['status'] == 'healthy' else 503

    return jsonify(health_status), status_code
```

### Load Balancer avec health checks int√©gr√©s

```python
import threading
import time
from typing import List, Dict

class SmartLoadBalancer:
    """
    Load balancer avec health checks automatiques
    """

    def __init__(self, servers: List[str], health_check_interval=5):
        self.all_servers = servers
        self.healthy_servers = servers.copy()
        self.health_check_interval = health_check_interval
        self.current_index = 0
        self.lock = threading.Lock()

        # D√©marrer le thread de health checks
        self.health_checker_thread = threading.Thread(
            target=self._health_check_loop,
            daemon=True
        )
        self.health_checker_thread.start()

    def _health_check_loop(self):
        """Boucle de health checks en arri√®re-plan"""
        while True:
            self._run_health_checks()
            time.sleep(self.health_check_interval)

    def _run_health_checks(self):
        """Ex√©cute les health checks sur tous les serveurs"""
        healthy = []

        for server in self.all_servers:
            if self._check_server_health(server):
                healthy.append(server)
                print(f"‚úÖ {server} is healthy")
            else:
                print(f"‚ùå {server} is down")

        with self.lock:
            previous = set(self.healthy_servers)
            current = set(healthy)

            # D√©tecter les changements
            newly_down = previous - current
            newly_up = current - previous

            if newly_down:
                print(f"‚ö†Ô∏è  Servers went down: {newly_down}")
            if newly_up:
                print(f"‚úÖ Servers recovered: {newly_up}")

            self.healthy_servers = healthy

    def _check_server_health(self, server):
        """Health check d'un serveur (simplifi√©)"""
        return tcp_health_check(server.split(':')[0], int(server.split(':')[1]))

    def get_next_server(self):
        """Obtenir le prochain serveur sain (round-robin)"""
        with self.lock:
            if not self.healthy_servers:
                raise Exception("No healthy servers available")

            server = self.healthy_servers[self.current_index]
            self.current_index = (self.current_index + 1) % len(self.healthy_servers)
            return server

    def get_stats(self):
        """Obtenir les statistiques du load balancer"""
        with self.lock:
            return {
                'total_servers': len(self.all_servers),
                'healthy_servers': len(self.healthy_servers),
                'unhealthy_servers': len(self.all_servers) - len(self.healthy_servers),
                'healthy_list': self.healthy_servers
            }

# Usage
lb = SmartLoadBalancer([
    '192.168.1.10:8080',
    '192.168.1.11:8080',
    '192.168.1.12:8080'
], health_check_interval=10)

# Le load balancer v√©rifie automatiquement la sant√© en arri√®re-plan
time.sleep(2)

# Obtenir le prochain serveur
for i in range(5):
    try:
        server = lb.get_next_server()
        print(f"Request {i+1} ‚Üí {server}")
    except Exception as e:
        print(f"Error: {e}")
    time.sleep(1)

# Statistiques
print(lb.get_stats())
```

## Session Persistence (Sticky Sessions)

La **session persistence** garantit que les requ√™tes d'un m√™me client vont toujours au m√™me serveur backend.

### Pourquoi c'est n√©cessaire ?

```
Probl√®me sans sticky sessions :
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User fait LOGIN                    ‚îÇ
‚îÇ   ‚Üì                                ‚îÇ
‚îÇ LB ‚Üí Server 1 (cr√©e session)       ‚îÇ
‚îÇ Session stock√©e en RAM sur Server 1‚îÇ
‚îÇ                                    ‚îÇ
‚îÇ User fait GET /profile             ‚îÇ
‚îÇ   ‚Üì                                ‚îÇ
‚îÇ LB ‚Üí Server 2 (round-robin)        ‚îÇ
‚îÇ ‚ùå Server 2 n'a pas la session     ‚îÇ
‚îÇ ‚Üí User redirig√© vers login         ‚îÇ
‚îÇ ‚Üí Mauvaise exp√©rience              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Solutions

#### 1. Cookie-based sticky sessions (L7)

Le load balancer injecte un cookie avec l'ID du serveur.

```python
from flask import Flask, request, make_response
import hashlib

app = Flask(__name__)

class CookieBasedLoadBalancer:
    def __init__(self, servers):
        self.servers = servers
        self.cookie_name = 'LB_SERVER_ID'

    def get_server_for_request(self, request):
        """
        D√©termine le serveur bas√© sur le cookie
        Si pas de cookie, assigne un serveur et cr√©e le cookie
        """
        server_id = request.cookies.get(self.cookie_name)

        if server_id and server_id in self.servers:
            return server_id

        # Nouveau client ‚Üí assigner un serveur (round-robin, random, etc.)
        server = self.choose_new_server(request)
        return server

    def choose_new_server(self, request):
        # Simple round-robin ou autre algorithme
        # En pratique, utiliser least connections
        import random
        return random.choice(self.servers)

    def inject_cookie(self, response, server_id):
        """Injecte le cookie de persistence"""
        response.set_cookie(
            self.cookie_name,
            server_id,
            max_age=3600,  # 1 heure
            httponly=True,  # S√©curit√©
            secure=True,    # HTTPS uniquement
            samesite='Lax'
        )
        return response

# Dans le reverse proxy
@app.route('/<path:path>', methods=['GET', 'POST', 'PUT', 'DELETE'])
def proxy(path):
    lb = CookieBasedLoadBalancer(['server1', 'server2', 'server3'])

    # D√©terminer le serveur
    server = lb.get_server_for_request(request)

    # Proxier la requ√™te (en pratique, utiliser requests)
    # response = requests.request(...)

    response = make_response(f"Proxied to {server}")

    # Injecter le cookie de persistence
    lb.inject_cookie(response, server)

    return response
```

#### 2. IP-based sticky sessions (L4)

Hash de l'IP client (d√©j√† vu dans les algorithmes).

```python
# HAProxy config pour sticky sessions via cookie
"""
backend web_servers
    balance roundrobin
    cookie SERVERID insert indirect nocache
    server web1 192.168.1.10:80 check cookie web1
    server web2 192.168.1.11:80 check cookie web2
    server web3 192.168.1.12:80 check cookie web3
"""

# HAProxy config pour sticky sessions via IP
"""
backend web_servers
    balance source  # IP hash
    server web1 192.168.1.10:80 check
    server web2 192.168.1.11:80 check
    server web3 192.168.1.12:80 check
"""
```

#### 3. Session store externe (meilleure solution)

Stocker les sessions dans Redis/Memcached au lieu de la RAM locale.

```python
from flask import Flask, session
from flask_session import Session
import redis

app = Flask(__name__)

# Configuration de session externe
app.config['SESSION_TYPE'] = 'redis'
app.config['SESSION_REDIS'] = redis.from_url('redis://localhost:6379')
app.config['SESSION_PERMANENT'] = False
app.config['SESSION_USE_SIGNER'] = True
app.config['SESSION_KEY_PREFIX'] = 'session:'

Session(app)

@app.route('/login', methods=['POST'])
def login():
    # La session est stock√©e dans Redis
    # Accessible depuis TOUS les serveurs backend
    session['user_id'] = 123
    session['username'] = 'alice'
    return {'message': 'Logged in'}

@app.route('/profile')
def profile():
    # Fonctionne peu importe le serveur qui traite la requ√™te
    user_id = session.get('user_id')
    if not user_id:
        return {'error': 'Not logged in'}, 401

    return {'user_id': user_id, 'username': session['username']}
```

**Architecture avec session store :**

```
Client
  ‚Üì
Load Balancer (pas de sticky sessions n√©cessaire)
  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Server1 ‚îÇ Server2 ‚îÇ Server3 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì          ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Redis Cluster     ‚îÇ
    ‚îÇ  (session store)    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Avantages :
‚úÖ Pas de sticky sessions n√©cessaire
‚úÖ Scale horizontalement
‚úÖ Survit au red√©marrage des serveurs
‚úÖ Partage de session entre microservices
```

## Configurations r√©elles

### HAProxy (L4 et L7)

**Configuration L4 (TCP mode) :**

```haproxy
# /etc/haproxy/haproxy.cfg

global
    log /dev/log local0
    maxconn 4096
    daemon

defaults
    log     global
    mode    tcp          # Mode L4
    option  tcplog
    timeout connect 5s
    timeout client  50s
    timeout server  50s

# Frontend L4 (PostgreSQL)
frontend postgres_frontend
    bind *:5432
    mode tcp
    default_backend postgres_servers

backend postgres_servers
    mode tcp
    balance leastconn    # Least connections pour DB
    option tcp-check

    server pg1 192.168.1.10:5432 check
    server pg2 192.168.1.11:5432 check backup  # Backup (failover)
    server pg3 192.168.1.12:5432 check backup

# Frontend L4 (Redis)
frontend redis_frontend
    bind *:6379
    mode tcp
    default_backend redis_servers

backend redis_servers
    mode tcp
    balance source       # IP hash pour persistence

    server redis1 192.168.1.20:6379 check
    server redis2 192.168.1.21:6379 check
    server redis3 192.168.1.22:6379 check
```

**Configuration L7 (HTTP mode) :**

```haproxy
# Mode HTTP avec fonctionnalit√©s L7

global
    log /dev/log local0
    maxconn 10000
    daemon

    # SSL
    tune.ssl.default-dh-param 2048

defaults
    log     global
    mode    http         # Mode L7
    option  httplog
    option  dontlognull
    timeout connect 5s
    timeout client  30s
    timeout server  30s

# Stats page
listen stats
    bind *:8404
    stats enable
    stats uri /stats
    stats refresh 5s
    stats admin if TRUE

# Frontend HTTPS
frontend https_frontend
    bind *:443 ssl crt /etc/ssl/certs/example.com.pem

    # Security headers
    http-response set-header Strict-Transport-Security "max-age=31536000"
    http-response set-header X-Frame-Options "SAMEORIGIN"
    http-response set-header X-Content-Type-Options "nosniff"

    # ACLs pour routing intelligent
    acl is_api path_beg /api/
    acl is_static path_beg /static/ /assets/ /images/
    acl is_admin hdr(host) -i admin.example.com
    acl is_mobile hdr(User-Agent) -i -m sub mobile android ios

    # Routing bas√© sur les ACLs
    use_backend api_servers if is_api
    use_backend static_servers if is_static
    use_backend admin_servers if is_admin
    use_backend mobile_servers if is_mobile
    default_backend web_servers

# Backend API (microservices)
backend api_servers
    balance leastconn
    option httpchk GET /health HTTP/1.1\r\nHost:\ api.example.com
    http-check expect status 200

    # Sticky sessions via cookie
    cookie SERVERID insert indirect nocache

    server api1 10.0.1.10:8080 check cookie api1 weight 10
    server api2 10.0.1.11:8080 check cookie api2 weight 10
    server api3 10.0.1.12:8080 check cookie api3 weight 5  # Moins puissant

# Backend static (CDN origin)
backend static_servers
    balance roundrobin
    option httpchk HEAD / HTTP/1.1\r\nHost:\ static.example.com

    # Compression
    compression algo gzip
    compression type text/html text/css text/javascript application/javascript

    server static1 10.0.2.10:80 check
    server static2 10.0.2.11:80 check

# Backend web (application principale)
backend web_servers
    balance source    # IP hash
    option httpchk GET /healthz HTTP/1.1\r\nHost:\ www.example.com
    http-check expect status 200

    # Timeouts sp√©cifiques
    timeout server 60s

    server web1 10.0.3.10:80 check maxconn 500
    server web2 10.0.3.11:80 check maxconn 500
    server web3 10.0.3.12:80 check maxconn 500

# Backend admin (interface admin)
backend admin_servers
    balance leastconn
    option httpchk GET /admin/health

    # Restriction IP (security layer)
    acl allowed_ips src 192.168.1.0/24 10.0.0.0/8
    http-request deny if !allowed_ips

    server admin1 10.0.4.10:8080 check
    server admin2 10.0.4.11:8080 check backup

# Backend mobile
backend mobile_servers
    balance leastconn

    # Optimisations mobile
    timeout server 120s  # Timeout plus long (r√©seau mobile)

    server mobile1 10.0.5.10:8080 check
    server mobile2 10.0.5.11:8080 check
```

### Nginx (L7 uniquement)

```nginx
# /etc/nginx/nginx.conf

user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 4096;
    use epoll;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'backend=$upstream_addr rt=$request_time';

    access_log /var/log/nginx/access.log main;

    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_types text/plain text/css application/json application/javascript text/xml;

    # Upstream definitions (backend pools)

    # API servers (least connections)
    upstream api_backend {
        least_conn;

        server 10.0.1.10:8080 max_fails=3 fail_timeout=30s;
        server 10.0.1.11:8080 max_fails=3 fail_timeout=30s;
        server 10.0.1.12:8080 max_fails=3 fail_timeout=30s;

        # Keepalive connections
        keepalive 32;
    }

    # Web servers (IP hash pour sticky sessions)
    upstream web_backend {
        ip_hash;

        server 10.0.2.10:80 weight=3;
        server 10.0.2.11:80 weight=2;
        server 10.0.2.12:80 weight=1;
    }

    # Static servers (round robin)
    upstream static_backend {
        server 10.0.3.10:80;
        server 10.0.3.11:80;
        server 10.0.3.12:80;
    }

    # Health check (Nginx Plus feature, ou utiliser un module tiers)
    # upstream api_backend {
    #     zone api_backend 64k;
    #     server 10.0.1.10:8080 max_fails=3 fail_timeout=30s;
    #     health_check interval=5s fails=2 passes=2 uri=/health;
    # }

    # Main server block
    server {
        listen 80;
        listen [::]:80;
        server_name example.com www.example.com;

        # Redirect HTTP to HTTPS
        return 301 https://$server_name$request_uri;
    }

    # HTTPS server
    server {
        listen 443 ssl http2;
        listen [::]:443 ssl http2;
        server_name example.com www.example.com;

        # SSL configuration
        ssl_certificate /etc/ssl/certs/example.com.crt;
        ssl_certificate_key /etc/ssl/private/example.com.key;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers HIGH:!aNULL:!MD5;
        ssl_prefer_server_ciphers on;

        # Security headers
        add_header Strict-Transport-Security "max-age=31536000" always;
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-Content-Type-Options "nosniff" always;

        # API routes
        location /api/ {
            proxy_pass http://api_backend;

            # Headers
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # Timeouts
            proxy_connect_timeout 5s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;

            # Buffering
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;

            # Health check via proxy
            proxy_next_upstream error timeout invalid_header http_500 http_502 http_503;
        }

        # Static content
        location /static/ {
            proxy_pass http://static_backend;

            # Caching
            proxy_cache_valid 200 1h;
            proxy_cache_key "$scheme$request_method$host$request_uri";
            expires 1h;
            add_header Cache-Control "public, immutable";
        }

        # Application
        location / {
            proxy_pass http://web_backend;

            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # WebSocket support
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }

        # Health check endpoint (public)
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }

    # Admin server (separate domain)
    server {
        listen 443 ssl http2;
        server_name admin.example.com;

        ssl_certificate /etc/ssl/certs/admin.example.com.crt;
        ssl_certificate_key /etc/ssl/private/admin.example.com.key;

        # IP restriction
        allow 192.168.1.0/24;
        allow 10.0.0.0/8;
        deny all;

        location / {
            proxy_pass http://admin_backend;
            proxy_set_header Host $host;
        }
    }
}

# Stream block pour L4 load balancing (Nginx Plus ou module)
stream {
    # PostgreSQL load balancing
    upstream postgres {
        least_conn;
        server 10.0.10.10:5432 max_fails=3 fail_timeout=30s;
        server 10.0.10.11:5432 max_fails=3 fail_timeout=30s;
    }

    server {
        listen 5432;
        proxy_pass postgres;
        proxy_connect_timeout 1s;
    }
}
```

## Load Balancing dans Kubernetes

Kubernetes offre plusieurs niveaux de load balancing.

### 1. Service ClusterIP (interne)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: api-service
spec:
  type: ClusterIP  # Par d√©faut
  selector:
    app: api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
      - name: api
        image: myapi:v1
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
```

**Fonctionnement :**
- kube-proxy cr√©e des r√®gles iptables
- Load balancing round-robin entre les pods
- Interne au cluster seulement

### 2. Service NodePort (externe basique)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-service
spec:
  type: NodePort
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
    nodePort: 30080  # Port expos√© sur chaque n≈ìud (30000-32767)
```

**Acc√®s :** `http://<ANY_NODE_IP>:30080`

### 3. Service LoadBalancer (cloud)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"  # AWS NLB
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
spec:
  type: LoadBalancer
  selector:
    app: frontend
  ports:
  - protocol: TCP
    port: 443
    targetPort: 8443
  # Cloud provider provisionne automatiquement un LB externe
```

### 4. Ingress (L7 intelligent)

```yaml
# Ingress Controller (Nginx)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: main-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/rate-limit: "100"  # 100 req/s
    nginx.ingress.kubernetes.io/load-balance: "least_conn"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - example.com
    secretName: example-com-tls

  rules:
  # Routing bas√© sur le hostname
  - host: api.example.com
    http:
      paths:
      - path: /v1
        pathType: Prefix
        backend:
          service:
            name: api-v1-service
            port:
              number: 80
      - path: /v2
        pathType: Prefix
        backend:
          service:
            name: api-v2-service
            port:
              number: 80

  # Frontend
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80

  # Static content
  - host: static.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: static-service
            port:
              number: 80
---
# Service pour API v1
apiVersion: v1
kind: Service
metadata:
  name: api-v1-service
spec:
  selector:
    app: api
    version: v1
  ports:
  - port: 80
    targetPort: 8080
---
# Service pour API v2
apiVersion: v1
kind: Service
metadata:
  name: api-v2-service
spec:
  selector:
    app: api
    version: v2
  ports:
  - port: 80
    targetPort: 8080
```

### 5. Canary Deployment avec Ingress

```yaml
# Production (90% du trafic)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-production
  annotations:
    nginx.ingress.kubernetes.io/canary: "false"
spec:
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-stable
            port:
              number: 80
---
# Canary (10% du trafic)
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: api-canary
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "10"  # 10% du trafic
    # Ou bas√© sur header:
    # nginx.ingress.kubernetes.io/canary-by-header: "X-Canary"
    # nginx.ingress.kubernetes.io/canary-by-header-value: "true"
spec:
  rules:
  - host: api.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-canary
            port:
              number: 80
```

## Cloud Load Balancers

### AWS

```python
import boto3

elb = boto3.client('elbv2', region_name='us-east-1')

# Cr√©er un Application Load Balancer (L7)
response = elb.create_load_balancer(
    Name='my-api-alb',
    Subnets=['subnet-12345', 'subnet-67890'],
    SecurityGroups=['sg-abc123'],
    Scheme='internet-facing',
    Type='application',  # 'application' (L7) ou 'network' (L4)
    IpAddressType='ipv4',
    Tags=[
        {'Key': 'Environment', 'Value': 'production'},
        {'Key': 'Application', 'Value': 'api'}
    ]
)

lb_arn = response['LoadBalancers'][0]['LoadBalancerArn']

# Cr√©er un Target Group
tg_response = elb.create_target_group(
    Name='api-targets',
    Protocol='HTTP',
    Port=8080,
    VpcId='vpc-123456',
    HealthCheckProtocol='HTTP',
    HealthCheckPath='/health',
    HealthCheckIntervalSeconds=30,
    HealthCheckTimeoutSeconds=5,
    HealthyThresholdCount=2,
    UnhealthyThresholdCount=3,
    Matcher={'HttpCode': '200'}
)

tg_arn = tg_response['TargetGroups'][0]['TargetGroupArn']

# Enregistrer des targets (instances EC2)
elb.register_targets(
    TargetGroupArn=tg_arn,
    Targets=[
        {'Id': 'i-instance1'},
        {'Id': 'i-instance2'},
        {'Id': 'i-instance3'}
    ]
)

# Cr√©er un Listener (HTTPS)
elb.create_listener(
    LoadBalancerArn=lb_arn,
    Protocol='HTTPS',
    Port=443,
    Certificates=[{'CertificateArn': 'arn:aws:acm:us-east-1:...'}],
    DefaultActions=[{
        'Type': 'forward',
        'TargetGroupArn': tg_arn
    }]
)

# Cr√©er des r√®gles de routing (L7)
elb.create_rule(
    ListenerArn=listener_arn,
    Conditions=[
        {
            'Field': 'path-pattern',
            'Values': ['/api/v2/*']
        }
    ],
    Priority=10,
    Actions=[{
        'Type': 'forward',
        'TargetGroupArn': api_v2_tg_arn
    }]
)
```

### GCP

```python
from google.cloud import compute_v1

# Cr√©er un Backend Service
backend_service = compute_v1.BackendService()
backend_service.name = "api-backend-service"
backend_service.protocol = "HTTP"
backend_service.port_name = "http"
backend_service.timeout_sec = 30
backend_service.load_balancing_scheme = "EXTERNAL"

# Health check
backend_service.health_checks = ["global/healthChecks/api-health-check"]

# Backends (instance groups)
backend = compute_v1.Backend()
backend.group = "zones/us-central1-a/instanceGroups/api-group"
backend.balancing_mode = "UTILIZATION"
backend.max_utilization = 0.8
backend.capacity_scaler = 1.0

backend_service.backends = [backend]

# Cr√©er
client = compute_v1.BackendServicesClient()
operation = client.insert(
    project="my-project",
    backend_service_resource=backend_service
)
```

## M√©triques et Monitoring

### M√©triques cl√©s d'un load balancer

```python
from prometheus_client import Counter, Histogram, Gauge
import time

# Requ√™tes par backend
backend_requests = Counter(
    'lb_backend_requests_total',
    'Total requests to backend',
    ['backend', 'method', 'status']
)

# Latence par backend
backend_latency = Histogram(
    'lb_backend_latency_seconds',
    'Backend response time',
    ['backend'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]
)

# Connexions actives
active_connections = Gauge(
    'lb_active_connections',
    'Number of active connections',
    ['backend']
)

# Backends sains
healthy_backends = Gauge(
    'lb_healthy_backends_total',
    'Number of healthy backends'
)

# Taux d'erreur
error_rate = Counter(
    'lb_errors_total',
    'Total errors',
    ['backend', 'error_type']
)

# Utilisation dans le load balancer
class MonitoredLoadBalancer:
    def __init__(self, servers):
        self.servers = servers
        self.healthy = {s: True for s in servers}

    def handle_request(self, request):
        # Choisir le backend
        backend = self.get_next_server()

        # M√©triques
        active_connections.labels(backend=backend).inc()

        start_time = time.time()

        try:
            # Proxier la requ√™te
            response = self.proxy_to_backend(backend, request)

            # Enregistrer succ√®s
            backend_requests.labels(
                backend=backend,
                method=request.method,
                status=response.status_code
            ).inc()

            return response

        except Exception as e:
            # Enregistrer erreur
            error_rate.labels(
                backend=backend,
                error_type=type(e).__name__
            ).inc()
            raise

        finally:
            # Latence
            duration = time.time() - start_time
            backend_latency.labels(backend=backend).observe(duration)

            # D√©cr√©menter connexions actives
            active_connections.labels(backend=backend).dec()
```

### Dashboard Grafana (Prometheus queries)

```yaml
# Queries pour dashboard

# Taux de requ√™tes par backend
rate(lb_backend_requests_total[5m])

# Latence P95 par backend
histogram_quantile(0.95,
  sum(rate(lb_backend_latency_seconds_bucket[5m])) by (backend, le)
)

# Taux d'erreur
rate(lb_errors_total[5m]) / rate(lb_backend_requests_total[5m])

# Distribution du trafic
sum(rate(lb_backend_requests_total[5m])) by (backend)

# Backends sains vs total
lb_healthy_backends_total / count(lb_backend_requests_total)
```

## Pi√®ges courants et solutions

### Pi√®ge 1 : Sticky sessions casse le load balancing

**Sympt√¥me :** Un serveur re√ßoit beaucoup plus de trafic que les autres.

**Cause :** Sticky sessions + distribution in√©gale des utilisateurs.

**Solution :**
```python
# Utiliser consistent hashing au lieu de sticky sessions
# OU utiliser session store externe (Redis)
# OU limiter la dur√©e des sticky sessions
```

### Pi√®ge 2 : Health checks trop agressifs

**Sympt√¥me :** Serveurs marqu√©s down alors qu'ils sont op√©rationnels.

**Cause :** Timeouts trop courts, seuil d'√©checs trop bas.

**Solution :**
```
# HAProxy - configuration raisonnable
option httpchk GET /health
http-check expect status 200
server web1 10.0.0.10:80 check inter 5s rise 2 fall 3

# inter 5s : v√©rifier toutes les 5s
# rise 2 : 2 checks OK pour marquer UP
# fall 3 : 3 checks KO pour marquer DOWN
```

### Pi√®ge 3 : Pas de drain avant shutdown

**Sympt√¥me :** Requ√™tes √©chouent lors du d√©ploiement.

**Solution :**
```python
# Graceful shutdown avec drain period

import signal
import time

class GracefulShutdown:
    def __init__(self, drain_period=30):
        self.drain_period = drain_period
        self.shutting_down = False

        signal.signal(signal.SIGTERM, self.handle_sigterm)

    def handle_sigterm(self, signum, frame):
        print(f"SIGTERM received, draining for {self.drain_period}s...")
        self.shutting_down = True

        # Retourner 503 au health check
        # pour que le LB arr√™te d'envoyer du trafic

        time.sleep(self.drain_period)

        # Maintenant, shutdown
        print("Drain complete, shutting down")
        exit(0)
```

### Pi√®ge 4 : Connection pooling mal configur√©

**Sympt√¥me :** Nouvelles connexions lentes alors que le serveur n'est pas charg√©.

**Solution :**
```python
# Nginx - keepalive connections vers backend
upstream backend {
    server 10.0.0.10:8080;
    keepalive 32;  # Garder 32 connexions idle
}

server {
    location / {
        proxy_pass http://backend;
        proxy_http_version 1.1;  # N√©cessaire pour keepalive
        proxy_set_header Connection "";
    }
}
```

## R√©sum√© : Choix du load balancer

### Checklist de d√©cision

| Besoin | Solution recommand√©e |
|--------|---------------------|
| **Load balancing DB/Cache** | HAProxy L4, AWS NLB |
| **Load balancing HTTP simple** | Nginx, HAProxy L7 |
| **Routing complexe (microservices)** | Traefik, Kong, AWS ALB |
| **Kubernetes** | Ingress (Nginx/Traefik) |
| **Global traffic** | Cloudflare, AWS Global Accelerator |
| **Performance maximale** | Envoy, HAProxy |
| **Simplicit√©** | Nginx |

### L4 vs L7 : Quand choisir quoi ?

**Utilisez L4 si :**
- Performance brute critique (millions de connexions/s)
- Trafic non-HTTP (DB, cache, custom protocol)
- Pas besoin de routing intelligent
- SSL passthrough voulu

**Utilisez L7 si :**
- Besoin de routing bas√© sur URL/headers
- SSL termination centralis√©e
- A/B testing, canary deployments
- Application-aware health checks
- Manipulation de headers/body

## Conclusion

Le load balancing est **fondamental** pour toute application moderne n√©cessitant scalabilit√© et haute disponibilit√©.

**√Ä retenir :**
- **L4 = rapide** mais simple (IP + port)
- **L7 = intelligent** mais plus lent (contenu HTTP)
- Choisir l'**algorithme** selon le cas d'usage
- Impl√©menter des **health checks** robustes
- √âviter sticky sessions si possible (**session store** externe)
- Monitorer les **m√©triques** cl√©s
- Tester le **failover** r√©guli√®rement

**Prochaine √©tape :** La section suivante explore les **Proxys et Reverse Proxys**, qui compl√®tent le load balancing avec des fonctionnalit√©s de cache, s√©curit√© et transformation de requ√™tes.

---


‚è≠Ô∏è [Proxys et reverse proxys](/09-architectures-avancees/04-proxys-reverse-proxys.md)
