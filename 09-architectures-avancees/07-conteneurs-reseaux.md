ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

# 9.7 Conteneurs et rÃ©seaux (Docker, Kubernetes networking)

## Introduction

Les **conteneurs** ont rÃ©volutionnÃ© le dÃ©ploiement d'applications en encapsulant le code et ses dÃ©pendances dans des unitÃ©s isolÃ©es et portables. Mais cette isolation pose un dÃ©fi rÃ©seau fondamental : **comment permettre aux conteneurs de communiquer entre eux et avec le monde extÃ©rieur ?**

### L'analogie des appartements

**Sans conteneurs (maison traditionnelle) :**
```
Une grande maison = Un serveur physique
  â€¢ Toutes les apps partagent le mÃªme rÃ©seau
  â€¢ Conflits de ports possibles
  â€¢ Isolation difficile
```

**Avec conteneurs (immeuble) :**
```
Immeuble = Serveur physique
  â”œâ”€ Appartement 1 (conteneur) : App A
  â”œâ”€ Appartement 2 (conteneur) : App B
  â””â”€ Appartement 3 (conteneur) : App C

Chaque appartement a :
  â€¢ Son propre rÃ©seau interne (namespaces rÃ©seau)
  â€¢ Peut communiquer via le rÃ©seau de l'immeuble (bridge)
  â€¢ Peut avoir un "interphone" vers l'extÃ©rieur (port mapping)
```

### Le problÃ¨me fondamental

```
CHALLENGE RÃ‰SEAU DES CONTENEURS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. ISOLATION
   Conteneur A ne doit pas voir le rÃ©seau de Conteneur B
   â†’ Namespaces rÃ©seau Linux

2. COMMUNICATION
   Conteneur A doit pouvoir appeler Conteneur B
   â†’ Virtual networks, bridges

3. DÃ‰COUVERTE
   Conteneur A doit trouver Conteneur B (adresse change)
   â†’ DNS intÃ©grÃ©, service discovery

4. ACCÃˆS EXTERNE
   Client externe doit atteindre Conteneur A
   â†’ Port mapping, load balancing

5. SÃ‰CURITÃ‰
   ContrÃ´ler qui peut parler Ã  qui
   â†’ Network policies, firewalls
```

## Docker Networking

### Les modes rÃ©seau Docker

Docker propose **5 modes rÃ©seau** principaux :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. BRIDGE (dÃ©faut)                                 â”‚
â”‚     Conteneurs sur rÃ©seau privÃ© virtualisÃ©          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2. HOST                                            â”‚
â”‚     Conteneur utilise directement le rÃ©seau hÃ´te    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. OVERLAY                                         â”‚
â”‚     RÃ©seau multi-hÃ´tes (Docker Swarm)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  4. MACVLAN                                         â”‚
â”‚     Conteneur avec sa propre adresse MAC            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. NONE                                            â”‚
â”‚     Pas de rÃ©seau (isolation totale)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 1. Mode Bridge (par dÃ©faut)

Le mode le plus courant. Chaque conteneur obtient une IP sur un rÃ©seau privÃ© virtuel.

```
ARCHITECTURE BRIDGE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Host (eth0: 192.168.1.10)
  â”‚
  â””â”€ docker0 (bridge virtuel: 172.17.0.1)
       â”‚
       â”œâ”€ Container 1 (172.17.0.2)
       â”œâ”€ Container 2 (172.17.0.3)
       â””â”€ Container 3 (172.17.0.4)

Communication :
  â€¢ Container 1 â†’ Container 2 : Direct via bridge
  â€¢ Container 1 â†’ Internet : NAT via docker0
  â€¢ Internet â†’ Container 1 : Port mapping requis
```

**Exemple pratique :**

```bash
# CrÃ©er un rÃ©seau bridge custom
docker network create --driver bridge my-app-network

# Inspecter le rÃ©seau
docker network inspect my-app-network
# Output :
# {
#     "Name": "my-app-network",
#     "Driver": "bridge",
#     "Subnet": "172.18.0.0/16",
#     "Gateway": "172.18.0.1"
# }

# Lancer des conteneurs sur ce rÃ©seau
docker run -d --name web --network my-app-network nginx
docker run -d --name api --network my-app-network python:3.11

# Les conteneurs peuvent communiquer par nom
docker exec web ping api  # âœ… Fonctionne !
```

**Configuration rÃ©seau dans un Dockerfile :**

```dockerfile
FROM python:3.11-slim

# L'application Ã©coute sur le port 8000 (interne au conteneur)
EXPOSE 8000

WORKDIR /app
COPY . .

RUN pip install -r requirements.txt

# Le conteneur Ã©coute sur 0.0.0.0 (toutes interfaces)
CMD ["python", "app.py"]
```

```bash
# Lancer avec port mapping
# Mapping : Host:8080 â†’ Container:8000
docker run -d -p 8080:8000 --name myapp myapp:latest

# AccÃ¨s depuis l'hÃ´te
curl http://localhost:8080  # â†’ RedirigÃ© vers le port 8000 du conteneur
```

**DNS intÃ©grÃ© Docker :**

```python
# app.py dans le conteneur "web"
import requests

# Appeler un autre conteneur par son nom (DNS interne)
# Docker rÃ©sout automatiquement "api" â†’ 172.18.0.3
response = requests.get('http://api:8000/data')

# Pas besoin de connaÃ®tre l'IP !
# Docker maintient un DNS interne qui mappe :
# api â†’ 172.18.0.3
# db â†’ 172.18.0.4
# cache â†’ 172.18.0.5
```

#### 2. Mode Host

Le conteneur **partage le rÃ©seau de l'hÃ´te** directement. Pas d'isolation rÃ©seau.

```
MODE HOST
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Host (eth0: 192.168.1.10)
  â”‚
  â””â”€ Container (utilise directement eth0: 192.168.1.10)

Avantages :
  âœ… Performance maximale (pas de NAT)
  âœ… AccÃ¨s direct aux interfaces hÃ´te

InconvÃ©nients :
  âŒ Pas d'isolation rÃ©seau
  âŒ Conflits de ports possibles
  âŒ Moins sÃ©curisÃ©
```

```bash
# Lancer en mode host
docker run --network host nginx

# Nginx Ã©coute directement sur le port 80 de l'hÃ´te
# Pas de port mapping nÃ©cessaire
curl http://192.168.1.10  # AccÃ¨s direct
```

**Cas d'usage :** Monitoring (Prometheus), profiling, applications nÃ©cessitant performances rÃ©seau maximales.

#### 3. Mode Overlay (Multi-host)

RÃ©seau virtuel **spanning plusieurs hÃ´tes** Docker. UtilisÃ© dans Docker Swarm ou Kubernetes.

```
MODE OVERLAY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Host 1 (192.168.1.10)           Host 2 (192.168.1.20)
  â”‚                                 â”‚
  â””â”€ overlay network â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       (10.0.0.0/24)
       â”‚                            â”‚
    Container A                  Container B
    (10.0.0.2)                   (10.0.0.3)

Container A peut ping Container B directement !
â†’ Tunneling VXLAN entre les hÃ´tes
```

```bash
# CrÃ©er un rÃ©seau overlay (Docker Swarm requis)
docker network create --driver overlay --attachable my-overlay

# DÃ©ployer des services sur plusieurs nÅ“uds
docker service create \
  --name web \
  --network my-overlay \
  --replicas 3 \
  nginx

# Les 3 rÃ©plicas peuvent Ãªtre sur des hÃ´tes diffÃ©rents
# mais communiquent comme s'ils Ã©taient sur le mÃªme rÃ©seau
```

#### 4. Mode Macvlan

Donne une **adresse MAC unique** Ã  chaque conteneur. ApparaÃ®t comme un device physique sur le rÃ©seau.

```
MODE MACVLAN
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

RÃ©seau physique : 192.168.1.0/24

Host (192.168.1.10)
  â”‚
  â”œâ”€ Container 1 (192.168.1.100, MAC: aa:bb:cc:dd:ee:01)
  â”œâ”€ Container 2 (192.168.1.101, MAC: aa:bb:cc:dd:ee:02)
  â””â”€ Container 3 (192.168.1.102, MAC: aa:bb:cc:dd:ee:03)

Les conteneurs sont "first-class citizens" sur le rÃ©seau
```

```bash
# CrÃ©er un rÃ©seau macvlan
docker network create -d macvlan \
  --subnet=192.168.1.0/24 \
  --gateway=192.168.1.1 \
  -o parent=eth0 \
  macvlan-net

# Lancer un conteneur avec IP statique
docker run -d \
  --network macvlan-net \
  --ip 192.168.1.100 \
  nginx
```

**Cas d'usage :** Applications legacy nÃ©cessitant adresse MAC dÃ©diÃ©e, monitoring rÃ©seau, intÃ©gration avec infra existante.

#### 5. Mode None

**Aucun rÃ©seau**. Isolation totale.

```bash
docker run -d --network none alpine sleep 3600

# Le conteneur n'a aucune interface rÃ©seau
docker exec <container> ip addr
# Output :
# 1: lo: <LOOPBACK,UP,LOWER_UP>
#     inet 127.0.0.1/8 scope host lo
# (seulement loopback local)
```

**Cas d'usage :** SÃ©curitÃ© maximale, batch jobs n'ayant pas besoin de rÃ©seau.

### Docker Compose Networking

Docker Compose crÃ©e automatiquement un rÃ©seau pour les services dÃ©finis.

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Service frontend
  web:
    image: nginx:alpine
    ports:
      - "80:80"
    networks:
      - frontend
      - backend
    depends_on:
      - api

  # Service API
  api:
    build: ./api
    ports:
      - "8000:8000"
    networks:
      - backend
      - database
    environment:
      - DB_HOST=postgres  # Utilise le nom du service
      - REDIS_HOST=redis
    depends_on:
      - postgres
      - redis

  # Base de donnÃ©es
  postgres:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: secret
    volumes:
      - pgdata:/var/lib/postgresql/data
    networks:
      - database
    # Pas de ports exposÃ©s â†’ accessible uniquement en interne

  # Cache
  redis:
    image: redis:7-alpine
    networks:
      - backend

# DÃ©finition des rÃ©seaux
networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
  database:
    driver: bridge
    internal: true  # Pas d'accÃ¨s Internet (sÃ©curitÃ©)

volumes:
  pgdata:
```

**RÃ©sultat :** Docker Compose crÃ©e automatiquement :

```
RÃ©seau : myapp_frontend (172.20.0.0/16)
  â€¢ web (172.20.0.2)

RÃ©seau : myapp_backend (172.21.0.0/16)
  â€¢ web (172.21.0.2)
  â€¢ api (172.21.0.3)
  â€¢ redis (172.21.0.4)

RÃ©seau : myapp_database (172.22.0.0/16) [internal]
  â€¢ api (172.22.0.2)
  â€¢ postgres (172.22.0.3)
```

**Communication :**

```python
# Dans le conteneur 'api'
import psycopg2
import redis

# Connection Ã  PostgreSQL par nom de service
conn = psycopg2.connect(
    host='postgres',  # Docker rÃ©sout automatiquement
    database='myapp',
    user='user',
    password='pass'
)

# Connection Ã  Redis par nom de service
cache = redis.Redis(host='redis', port=6379)

# Les noms de services = hostnames DNS
# Docker maintient un DNS interne
```

### Port Mapping avancÃ©

```bash
# Syntaxe : -p [HOST_IP:]HOST_PORT:CONTAINER_PORT[/PROTOCOL]

# Basique : Host:8080 â†’ Container:80
docker run -p 8080:80 nginx

# IP spÃ©cifique : Ã‰couter uniquement sur localhost
docker run -p 127.0.0.1:8080:80 nginx

# Plage de ports
docker run -p 8080-8090:8080-8090 myapp

# UDP au lieu de TCP
docker run -p 53:53/udp dns-server

# Plusieurs mappings
docker run \
  -p 80:80 \
  -p 443:443 \
  -p 8080:8080 \
  myapp

# Port alÃ©atoire sur l'hÃ´te (utile pour scaling)
docker run -P nginx  # -P mappe tous les EXPOSE vers ports alÃ©atoires
docker port <container>  # Voir les mappings
```

**VÃ©rification des ports :**

```bash
# Lister les ports d'un conteneur
docker port myapp
# 80/tcp -> 0.0.0.0:8080
# 443/tcp -> 0.0.0.0:8443

# Depuis l'hÃ´te
netstat -tlnp | grep docker
# tcp  0  0 0.0.0.0:8080  0.0.0.0:*  LISTEN  1234/docker-proxy

# Test de connectivitÃ©
curl http://localhost:8080
```

### Inspection et debugging rÃ©seau Docker

```bash
# Lister tous les rÃ©seaux
docker network ls

# Inspecter un rÃ©seau
docker network inspect bridge

# Voir les conteneurs sur un rÃ©seau
docker network inspect bridge | jq '.[0].Containers'

# Connecter un conteneur Ã  un rÃ©seau supplÃ©mentaire
docker network connect my-network mycontainer

# DÃ©connecter
docker network disconnect my-network mycontainer

# Voir les interfaces rÃ©seau d'un conteneur
docker exec mycontainer ip addr

# Voir les routes
docker exec mycontainer ip route

# Voir les connexions actives
docker exec mycontainer netstat -tlnp

# Capturer le trafic rÃ©seau (tcpdump dans conteneur)
docker exec mycontainer tcpdump -i eth0 -w /tmp/capture.pcap
docker cp mycontainer:/tmp/capture.pcap .
wireshark capture.pcap
```

**Script de diagnostic rÃ©seau Docker :**

```python
#!/usr/bin/env python3
import subprocess
import json

def diagnose_container_network(container_name):
    """
    Diagnostic rÃ©seau complet d'un conteneur
    """
    print(f"=== Diagnostic rÃ©seau pour {container_name} ===\n")

    # 1. Informations gÃ©nÃ©rales
    inspect = subprocess.run(
        ['docker', 'inspect', container_name],
        capture_output=True,
        text=True
    )

    if inspect.returncode != 0:
        print(f"Erreur : conteneur {container_name} non trouvÃ©")
        return

    data = json.loads(inspect.stdout)[0]

    # 2. RÃ©seaux attachÃ©s
    print("ğŸ“¡ RÃ©seaux attachÃ©s :")
    networks = data['NetworkSettings']['Networks']
    for net_name, net_info in networks.items():
        print(f"  â€¢ {net_name}")
        print(f"    IP: {net_info['IPAddress']}")
        print(f"    Gateway: {net_info['Gateway']}")
        print(f"    MAC: {net_info['MacAddress']}")

    # 3. Port mappings
    print("\nğŸ”Œ Port mappings :")
    ports = data['NetworkSettings']['Ports'] or {}
    if ports:
        for container_port, host_bindings in ports.items():
            if host_bindings:
                for binding in host_bindings:
                    print(f"  â€¢ {binding['HostIp']}:{binding['HostPort']} â†’ {container_port}")
    else:
        print("  Aucun port mappÃ©")

    # 4. DNS
    print("\nğŸŒ Configuration DNS :")
    dns_config = data['HostConfig']
    print(f"  DNS servers: {dns_config.get('Dns', [])}")
    print(f"  DNS search: {dns_config.get('DnsSearch', [])}")

    # 5. Test de connectivitÃ©
    print("\nğŸ” Tests de connectivitÃ© :")

    # Ping gateway
    gateway = list(networks.values())[0]['Gateway']
    ping_gw = subprocess.run(
        ['docker', 'exec', container_name, 'ping', '-c', '1', '-W', '1', gateway],
        capture_output=True
    )
    print(f"  Gateway ({gateway}): {'âœ… OK' if ping_gw.returncode == 0 else 'âŒ FAIL'}")

    # Ping DNS public
    ping_dns = subprocess.run(
        ['docker', 'exec', container_name, 'ping', '-c', '1', '-W', '1', '8.8.8.8'],
        capture_output=True
    )
    print(f"  Internet (8.8.8.8): {'âœ… OK' if ping_dns.returncode == 0 else 'âŒ FAIL'}")

    # DNS resolution
    dns_test = subprocess.run(
        ['docker', 'exec', container_name, 'nslookup', 'google.com'],
        capture_output=True
    )
    print(f"  DNS resolution: {'âœ… OK' if dns_test.returncode == 0 else 'âŒ FAIL'}")

# Usage
if __name__ == '__main__':
    import sys
    if len(sys.argv) != 2:
        print("Usage: python diagnose.py <container_name>")
        sys.exit(1)

    diagnose_container_network(sys.argv[1])
```

## Kubernetes Networking

Kubernetes a un modÃ¨le rÃ©seau **radicalement diffÃ©rent** de Docker. Il repose sur des principes plus complexes mais plus puissants.

### Principes fondamentaux du rÃ©seau Kubernetes

```
MODÃˆLE RÃ‰SEAU KUBERNETES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Tous les Pods peuvent communiquer avec tous les Pods
   sans NAT (flat network)

2. Tous les Nodes peuvent communiquer avec tous les Pods
   sans NAT

3. L'IP qu'un Pod voit de lui-mÃªme est la mÃªme IP
   que les autres Pods voient

4. Services : abstraction stable devant des Pods Ã©phÃ©mÃ¨res
```

### Architecture rÃ©seau Kubernetes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  KUBERNETES CLUSTER                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  Node 1 (192.168.1.10)      Node 2 (192.168.1.20)   â”‚
â”‚    â”‚                           â”‚                    â”‚
â”‚    â”œâ”€ Pod A (10.244.1.5)       â”œâ”€ Pod C (10.244.2.8)â”‚
â”‚    â”‚   â””â”€ Container            â”‚   â””â”€ Container     â”‚
â”‚    â”‚                           â”‚                    â”‚
â”‚    â””â”€ Pod B (10.244.1.6)       â””â”€ Pod D (10.244.2.9)â”‚
â”‚        â””â”€ Container                â””â”€ Container     â”‚
â”‚                                                     â”‚
â”‚  Pod A peut ping Pod C directement (10.244.2.8)     â”‚
â”‚  â†’ Routing assurÃ© par le CNI                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CNI (Container Network Interface)

Le **CNI** est le plugin qui implÃ©mente le rÃ©seau. Kubernetes dÃ©lÃ¨gue complÃ¨tement le rÃ©seau au CNI.

**CNI populaires :**

| CNI | Type | CaractÃ©ristiques | Usage |
|-----|------|------------------|-------|
| **Calico** | L3 | Network Policies avancÃ©es, BGP | Production, sÃ©curitÃ© |
| **Cilium** | eBPF | Performance, observabilitÃ© | Moderne, performant |
| **Flannel** | Overlay | Simple, facile | Dev, petits clusters |
| **Weave** | Overlay | Encryption native | SÃ©curitÃ© simple |
| **Canal** | Calico+Flannel | Combo rÃ©seau+policies | Polyvalent |

**Installation d'un CNI (Calico) :**

```bash
# Installer Calico sur Kubernetes
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

# VÃ©rifier l'installation
kubectl get pods -n kube-system | grep calico
# calico-node-xxxxx    1/1     Running
# calico-kube-controllers-xxxxx    1/1     Running

# Voir les interfaces rÃ©seau crÃ©Ã©es
ip link show | grep cali
# cali1a2b3c4d5e6f@if4: <BROADCAST,MULTICAST,UP,LOWER_UP>
# (une interface veth par Pod)
```

### Les Pods et leur rÃ©seau

Chaque **Pod** obtient sa propre IP unique dans le cluster.

```yaml
# simple-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    ports:
    - containerPort: 80
      name: http
```

```bash
# DÃ©ployer le Pod
kubectl apply -f simple-pod.yaml

# Voir l'IP du Pod
kubectl get pod nginx-pod -o wide
# NAME        READY   STATUS    IP            NODE
# nginx-pod   1/1     Running   10.244.1.5    node1

# Depuis un autre Pod, on peut accÃ©der directement
kubectl run -it --rm debug --image=busybox --restart=Never -- sh
/ # wget -O- http://10.244.1.5
# <!DOCTYPE html>...  âœ… Fonctionne !
```

**Pod multi-conteneurs (mÃªme namespace rÃ©seau) :**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-container
spec:
  containers:
  # Conteneur 1 : Application
  - name: app
    image: myapp:latest
    ports:
    - containerPort: 8080

  # Conteneur 2 : Sidecar (logs, metrics)
  - name: sidecar
    image: fluent/fluentd
    # Les deux conteneurs partagent localhost !
    # Le sidecar peut accÃ©der Ã  l'app via localhost:8080
```

```python
# Dans le conteneur 'app'
from flask import Flask
app = Flask(__name__)

@app.route('/')
def index():
    return "Hello from app"

if __name__ == '__main__':
    # Ã‰coute sur localhost (partagÃ© avec sidecar)
    app.run(host='127.0.0.1', port=8080)
```

```python
# Dans le conteneur 'sidecar'
import requests

# AccÃ¨s Ã  l'app via localhost (mÃªme Pod)
response = requests.get('http://localhost:8080/')
print(response.text)  # "Hello from app"
```

### Services Kubernetes

Les **Services** fournissent une **IP stable** et un **DNS** pour accÃ©der Ã  un ensemble de Pods.

#### 1. ClusterIP (dÃ©faut)

Service accessible **uniquement dans le cluster**.

```yaml
# service-clusterip.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: ClusterIP
  selector:
    app: nginx  # SÃ©lectionne les Pods avec ce label
  ports:
  - port: 80        # Port du Service
    targetPort: 80  # Port du Pod
    protocol: TCP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
```

```bash
kubectl apply -f service-clusterip.yaml

# Voir le Service
kubectl get svc nginx-service
# NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)
# nginx-service   ClusterIP   10.96.100.200   <none>        80/TCP

# Le Service est accessible via :
# 1. IP du Service : 10.96.100.200
# 2. DNS : nginx-service.default.svc.cluster.local

# Test depuis un Pod
kubectl run -it --rm test --image=busybox --restart=Never -- sh
/ # wget -O- http://nginx-service
# <!DOCTYPE html>...  âœ… Load-balancÃ© entre les 3 Pods !

/ # nslookup nginx-service
# Server:    10.96.0.10
# Address:   10.96.0.10:53
#
# Name:      nginx-service.default.svc.cluster.local
# Address:   10.96.100.200
```

**Comment Ã§a marche (kube-proxy) :**

```
CLIENT POD
    â†“ (demande: nginx-service:80)
DNS interne (CoreDNS)
    â†“ (rÃ©sout: 10.96.100.200)
kube-proxy (iptables rules)
    â†“ (load balance vers un Pod backend)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Pod 1     Pod 2     Pod 3
(10.244.1.5) (10.244.1.6) (10.244.2.7)
```

**Voir les rÃ¨gles iptables crÃ©Ã©es :**

```bash
# Sur un nÅ“ud du cluster
sudo iptables-save | grep nginx-service

# Exemple de rÃ¨gles crÃ©Ã©es :
# -A KUBE-SERVICES -d 10.96.100.200/32 -p tcp -m tcp --dport 80 \
#   -j KUBE-SVC-XXXXX
# -A KUBE-SVC-XXXXX -m statistic --mode random --probability 0.33 \
#   -j KUBE-SEP-POD1  # 33% vers Pod 1
# -A KUBE-SVC-XXXXX -m statistic --mode random --probability 0.50 \
#   -j KUBE-SEP-POD2  # 50% vers Pod 2
# -A KUBE-SVC-XXXXX -j KUBE-SEP-POD3  # Reste vers Pod 3
```

#### 2. NodePort

Service accessible via **un port sur chaque nÅ“ud** du cluster.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-nodeport
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080  # Port sur chaque nÅ“ud (30000-32767)
    protocol: TCP
```

```bash
kubectl apply -f service-nodeport.yaml

kubectl get svc nginx-nodeport
# NAME             TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)
# nginx-nodeport   NodePort   10.96.100.201   <none>        80:30080/TCP

# Accessible via N'IMPORTE QUEL nÅ“ud du cluster
curl http://192.168.1.10:30080  # Node 1
curl http://192.168.1.20:30080  # Node 2
curl http://192.168.1.30:30080  # Node 3
# Tous routent vers les mÃªmes Pods backend !
```

**Flux de requÃªte :**

```
Client externe
    â†“ (http://node-ip:30080)
Node (kube-proxy)
    â†“ (DNAT vers ClusterIP)
Service ClusterIP
    â†“ (load balance)
Backend Pods
```

#### 3. LoadBalancer

Service avec **load balancer externe** (cloud provider).

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-lb
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
  - port: 80
    targetPort: 80
```

```bash
kubectl apply -f service-loadbalancer.yaml

# Sur un cloud provider (AWS, GCP, Azure)
kubectl get svc nginx-lb
# NAME       TYPE           CLUSTER-IP      EXTERNAL-IP
# nginx-lb   LoadBalancer   10.96.100.202   35.123.45.67

# Accessible publiquement
curl http://35.123.45.67
```

**Architecture (AWS EKS exemple) :**

```
Internet
    â†“
AWS ELB (35.123.45.67)
    â†“
NodePort :30080 sur chaque nÅ“ud
    â†“
Service ClusterIP
    â†“
Backend Pods
```

#### 4. ExternalName

CrÃ©e un alias DNS vers un service externe.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: external-database
spec:
  type: ExternalName
  externalName: mysql.example.com  # DNS externe
```

```python
# Dans un Pod
import pymysql

# Connection via le Service ExternalName
conn = pymysql.connect(
    host='external-database',  # RÃ©solu vers mysql.example.com
    user='user',
    password='pass'
)
```

### Ingress : Routage L7

Les **Ingress** permettent le routage HTTP/HTTPS intelligent.

```yaml
# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  ingressClassName: nginx

  tls:
  - hosts:
    - www.example.com
    - api.example.com
    secretName: example-tls

  rules:
  # Routage basÃ© sur le hostname
  - host: www.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80

  - host: api.example.com
    http:
      paths:
      # Routage vers diffÃ©rents backends selon le path
      - path: /v1
        pathType: Prefix
        backend:
          service:
            name: api-v1-service
            port:
              number: 8000

      - path: /v2
        pathType: Prefix
        backend:
          service:
            name: api-v2-service
            port:
              number: 8000

      - path: /auth
        pathType: Prefix
        backend:
          service:
            name: auth-service
            port:
              number: 9000
```

```bash
# Installer un Ingress Controller (Nginx)
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml

# Appliquer l'Ingress
kubectl apply -f ingress.yaml

# VÃ©rifier
kubectl get ingress
# NAME          CLASS   HOSTS                            ADDRESS
# app-ingress   nginx   www.example.com,api.example.com  35.123.45.67

# Le trafic est routÃ© selon les rÃ¨gles
curl https://www.example.com/         # â†’ frontend-service
curl https://api.example.com/v1/data  # â†’ api-v1-service
curl https://api.example.com/v2/data  # â†’ api-v2-service
curl https://api.example.com/auth     # â†’ auth-service
```

### Network Policies

Les **Network Policies** contrÃ´lent le trafic entre Pods (firewall).

```yaml
# network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-network-policy
  namespace: production
spec:
  # Appliquer Ã  tous les Pods avec le label app=api
  podSelector:
    matchLabels:
      app: api

  # Types de trafic Ã  contrÃ´ler
  policyTypes:
  - Ingress
  - Egress

  # RÃ¨gles INGRESS (trafic entrant)
  ingress:
  # RÃ¨gle 1 : Accepter le trafic depuis les Pods frontend
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8000

  # RÃ¨gle 2 : Accepter le trafic depuis l'Ingress Controller
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8000

  # RÃ¨gles EGRESS (trafic sortant)
  egress:
  # RÃ¨gle 1 : Autoriser connexion Ã  la DB
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432

  # RÃ¨gle 2 : Autoriser connexion Ã  Redis
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379

  # RÃ¨gle 3 : Autoriser DNS
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53

  # RÃ¨gle 4 : Autoriser HTTPS vers Internet
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
```

**Politique par dÃ©faut : Deny All**

```yaml
# deny-all.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}  # Tous les Pods du namespace
  policyTypes:
  - Ingress
  - Egress
  # Pas de rÃ¨gles â†’ tout est bloquÃ© par dÃ©faut
```

```bash
# Appliquer les policies
kubectl apply -f deny-all.yaml
kubectl apply -f network-policy.yaml

# Tester
kubectl run test --image=busybox -it --rm -- sh

# Depuis le Pod 'test', essayer d'accÃ©der Ã  l'API
/ # wget -O- http://api-service:8000
# âŒ Timeout (bloquÃ© par Network Policy)

# Mais depuis un Pod frontend
kubectl run frontend --labels="app=frontend" --image=busybox -it --rm -- sh
/ # wget -O- http://api-service:8000
# âœ… Fonctionne (autorisÃ© par la policy)
```

### DNS interne Kubernetes

Kubernetes maintient un DNS interne (CoreDNS).

```
FORMAT DNS KUBERNETES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Service DNS :
  <service-name>.<namespace>.svc.cluster.local

Pod DNS :
  <pod-ip-avec-tirets>.<namespace>.pod.cluster.local

Exemple :
  nginx-service.default.svc.cluster.local
  10-244-1-5.default.pod.cluster.local
```

```bash
# Voir les pods CoreDNS
kubectl get pods -n kube-system -l k8s-app=kube-dns

# Config CoreDNS
kubectl get configmap coredns -n kube-system -o yaml
```

**Utilisation du DNS :**

```python
# app.py dans le namespace 'production'
import requests

# MÃªme namespace (production)
response = requests.get('http://api-service:8000')
# RÃ©solu vers : api-service.production.svc.cluster.local

# Autre namespace
response = requests.get('http://auth-service.auth:9000')
# RÃ©solu vers : auth-service.auth.svc.cluster.local

# FQDN complet (toujours fonctionne)
response = requests.get('http://database.data.svc.cluster.local:5432')
```

**Customisation DNS du Pod :**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: custom-dns
spec:
  containers:
  - name: app
    image: myapp:latest

  # Configuration DNS custom
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
    - 8.8.8.8
    - 8.8.4.4
    searches:
    - production.svc.cluster.local
    - svc.cluster.local
    - cluster.local
    options:
    - name: ndots
      value: "2"
```

## Cas d'usage rÃ©els

### Cas 1 : Application microservices complÃ¨te

```yaml
# full-app.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: myapp
---
# Frontend (React)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: myapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
      tier: web
  template:
    metadata:
      labels:
        app: frontend
        tier: web
    spec:
      containers:
      - name: frontend
        image: myapp/frontend:v1
        ports:
        - containerPort: 3000
        env:
        - name: REACT_APP_API_URL
          value: "http://api-service:8000"
---
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  namespace: myapp
spec:
  selector:
    app: frontend
  ports:
  - port: 80
    targetPort: 3000
---
# Backend API (Python)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
  namespace: myapp
spec:
  replicas: 5
  selector:
    matchLabels:
      app: api
      tier: backend
  template:
    metadata:
      labels:
        app: api
        tier: backend
    spec:
      containers:
      - name: api
        image: myapp/api:v2
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          value: "postgresql://postgres-service:5432/myapp"
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: api-service
  namespace: myapp
spec:
  selector:
    app: api
  ports:
  - port: 8000
    targetPort: 8000
---
# Database (PostgreSQL)
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: myapp
spec:
  serviceName: postgres-service
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
        tier: database
    spec:
      containers:
      - name: postgres
        image: postgres:15
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          value: myapp
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
  namespace: myapp
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
  clusterIP: None  # Headless service (pour StatefulSet)
---
# Cache (Redis)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
        tier: cache
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  namespace: myapp
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
---
# Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-ingress
  namespace: myapp
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - myapp.example.com
    secretName: myapp-tls
  rules:
  - host: myapp.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8000
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
---
# Network Policies
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-policy
  namespace: myapp
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - port: 8000
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - port: 5432
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - port: 6379
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
```

### Cas 2 : Service Mesh avec Istio

Istio ajoute une couche rÃ©seau avancÃ©e avec des sidecars Envoy.

```yaml
# app-with-istio.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: myapp
  labels:
    istio-injection: enabled  # Auto-inject Envoy sidecar
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
  namespace: myapp
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api
      version: v1
  template:
    metadata:
      labels:
        app: api
        version: v1
    spec:
      containers:
      - name: api
        image: myapp/api:v1
        ports:
        - containerPort: 8000
---
# VirtualService pour routing intelligent
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: api-routes
  namespace: myapp
spec:
  hosts:
  - api-service
  http:
  # Canary deployment : 90% v1, 10% v2
  - match:
    - headers:
        x-user-group:
          exact: beta
    route:
    - destination:
        host: api-service
        subset: v2
      weight: 100

  - route:
    - destination:
        host: api-service
        subset: v1
      weight: 90
    - destination:
        host: api-service
        subset: v2
      weight: 10

    # Retry automatique
    retries:
      attempts: 3
      perTryTimeout: 2s
      retryOn: 5xx,reset,connect-failure

    # Timeout
    timeout: 10s
---
# DestinationRule pour load balancing et circuit breaking
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: api-destination
  namespace: myapp
spec:
  host: api-service
  trafficPolicy:
    loadBalancer:
      consistentHash:
        httpHeaderName: "x-user-id"  # Sticky session

    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 100

    outlierDetection:
      consecutiveErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50

  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
```

**ObservabilitÃ© Istio :**

```bash
# Voir le trafic entre services
kubectl exec -it <pod> -c istio-proxy -- pilot-agent request GET stats

# Metrics Prometheus
kubectl port-forward -n istio-system svc/prometheus 9090:9090

# Distributed tracing (Jaeger)
kubectl port-forward -n istio-system svc/tracing 16686:80

# Service graph (Kiali)
kubectl port-forward -n istio-system svc/kiali 20001:20001
```

## Debugging rÃ©seau Kubernetes

### Outils essentiels

```bash
# 1. Pod de debug (Swiss Army Knife)
kubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- bash

# Dans le pod debug :
# - ping, curl, wget, nc
# - tcpdump, nmap
# - dig, nslookup
# - iperf3
# - etc.

# 2. Exec dans un pod existant
kubectl exec -it <pod-name> -- /bin/bash

# 3. Logs rÃ©seau
kubectl logs <pod-name>

# 4. Describe (voir les events)
kubectl describe pod <pod-name>

# 5. Port-forward pour debug local
kubectl port-forward pod/<pod-name> 8080:8080
curl http://localhost:8080
```

### ScÃ©narios de debugging

**ProblÃ¨me 1 : Pod ne peut pas atteindre un Service**

```bash
# 1. VÃ©rifier que le Pod est Running
kubectl get pod <pod-name>

# 2. VÃ©rifier le Service existe
kubectl get svc <service-name>

# 3. VÃ©rifier les endpoints (Pods behind Service)
kubectl get endpoints <service-name>
# Si vide â†’ aucun Pod ne match le selector

# 4. Test DNS
kubectl exec <pod-name> -- nslookup <service-name>

# 5. Test connectivitÃ© IP
kubectl exec <pod-name> -- curl http://<service-ip>:<port>

# 6. VÃ©rifier Network Policies
kubectl get networkpolicy
kubectl describe networkpolicy <policy-name>

# 7. Logs CNI
kubectl logs -n kube-system -l k8s-app=calico-node
```

**ProblÃ¨me 2 : Ingress ne route pas correctement**

```bash
# 1. VÃ©rifier Ingress Controller
kubectl get pods -n ingress-nginx

# 2. VÃ©rifier Ingress rules
kubectl get ingress
kubectl describe ingress <ingress-name>

# 3. Logs Ingress Controller
kubectl logs -n ingress-nginx <ingress-controller-pod>

# 4. Test depuis l'intÃ©rieur
kubectl run test --image=busybox -it --rm -- sh
/ # wget -O- http://<ingress-controller-svc>

# 5. VÃ©rifier DNS externe
nslookup myapp.example.com

# 6. VÃ©rifier certificats TLS
kubectl get secret <tls-secret> -o yaml
```

**Script de diagnostic complet :**

```python
#!/usr/bin/env python3
import subprocess
import json
import sys

def run_cmd(cmd):
    """ExÃ©cuter une commande kubectl"""
    result = subprocess.run(
        cmd.split(),
        capture_output=True,
        text=True
    )
    return result.stdout, result.returncode

def diagnose_service(namespace, service_name):
    """Diagnostic complet d'un Service Kubernetes"""

    print(f"=== Diagnostic pour {namespace}/{service_name} ===\n")

    # 1. Service existe ?
    print("1ï¸âƒ£  VÃ©rification Service...")
    stdout, code = run_cmd(f"kubectl get svc {service_name} -n {namespace} -o json")

    if code != 0:
        print(f"âŒ Service {service_name} n'existe pas dans {namespace}")
        return

    svc = json.loads(stdout)
    print(f"âœ… Service trouvÃ© : {svc['spec']['type']}")
    print(f"   ClusterIP : {svc['spec']['clusterIP']}")
    print(f"   Ports : {svc['spec']['ports']}")

    # 2. Endpoints
    print("\n2ï¸âƒ£  VÃ©rification Endpoints...")
    stdout, code = run_cmd(f"kubectl get endpoints {service_name} -n {namespace} -o json")

    if code == 0:
        ep = json.loads(stdout)
        subsets = ep.get('subsets', [])

        if not subsets:
            print("âŒ Aucun endpoint (aucun Pod ne match le selector)")
            print(f"   Selector : {svc['spec']['selector']}")
            return

        total_pods = sum(len(s.get('addresses', [])) for s in subsets)
        print(f"âœ… {total_pods} Pod(s) backend trouvÃ©(s)")

        for subset in subsets:
            for addr in subset.get('addresses', []):
                print(f"   â€¢ {addr['ip']} ({addr.get('targetRef', {}).get('name', 'unknown')})")

    # 3. Test DNS
    print("\n3ï¸âƒ£  Test DNS...")
    test_pod = "debug-test-dns"

    # CrÃ©er pod de test
    run_cmd(f"kubectl run {test_pod} --image=busybox --restart=Never -n {namespace} -- sleep 3600")

    # Attendre que le pod soit prÃªt
    import time
    time.sleep(5)

    # Test DNS
    stdout, code = run_cmd(f"kubectl exec {test_pod} -n {namespace} -- nslookup {service_name}")

    if code == 0:
        print("âœ… DNS fonctionne")
        print(stdout)
    else:
        print("âŒ Ã‰chec rÃ©solution DNS")

    # Test connectivitÃ©
    print("\n4ï¸âƒ£  Test connectivitÃ©...")
    port = svc['spec']['ports'][0]['port']
    stdout, code = run_cmd(f"kubectl exec {test_pod} -n {namespace} -- wget -O- http://{service_name}:{port} --timeout=5")

    if code == 0:
        print("âœ… ConnectivitÃ© OK")
    else:
        print("âŒ Ã‰chec connectivitÃ©")
        print("   VÃ©rifier Network Policies...")

    # Nettoyer
    run_cmd(f"kubectl delete pod {test_pod} -n {namespace}")

    # 5. Network Policies
    print("\n5ï¸âƒ£  Network Policies applicables...")
    stdout, code = run_cmd(f"kubectl get networkpolicy -n {namespace} -o json")

    if code == 0:
        policies = json.loads(stdout)
        if policies['items']:
            for policy in policies['items']:
                print(f"   â€¢ {policy['metadata']['name']}")
        else:
            print("   Aucune Network Policy")

# Usage
if __name__ == '__main__':
    if len(sys.argv) != 3:
        print("Usage: python k8s-diagnose.py <namespace> <service-name>")
        sys.exit(1)

    diagnose_service(sys.argv[1], sys.argv[2])
```

## Best Practices

### Docker

- âœ… **Utiliser des rÃ©seaux custom** (pas le bridge par dÃ©faut)
- âœ… **Nommer les conteneurs** pour DNS interne
- âœ… **Limiter les ports exposÃ©s** (principe du moindre privilÃ¨ge)
- âœ… **Utiliser docker-compose** pour orchestration multi-conteneurs
- âœ… **Ã‰viter le mode host** en production (sauf nÃ©cessitÃ© absolue)
- âœ… **Monitorer les rÃ©seaux** (docker network inspect)

### Kubernetes

- âœ… **Utiliser des Services** (pas d'IP Pods directes)
- âœ… **ImplÃ©menter Network Policies** (sÃ©curitÃ©)
- âœ… **DÃ©finir des Resource Limits** (Ã©viter saturation)
- âœ… **Utiliser des Liveness/Readiness Probes** (santÃ©)
- âœ… **PrÃ©fÃ©rer ClusterIP** + Ingress (vs NodePort/LoadBalancer)
- âœ… **Documenter les dÃ©pendances rÃ©seau** (qui parle Ã  qui)
- âœ… **Tester les failovers rÃ©seau** (chaos engineering)

### SÃ©curitÃ© rÃ©seau

```yaml
# Checklist sÃ©curitÃ© rÃ©seau conteneurs

## Isolation
- [ ] Network Policies en place (deny-all par dÃ©faut)
- [ ] Namespaces sÃ©parÃ©s par environnement
- [ ] Pas de mode host en production
- [ ] Pas de ports privilÃ©giÃ©s (< 1024) sans nÃ©cessitÃ©

## Chiffrement
- [ ] TLS sur tous les endpoints exposÃ©s
- [ ] mTLS entre services (Istio/Linkerd)
- [ ] Secrets chiffrÃ©s (KMS, Sealed Secrets)

## Monitoring
- [ ] Logs rÃ©seau centralisÃ©s
- [ ] Alertes sur trafic anormal
- [ ] Network flow monitoring (Cilium Hubble)

## ConformitÃ©
- [ ] Pod Security Standards (restricted)
- [ ] Network scan rÃ©guliers (Trivy, Falco)
- [ ] Audit des Network Policies
```

## RÃ©sumÃ© : RÃ©seaux conteneurs

### Concepts clÃ©s

**Docker :**
- **Namespaces rÃ©seau** â†’ Isolation
- **Bridge networks** â†’ Communication inter-conteneurs
- **Port mapping** â†’ AccÃ¨s depuis l'hÃ´te
- **DNS intÃ©grÃ©** â†’ Service discovery

**Kubernetes :**
- **Flat network** â†’ Tous les Pods peuvent communiquer
- **Services** â†’ Abstraction stable sur Pods Ã©phÃ©mÃ¨res
- **Ingress** â†’ Routage L7 intelligent
- **Network Policies** â†’ Firewall inter-Pods

### Tableaux de comparaison

| Feature | Docker | Kubernetes |
|---------|--------|------------|
| **Isolation** | Namespaces | Namespaces + CNI |
| **Service Discovery** | DNS interne | Services + CoreDNS |
| **Load Balancing** | Round-robin simple | kube-proxy (iptables/IPVS) |
| **Ingress** | Pas natif | Ingress Controllers |
| **Network Policies** | Pas natif | Natif (via CNI) |
| **Multi-host** | Swarm overlay | CNI (Calico, Cilium, etc.) |

### Troubleshooting rapide

| SymptÃ´me | Docker | Kubernetes |
|----------|--------|------------|
| **Conteneur/Pod injoignable** | `docker network inspect` | `kubectl describe pod` |
| **DNS ne rÃ©sout pas** | VÃ©rifier nom conteneur | `kubectl exec -- nslookup` |
| **Port non accessible** | `docker port`, mapping correct ? | Service existe ? Endpoints ? |
| **Connexion refusÃ©e** | Firewall hÃ´te ? | Network Policy ? |
| **Lenteur rÃ©seau** | Mode host ? | CNI performant (Cilium) ? |

## Conclusion

Les rÃ©seaux conteneurs sont **fondamentaux** pour dÃ©ployer des applications modernes. La comprÃ©hension de ces concepts vous permet de :

**DÃ©velopper efficacement :**
- Architecturer des microservices communicants
- DÃ©bugger rapidement les problÃ¨mes rÃ©seau
- Optimiser les performances

**DÃ©ployer en confiance :**
- Configurer la sÃ©curitÃ© rÃ©seau (Network Policies)
- GÃ©rer le trafic (Services, Ingress)
- Assurer la rÃ©silience (Health checks, DNS)

**Ã€ retenir :**
- Docker â†’ **SimplicitÃ©**, bon pour dev/test
- Kubernetes â†’ **Production**, orchestration Ã  grande Ã©chelle
- **Services** > IPs directes (toujours)
- **Network Policies** = essentiel en production
- **DNS interne** = votre ami pour service discovery
- **Debugger** avec les bons outils (netshoot, kubectl exec)

**Prochaine Ã©tape :** La section suivante explore le **Software-Defined Networking (SDN)**, qui abstrait encore davantage le rÃ©seau physique pour plus de flexibilitÃ© et d'automatisation.

---


*DerniÃ¨re mise Ã  jour : DÃ©cembre 2025*

â­ï¸ [Software-Defined Networking (SDN) : introduction](/09-architectures-avancees/08-sdn.md)
